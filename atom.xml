<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yuyang&#39;s Blog</title>
  
  <subtitle>Everyone is fighting their own battle.</subtitle>
  <link href="https://enblog.crocodilezs.top/atom.xml" rel="self"/>
  
  <link href="https://enblog.crocodilezs.top/"/>
  <updated>2022-08-29T16:13:05.664Z</updated>
  <id>https://enblog.crocodilezs.top/</id>
  
  <author>
    <name>CrocodileZS</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>How to get a subgraph using SPARQL query in Python</title>
    <link href="https://enblog.crocodilezs.top/202208/2022-06-27-SPARQL/"/>
    <id>https://enblog.crocodilezs.top/202208/2022-06-27-SPARQL/</id>
    <published>2022-06-27T15:14:06.000Z</published>
    <updated>2022-08-29T16:13:05.664Z</updated>
    
    <content type="html"><![CDATA[<p>Knowledge Graph is one of the most common forms of Factual Knowledge. Many researches have combined pretrained language model  with knowledge graph to improve its performance on NLU tasks. Before injecting the knowledge into the model, we should go through the <u>tagging (named entity recognition), entity grounding (entity linking), and retrieving (e.g. SPARQL)</u>process. This blog is talking about <u>how to get a specific subgraph by SPARQL queries</u>.</p><span id="more"></span><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>As a query language, SPARQL can be used to add, remove and retrieve data from RDF-style graph databases. Wikidata use SPARQL to retrieval its subject-predicate-object triplets. Wikidata provide <a href="https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/A_gentle_introduction_to_the_Wikidata_Query_Service">a gentle introduction to the Wikidata Query Service</a>. <u>However, it is only suitable for non-complicated situations. For instance, given a subject and a predicate, to find the object.</u></p><p>In knowledge-based NLU(natural language understanding) tasks, we usually need to find the subgraph of an entity. For example, given a sentence <code>Bert likes reading in the Sesame Street Library.</code>, the NER process has found 2 entities <code>Bert(Sesame Street)</code> and <code>Sesame Street</code>, and these 2 entities are linked to wikidata ids(<code>Q584184</code> and <code>Q155629</code>). Then we need to find subgraphs which take the 2 entities as center respectively.</p><p>To make it easier to understand, we could run SPARQL in <a href="https://query.wikidata.org/">Wikidata Query Service</a> before we run it in Python.</p><h2 id="Construct-the-query"><a href="#Construct-the-query" class="headerlink" title="Construct the query"></a>Construct the query</h2><p>They final query is complicated, so we start trying to decompose it.<br>Final query:<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT DISTINCT ?label ?property ?propertynameLabel ?value ?valueLabel</span><br><span class="line">WHERE &#123;</span><br><span class="line">    wd:Q584184 ?property ?value .</span><br><span class="line">    wd:Q584184 rdfs:label ?label .</span><br><span class="line">    SERVICE wikibase:label &#123; bd:serviceParam wikibase:language &quot;en&quot; &#125;</span><br><span class="line">    ?propertyname wikibase:directClaim ?property</span><br><span class="line">    FILTER (STRSTARTS(STR(?property), &quot;http://www.wikidata.org/prop/direct&quot;))</span><br><span class="line">    FILTER (STRSTARTS(STR(?value), &quot;http://www.wikidata.org/entity&quot;))</span><br><span class="line">    FILTER (langMatches(lang(?label), &quot;EN&quot;))              </span><br><span class="line">&#125; limit 20</span><br></pre></td></tr></table></figure></p><p>At first, we could make a simple query.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT DISTINCT ?property ?value </span><br><span class="line">WHERE &#123;</span><br><span class="line">    wd:Q584184 ?property ?value .</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The result is shown below:<br><img src="https://bu.dusays.com/2022/08/29/630cbe380a642.png" alt=""><br>We can only find the wikidata id of object and predicate. Now we try to find what the wikidata id means.</p><hr><p>Find the object label:<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT DISTINCT ?property ?value ?valueLabel</span><br><span class="line">WHERE &#123;</span><br><span class="line">wd:Q584184 ?property ?value .</span><br><span class="line">SERVICE wikibase:label &#123; bd:serviceParam wikibase:language &quot;en&quot; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>We could get the result:<br><img src="https://bu.dusays.com/2022/08/29/630cbde8175b5.png" alt=""></p><p>Find the predicate label:<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT DISTINCT ?property ?propertynameLabel ?value ?valueLabel</span><br><span class="line">WHERE &#123;</span><br><span class="line">    wd:Q584184 ?property ?value .</span><br><span class="line">    SERVICE wikibase:label &#123; bd:serviceParam wikibase:language &quot;en&quot; &#125;</span><br><span class="line">    ?propertyname wikibase:directClaim ?property</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>Result:<br><img src="https://bu.dusays.com/2022/08/29/630cbfe1cf6a4.png" alt=""></p><hr><p>We want to get the subgraph of the specific entity <code>Bert</code>. But now there are many numbers and strings in the result. For example, the FAST ID of Bert is 830807. Obviously, 830807 is not an entity and we only want entities in our object results. So we need to limit the prefix of the predicate and object.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT DISTINCT ?property ?propertynameLabel ?value ?valueLabel</span><br><span class="line">WHERE &#123;</span><br><span class="line">    wd:Q584184 ?property ?value .</span><br><span class="line">    SERVICE wikibase:label &#123; bd:serviceParam wikibase:language &quot;en&quot; &#125;</span><br><span class="line">    ?propertyname wikibase:directClaim ?property</span><br><span class="line">    FILTER (STRSTARTS(STR(?property), &quot;http://www.wikidata.org/prop/direct&quot;))</span><br><span class="line">    FILTER (STRSTARTS(STR(?value), &quot;http://www.wikidata.org/entity&quot;))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://bu.dusays.com/2022/08/29/630cc15c247bc.png" alt=""></p><p>We can see that we filter the number objects out.</p><hr><p>Finally, we get the label (english) of subject.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT DISTINCT ?label ?property ?propertynameLabel ?value ?valueLabel</span><br><span class="line">WHERE &#123;</span><br><span class="line">    wd:Q584184 ?property ?value .</span><br><span class="line">    wd:Q584184 rdfs:label ?label .</span><br><span class="line">    SERVICE wikibase:label &#123; bd:serviceParam wikibase:language &quot;en&quot; &#125;</span><br><span class="line">    ?propertyname wikibase:directClaim ?property</span><br><span class="line">    FILTER (STRSTARTS(STR(?property), &quot;http://www.wikidata.org/prop/direct&quot;))</span><br><span class="line">    FILTER (STRSTARTS(STR(?value), &quot;http://www.wikidata.org/entity&quot;))</span><br><span class="line">    FILTER (langMatches(lang(?label), &quot;EN&quot;))              </span><br><span class="line">&#125; limit 20</span><br></pre></td></tr></table></figure><p>Here is the result:</p><p><img src="https://bu.dusays.com/2022/08/29/630cc2d62d2e9.png" alt=""></p><h2 id="Query-in-Python"><a href="#Query-in-Python" class="headerlink" title="Query in Python"></a>Query in Python</h2><p>I use <a href="https://qwikidata.readthedocs.io/en/stable/readme.html">qwikidata</a> to search wikidata online service in python. There are other <a href="https://www.wikidata.org/wiki/Wikidata:Tools/For_programmers">Wikidata Tools</a> like go-wikidata.</p><p>You can also use <a href="https://dumps.wikimedia.org/wikidatawiki/entities/">wikidata dump</a> to search at local.(I don’t recommend it because it is really resource-consuming. <a href="https://akbaritabar.netlify.app/how_to_use_a_wikidata_dump">A tutorial</a>)</p><p>Here is <a href="https://www.wikidata.org/wiki/Wikidata:Data_access">all methods of wikidata access</a></p><p>Query in python:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> qwikidata.sparql <span class="keyword">import</span> (get_subclasses_of_item,</span><br><span class="line">                              return_sparql_query_results)</span><br><span class="line">                              </span><br><span class="line">sparql_query = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    SELECT DISTINCT ?label ?property1 ?property1nameLabel ?value1 ?value1Label</span></span><br><span class="line"><span class="string">    WHERE &#123;</span></span><br><span class="line"><span class="string">        wd:%s ?property1 ?value1 .</span></span><br><span class="line"><span class="string">        wd:%s rdfs:label ?label .</span></span><br><span class="line"><span class="string">        FILTER (langMatches( lang(?label), &quot;EN&quot; ) )</span></span><br><span class="line"><span class="string">        FILTER(STRSTARTS(STR(?property1), &quot;http://www.wikidata.org/prop/direct/&quot;))</span></span><br><span class="line"><span class="string">        FILTER(STRSTARTS(STR(?value1), &quot;http://www.wikidata.org/entity/&quot;))</span></span><br><span class="line"><span class="string">        SERVICE wikibase:label &#123; bd:serviceParam wikibase:language &quot;en&quot;. &#125;</span></span><br><span class="line"><span class="string">        ?property1name wikibase:directClaim ?property1.</span></span><br><span class="line"><span class="string">    &#125; limit %d</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span> % (wikidata_id, wikidata_id, result_num)</span><br><span class="line">            </span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    res = return_sparql_query_results(sparql_query)</span><br><span class="line">    <span class="comment"># print(&quot;finish query.&quot;)</span></span><br><span class="line">    <span class="keyword">for</span> triple <span class="keyword">in</span> res[<span class="string">&#x27;results&#x27;</span>][<span class="string">&#x27;bindings&#x27;</span>]:</span><br><span class="line">        subjection_id = wikidata_id</span><br><span class="line">        subjection_name = triple[<span class="string">&#x27;label&#x27;</span>][<span class="string">&#x27;value&#x27;</span>]</span><br><span class="line">        predicate_id = triple[<span class="string">&#x27;property1&#x27;</span>][<span class="string">&#x27;value&#x27;</span>].split(<span class="string">&quot;/prop/direct/&quot;</span>)[<span class="number">1</span>]</span><br><span class="line">        predicate_name = triple[<span class="string">&#x27;property1nameLabel&#x27;</span>][<span class="string">&#x27;value&#x27;</span>]</span><br><span class="line">        objection_id = triple[<span class="string">&#x27;value1&#x27;</span>][<span class="string">&#x27;value&#x27;</span>].split(<span class="string">&quot;/entity/&quot;</span>)[<span class="number">1</span>]</span><br><span class="line">        objection_name = triple[<span class="string">&#x27;value1Label&#x27;</span>][<span class="string">&#x27;value&#x27;</span>]</span><br><span class="line">        triple_list.append(&#123;<span class="string">&quot;subjection_id&quot;</span>: subjection_id, <span class="string">&quot;subjection_name&quot;</span>: subjection_name,</span><br><span class="line">                            <span class="string">&quot;predicate_id&quot;</span>: predicate_id, <span class="string">&quot;predicate_name&quot;</span>: predicate_name,</span><br><span class="line">                            <span class="string">&quot;objection_id&quot;</span>: objection_id, <span class="string">&quot;objection_name&quot;</span>: objection_name&#125;)</span><br><span class="line">    <span class="keyword">return</span> triple_list</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Error in wikidata_id: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(wikidata_id))</span><br></pre></td></tr></table></figure></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Knowledge Graph is one of the most common forms of Factual Knowledge. Many researches have combined pretrained language model  with knowledge graph to improve its performance on NLU tasks. Before injecting the knowledge into the model, we should go through the &lt;u&gt;tagging (named entity recognition), entity grounding (entity linking), and retrieving (e.g. SPARQL)&lt;/u&gt;process. This blog is talking about &lt;u&gt;how to get a specific subgraph by SPARQL queries&lt;/u&gt;.&lt;/p&gt;</summary>
    
    
    
    <category term="Knowledge Graph" scheme="https://enblog.crocodilezs.top/categories/Knowledge-Graph/"/>
    
    
  </entry>
  
  <entry>
    <title>ICM 2021 | Social Network | Cracking the Secret of Musical Influence</title>
    <link href="https://enblog.crocodilezs.top/202102/2021-02-10-2021ICM-Music/"/>
    <id>https://enblog.crocodilezs.top/202102/2021-02-10-2021ICM-Music/</id>
    <published>2021-04-10T10:24:24.000Z</published>
    <updated>2022-08-29T09:19:46.932Z</updated>
    
    <content type="html"><![CDATA[<p>We participated in ICM 2021 during Feb 5th - Feb 9th and finally honored <strong>Meritorious Winner(Top 7%)</strong>😆. Because we 3 people are all music enthusiasts, we chose the problem D about data mining in music similarity and influence. We thought we could do better in this field. It turns out that we were right. Here is our thinking and solution to this problem.</p><span id="more"></span><h2 id="Topic-and-Datasets"><a href="#Topic-and-Datasets" class="headerlink" title="Topic and Datasets"></a>Topic and Datasets</h2><p>Here are <a href="https://www.mathmodels.org/Problems/2021/ICM-D/2021_ICM_Problem_D.pdf">the topic and the datasets</a>. You can also see them on <a href="https://www.mathmodels.org/Problems/2021/ICM-D/index.html">COMAP</a>.</p><p>In short, our team has been identified by an organization to develop a model that measures musical influence. This problem asks us to examine evolutionary and revolutionary trends of artists and genres. We has been given 4 data sets:</p><ol><li><code>influence_data.csv</code> represents musical influencers and followers, as reported by the artists themselves, as well as the opinions of industry experts. These data contains influencers and followers for 5,854 artists in the last 90 years.</li><li><code>full_music_data.csv</code> provides 16 variable entries, including musical features such as <code>danceability</code>, <code>tempo</code>, <code>loudness</code>, and <code>key</code>, along with <code>artist_name</code> and <code>artist_id</code> for each of 98,340 songs. These data are used to create two summary data sets, including: mean values by artist - <code>data_by_artist.csv</code>, means across years <code>data_by_year.csv</code>.</li></ol><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><blockquote><p>Music has been a necessary part of human life and history since we human have consciousness. At the same time of human evolution, music is constantly changing new forms and contents. Music itself doesn’t evolve, and there’s no doubt that it’s done by a group of people called “artists”. It’s a common sense that a new form of music occurs under the action of many factors, such as artists’ innate creativity, current social or political events, access to new instruments or tools, or other personal experiences.</p></blockquote><p><img src="https://bu.dusays.com/2022/08/29/630c817ed1252.png" alt=""></p><p><font size=3 color="black">Figure 1: The architecture of our model</font></p><p>Based on some basic musical attributes, our aim in this report is to build a model to quantify musical evolution. We are expected to provide a measurement mechanism to observe how the previous music affects the later music and musicians. So two major models are established to finish that job.</p><p>For the model 1, we analyzed <strong>the relationships</strong> between genre, artist and music from the perspective of influence and similarity. We use the <strong>BA model</strong> to explain the influence network, which connects the influencers and followers. Based on the influence relationships between artists, we use <strong>the directed edge of the model</strong> to construct an influence-index. And then based on this index, we use <strong>the improved Louvain algorithm</strong> to in <strong>community partition</strong> to get The partition based on influence.</p><p>On the other hand, from the perspective of the similarity between artists, we use <strong>Pearson correlation coefficient</strong> to remove the redundant of music attributes, and construct the measure index of similarity. Based on this index, we use <strong>the improved K-Means algorithm</strong> in community partition, and finally obtain The partition based on similarity.</p><p>We creatively proposed to <strong>combine these two networks</strong>, and <strong>the NMI index</strong> is used to analyze the relationship between similarity and influence. Finally, we prove that the influencers actually affect the music created by the followers.</p><p>For the model 2, we took <strong>time</strong> into consideration. In order to analyze the musical evolution process, We use <strong>ARIMA model</strong> to create an ideal evolution curve. We compare the real curve with the ideal evolution curve to find how the social, culture and technology affect the music. We give an example of electronic music. Symbols and Definitions show in the Table 1.</p><p><img src="https://bu.dusays.com/2022/08/29/630c817becb40.png" alt=""></p><h2 id="Model-1-BA-Model-and-Optimized-Louvain-KMeans-Algorithm"><a href="#Model-1-BA-Model-and-Optimized-Louvain-KMeans-Algorithm" class="headerlink" title="Model 1: BA Model and Optimized Louvain-KMeans Algorithm"></a>Model 1: BA Model and Optimized Louvain-KMeans Algorithm</h2><p>Model 1 we analyze social networks from the perspective of influence and similarity, and then analyze the influence and similarity relationship among genres, artists and music.</p><p>In this model, we use directed graph to construct influence network and BA model to explain the influence propagation relationship in the network. First, we analyzed the influence relationship between artists. We propose the influence index by using the indegree and outdegree parameters inthe directed graph, and use the improved Louvain algorithm in community partition to get the “The partition based on influence”.</p><p>Then we analyze the similarity among the music, we use <strong>Pearson correlation coefficient</strong> to delete redundant attributes, and propose the measure of “similarity”. Through similarity measurement, the improved KMeans algorithm is used in community partition to obtain “The partition based on “similarity”.</p><p>After that, we innovatively combine the above two networks, and perfectly explain the relationship between music influence and similarity by using NMI parameters. We call this innovative method Optimized Louvain KMeans Algorithm.</p><h3 id="Analysis-of-Music-Influence"><a href="#Analysis-of-Music-Influence" class="headerlink" title="Analysis of Music Influence"></a>Analysis of Music Influence</h3><h4 id="The-influence-of-network-BA-model"><a href="#The-influence-of-network-BA-model" class="headerlink" title="The influence of network (BA model)"></a>The influence of network (BA model)</h4><p>In Model 1 we analyze social networks from the perspective of influence and similarity, and then analyze the influence and similarity relationship between genres, artists and music.</p><p><code>Influence_data.csv</code> includes 5854 artists in the past 90 years according to the artists themselves and industry experts. <strong>We construct 42770 influence relationships among the 5854 artists through the data set, and the directed edges in the network point from the follower to the influencer</strong>. In the Figure3.1.1, artists of different genres are represented by different colors. There are <strong>19 genres of music (except unknown) in the dataset</strong>.</p><p>Through the Figure2, we can roughly know the influence relationship between different genres. For example, we can easily see that Pop/Rock music has influence on all other genres, which is also determined by the characteristics of Pop/Rock music itself. Based on this network, the later content of this paper will further analyze the relationship between the influence of music characteristics and social network, age, policy and so on.</p><p><img src="https://bu.dusays.com/2022/08/29/630c818076d7e.png" alt=""></p><p><font size=3 color="black">Figure 2: Directed network of musical influence</font></p><p>In network theory, scale-free network is a kind of complex network. Its typical feature is that most nodes in the network are only connected with a few nodes, and a few nodes are connected with a lot of nodes. In reality, many networks have scale-free characteristics, such as Internet, financial system network, social network and so on.</p><p>According to the influence direct graph, we guess that the influence network is a scale-free network, and then we will verify it by BA model.This model is based on two assumptions.</p><ul><li>Growth model: many real networks are constantly expanding and growing, such as the birth of new web pages in the Internet, the publication of new papers and so on. Obviously, the musical influence network is also expanding through the increasing influence relationship.</li><li>Priority connection mode: when new nodes join, they tend to connect with nodes with more connections. For example, new web pages usually have connections to well-known Web sites, and new papers tend to cite well-known literatures that have been widely cited, etc. In reality, the same is true of influence. New musicians are more likely to be influenced by<br>influential influencers.</li></ul><p>Based on the above two hypotheses, we randomly selected 500 followers from the data set, removed them from the established social network, and then connected them back to the influence network by building a scale-free network with BA model.</p><p>There are \(N<em>a\) nodes in the original network, we will add a new artist \(a_i\) to the network. When \(a_i\) is a new node, \(m\) edges are connected from the new node to the original node, and the connection mode is the node with priority given to the high heights. For the original artist \(a_j\) in the network,the number of degree in the original network is recorded as \(idg</em>{a<em>j}\). Then the probability \(p</em>{a_i,a_j}\) of the new node can be canculate as follows:</p><script type="math/tex; mode=display">P_{a_i,a_j}=\frac{idg_{a_j}}{\sum_{K \in artist_{id}}idg_{a_k}}\tag{1}</script><p>By choosing the appropriate probability threshold, we add these 500 nodes back to the influence network, and create related directed edges. According to the real influence network, we calculate the connection accuracy of each new node(\(a_i\)) and the original node. The accuracy of the connection betweennew node(\(a_i\)) and the original node is 81.74%. This shows that the influence network basically conforms to the BA model.</p><p>According to the properties of BA model, we can know that the distribution of the number of the followers of artists can be approximately described by a power function with a power exponent of 3. For artist \(a_i\), the distribution function of the number of people affected is:</p><script type="math/tex; mode=display">p(idg_{a_i}) \propto 2idg^3_{a_i}\tag{2}</script><p>The distribution function can help us better explain the spread of influence, and can also be used to predict which artists are more attractive to the new artists.</p><h4 id="Analysis-of-music-influence-from-the-perspective-of-genre"><a href="#Analysis-of-music-influence-from-the-perspective-of-genre" class="headerlink" title="Analysis of music influence from the perspective of genre"></a>Analysis of music influence from the perspective of genre</h4><p>To quantify the influence of music genres, based on the previous social network, we counted the number of directed edges between genres as the influence parameter, and built the heat map between genres. In order to show the heat map better, we take logarithm of the number of directed edges.</p><p>As can be seen from the below Figure 3, the influence of all artists is mainly reflected in the genre itself. Jazz has an impact on all other genres of music. Pop / Rock, R&amp;B and Vocal also have a great impact on other fields.</p><p><img src="https://bu.dusays.com/2022/08/29/630c816b2af30.png" alt=""></p><p><font size=3 color="black">Figure 3: Heat map between genres</font></p><h4 id="Influence-index"><a href="#Influence-index" class="headerlink" title="Influence-index"></a>Influence-index</h4><p>In order to evaluate the music influence of artists better, we introduce the following evaluation measures. On the basis of the number of followers, we construct the comprehensive measure of influence index, which combines the influence of artists in and out of their genres to give an objective evaluation of the influence of artists.</p><script type="math/tex; mode=display">Influence-index_i=F_{in_i}+logG_i * F_{out_i}\tag{3}</script><p>We choose pop / rock artists as a sub network to investigate the influence of all artists. According to the influence index we set, we find out the top ten artists of this genre.The music genres and influence-index influenced by the ten artists are shown in the following table.</p><p><img src="https://bu.dusays.com/2022/08/29/630c817ce293e.png" alt=""></p><p>It is easy to check the influencer and followers of each artist in the network we built. Taking The Beatles as an example, the influence network chart of its music is shown in the Figure 4.</p><p><img src="https://bu.dusays.com/2022/08/29/630c816c81ddd.png" alt=""></p><p><font size=3 color="black">Figure 4: The influence network chart of The Beatles</font></p><h3 id="Analysis-of-Music-Similarity"><a href="#Analysis-of-Music-Similarity" class="headerlink" title="Analysis of Music Similarity"></a>Analysis of Music Similarity</h3><h4 id="The-similarity-index"><a href="#The-similarity-index" class="headerlink" title="The similarity index"></a>The similarity index</h4><p>In order to establish the music similarity measurement model, we can compare the similarity degree of each parameter between different music. In this question, there are 7 characteristics of the music and 5 types of vocals, a total of 12 parameters to reflect the characteristics of music (<code>danceability</code>, <code>energy</code>, <code>valence</code>, <code>tempo</code>, <code>loudness</code>, <code>mode</code>, <code>key</code>, <code>acousticenss</code>, <code>instrumentalness</code>, <code>livenss</code>, <code>speechiness</code>, <code>explicit</code>).</p><p>By calculating Pearson correlation coefficients between 12 music parameters, the correlation co-efficient matrix is obtained, and the redundant parameters with high correlation can be eliminated.</p><p>Energy and valence data in <code>full-music-data.csv</code> were taken as an example to calculate the correlation coefficient, and the data could be expressed as: \( e:\{e<em>{m_1}, e</em>{m<em>2}, e</em>{m<em>3}, …, e</em>{m<em>{N_n}}\} \), \( v:\{v</em>{m<em>1}, v</em>{m<em>2}, v</em>{m<em>3},…, v</em>{m_{N_n}}\} \).</p><script type="math/tex; mode=display">\begin{align}&Mean Value: E(e) = \frac{\sum_{i=1}^ne_{m_i}}{n}, E(v)=\frac{\sum_{i=1}^nv_{m_i}}{n}, \\&Covariance: Cov(e, v) = \frac{\sum_{i=1}^n(e_{m_i}-E(e))(v_{m_i}-E(v))}{n}, \\&Standard Deviation: \delta_e=\sqrt{\frac{\sum_{i=1}^n(e_{m_i}-E(e))^2}{n}}, \delta_v=\sqrt{\frac{\sum_{i=1}^n(v_{m_i}-E(v))^2}{n}}, \\&Pearson = \rho_{ev} = \frac{Cov(e, v)}{\delta_e\delta_v}=\frac{\frac{\sum_{i=1}^n(e_{m_i}-E(e))(V_{m_i}-E(v))}{\delta_e\delta_v}}{n}\end{align}\tag{4}</script><p>Using the data in full-music-data.csv, the correlation coefficient between each index can be calculated. Its correlation coefficient matrix is shown in Figure 5.</p><p><img src="https://bu.dusays.com/2022/08/29/630c816dee4cb.png" alt=""></p><p><font size=3 color="black">Figure 5: The correlation coefficient between each index</font></p><p>In order to reduce the influence of the correlation between indicators, indicators in each group with the absolute value of correlation coefficient greater than 0.3 were selected. Then, one of the two indicators of each group was selected, and the index used to evaluate music similarity was as follows:<code>danceability</code>, <code>energy</code>, <code>mode</code>, <code>key</code>, <code>liveness</code>, <code>speechiness</code> and <code>explicit</code>.</p><p>The values of <code>mode</code> and <code>explict</code> are Boolean values, so they are excluded from the music similarity model. The similarity of songs was evaluated by comparing the degree of differentiation of the remaining five indicators between different songs. We defined a numerical quantity called similarity to indicate the degree of difference between songs, and the higher the similarity value is, the more similar the songs are.</p><p>For example, calculate the similarity of song j and song k in full-musci-data file, and its data can be expressed as: \(M<em>j{d</em>{m<em>j}, e</em>{m<em>j}, l</em>{m<em>j}, s</em>{m<em>j}, k</em>{m<em>j}}, M_k{d</em>{m<em>k}, e</em>{m<em>k}, l</em>{m<em>k}, s</em>{m<em>k}, k</em>{m_k}}\).<br>The similarity of the two songs is defined as:</p><script type="math/tex; mode=display">def: Similarity = \frac{1}{\sqrt{(d_{m_j}-d_{m_k})^2+(e_{m_j}-e_{m_K})^2+...+(k_{m_j}-k_{m_k})^2}}\tag{5}</script><h4 id="Similarity-analysis-between-music-genres"><a href="#Similarity-analysis-between-music-genres" class="headerlink" title="Similarity analysis between music genres"></a>Similarity analysis between music genres</h4><p>We selected 12 music genres in the data set to analyze their music similarity. For each genre, we selected 50 artists with the highest influence index, and calculated the music style similarity between these 500 artists. Then the music similarity between different genres is calculated according to the same genre and different genres. The results are shown in the Figure 6 below.</p><p><img src="https://bu.dusays.com/2022/08/29/630c816edccf8.png" alt=""></p><p><font size=3 color="black">Figure 6: Music similarity between different genres</font></p><p>The darker the color in the figure 6 is, the higher the similarity between genres. For example, the similarity between Blues and Pop/Rock,jazz and Country is very high, while the similarity between R&amp;B and Vocal is very low. Similar conclusions can be drawn directly from the figure.</p><p>In order to better distinguish the characteristics of music genres, we use radar chart to intuitively show the prominent characteristics of each genre.The musical characteristics of these 12 genres are shown in the Figure 7.</p><p><img src="https://bu.dusays.com/2022/08/29/630c816fd7999.png" alt=""></p><p><font size=3 color="black">Figure 7: Music characteristics</font></p><h4 id="The-analysis-of-the-factors-that-influence-the-spread-of-music"><a href="#The-analysis-of-the-factors-that-influence-the-spread-of-music" class="headerlink" title="The analysis of the factors that influence the spread of music"></a>The analysis of the factors that influence the spread of music</h4><p>Using the data, the correlation coefficients between 12 indexes and the popularity of music are calculated.</p><p>In this question, we assume that if the absolute value of the correlation coefficient is greater than 0.3, we can think that there is a strong correlation between the data. When the absolute value of the correlation coefficient is between 0.1 and 0.3, we can think that there is a correlation between the data.</p><p>The characteristics of music communication can be reflected from the popularity of music, so from the correlation coefficient, it can be concluded that the communicability of music is positively correlated with the dancebility, energy, loudness and explicit, and negatively correlated with the instrumental, acoustic and other indicators, and is most affected by the dancebility, loudness and acoustic.</p><h3 id="Optimized-Louvain-KMeans-Algorithm-and-NMI"><a href="#Optimized-Louvain-KMeans-Algorithm-and-NMI" class="headerlink" title="Optimized Louvain-KMeans Algorithm and NMI"></a>Optimized Louvain-KMeans Algorithm and NMI</h3><p>Through the heat map of influence and similarity, we can qualitatively observe the relationship between music influence and similarity. </p><p>Next, we innovatively propose the optimized louvain-kmeans algorithm to quantitatively analyze the relationship between music influence and similarity with the help of NMI.The architecture of this model is shown in the figure 8.</p><p><img src="https://bu.dusays.com/2022/08/29/630c8170e57ce.png" alt=""></p><p><font size=3 color="black">Figure 8: Flow chart of the model</font></p><h4 id="The-community-partition-based-on-influence"><a href="#The-community-partition-based-on-influence" class="headerlink" title="The community partition based on influence"></a>The community partition based on influence</h4><p>The Louvain method for community detection is a method to extract communities from large networks created by Blondel et al. from the University of Louvain (the source of this method’s name). The method is a greedy optimization method that appears to run in time \(O(n · log_2n)\) if \(n\) is the number of nodes in the network.We will use the Louvain algorithm to divide the influence network.</p><p>Before using Louvain, we need to introduce the concept of modularity. We use the Louvain algorithm to make as many edges as possible in the community and as few as possible between the communities. The measure of this index is modularity. </p><p>The number of nodes in the network is \(N<em>a\), the number of edges is \(N</em>{infl}\), and the indegree of node \(a<em>i\) is \(idg</em>{a<em>i}\). The adjacency matrix of the network is expressed as A, \(A</em>{a_i,a_j}=0\) means there is no edge between \(a_i\) and \(a_j\). \(AVW = 1\) means there are edges between the two nodes. </p><p>Define variable s, and \(s<em>{a_ia_j}\) means that \(a_i\) and \(a_j\) belong to the same partition. \(s</em>{a<em>ia_j}=-1\) means that the two nodes belong to different partition. Then we can use \(\delta</em>{a<em>ia_j} = 1/2(s</em>{a_ia_j} + 1) \) to verify if \(a_i\) and \(a_j\) belong to the same partition in a quantitive way. If the result equals one, we can say the two nodes belong to the same partition. If not, the result equals zaro. Then the probability expectation of modularity can be expressed as:</p><script type="math/tex; mode=display">\overline{Q} = 1/2\frac{\sum_{a_ia_j}A_{a_ia_j}}{N_{infl}} \delta_{a_ia_j}\tag{6}</script><p>Modularity can be finally expressed as:</p><script type="math/tex; mode=display">Q=\frac{1}{2m}\sum_{a_ia_j}(A_{a_ia_j}-\frac{idg_{a_i}idg_{a_j}}{2N_{infl}})\delta_{a_ia_j}\tag{7}</script><p>Through greedy algorithm, the modularity is continuously optimized to achieve “community partition based on influence”,The schematic diagram of the improved Louvain algorithm is shown in the Figure 9.</p><p><img src="https://bu.dusays.com/2022/08/29/630c8171e043b.png" alt=""></p><p><font size=3 color="black">Figure 9: Optimized louvain algorithm</font></p><p>In the process of optimization, we select the artist with the biggest influence factor among 19 music genres, and prevent them from being divided into a module, so as to improve the efficiency of community partition. The community partition results are shown in the Figure 10.(different colors represent different communities)</p><p><img src="https://bu.dusays.com/2022/08/29/630c81735e2f8.png" alt=""></p><p><font size=3 color="black">Figure 10: The partition based on influence</font></p><h4 id="The-communiy-partition-based-on-similarity"><a href="#The-communiy-partition-based-on-similarity" class="headerlink" title="The communiy partition based on similarity"></a>The communiy partition based on similarity</h4><p>We use KMeans to divide all the artist nodes into communities, so that the music similarity within the community is higher, and the music similarity between the communities is lower. Similar to the Louvain algorithm, we use the artist with the largest influence factor in the same 19 music genres, and prevent them from being divided into a module, so as to improve the efficiency of community partition.The community partition results are shown in the Figure 11(different colors represent different communities)</p><p><img src="https://bu.dusays.com/2022/08/29/630c8175f0dcf.png" alt=""></p><p><font size=3 color="black">Figure 11: The partition based on similarity</font></p><h4 id="The-relationship-between-influence-and-similarity-was-analyzed-through-NMI"><a href="#The-relationship-between-influence-and-similarity-was-analyzed-through-NMI" class="headerlink" title="The relationship between influence and similarity was analyzed through NMI"></a>The relationship between influence and similarity was analyzed through NMI</h4><p>NMI is often used to detect the difference between the results of the partition and the true partition of the network and calculate the correct rate. Here we use the NMI index to evaluate the similarity between the influence-based and similarity-based social partitions.The higher the NMI index is, the more similar the two partitions are, which indicates that the influencers actually affect the music created by the followers.</p><p>In our model,\(P<em>{SC}\) stands for “similarity-based community partition” and \(P</em>{IC}\) for “influence-based community partition”, and the NMI index of these two can be expressed by the following formula:</p><script type="math/tex; mode=display">\mathrm{NMI}=\frac{-2 \sum_{i=1}^{C_{P_{I C}}} \sum_{j=1}^{C_{P_{S C}}} C_{i j} \cdot \log \left(\frac{C_{i j} \cdot N}{C_{i} \cdot C_{j}}\right)}{\sum_{i=1}^{C_{P_{I C}}} C_{i} \cdot \log \left(\frac{C_{i}}{N}\right)+\sum_{j=1}^{C_{P S C}} C_{i j} \cdot \log \left(\frac{C_{j}}{N}\right)}\tag{8}</script><p>where N is the number of nodes, C is a confusion matrix, the element \(C<em>{ij}\) in the matrix indicates the number that the nodes belonging to the community i in the SC partition also belong to communities j in the IC partition. \(P</em>{IC}(P_{SC})\) is the number of communities in IC(SC) partition, \(C_i(C_j)\) is the sum of elements in matrix C. The grater the value of NMI,the more similarity between SC and IC partition, when the NMI value is 1, it indicates that SC and IC are the same partition of the network.</p><p>Finally, the NMI value we calculated is 0.6237, indicating that the influencers actually affect the music created by the followers.</p><h2 id="Model-2-Time-Series-Analysis"><a href="#Model-2-Time-Series-Analysis" class="headerlink" title="Model 2: Time-Series Analysis"></a>Model 2: Time-Series Analysis</h2><p>It is a normal process that music genres emerge, evolve, and disappear. Our team member managed to observe big turns over time, and identify the key revolutionary artist of each genre. Whenever a music genre is about to leap, there will always be clues to change. As for a music genre, it is obvious that the explosive growth in the number of new artists and new songs indicates the prevalence and significant leap of the genre. So our team counted the number of artists and songs in the history of ten major music genres, and found the time of change in the visual image. According to the influence network, the most influential artists in these years were identified as the pioneers of the music revolution, that is, the so-called music revolutionaries. The details are shown below.</p><h3 id="The-evolution-of-genres-over-time"><a href="#The-evolution-of-genres-over-time" class="headerlink" title="The evolution of genres over time"></a>The evolution of genres over time</h3><p>We have studied the evolution of ten genres over time. In the text, we choose Jazz, R&amp;B and Country as three genres to illustrate their evolution over time.</p><h4 id="New-Artists"><a href="#New-Artists" class="headerlink" title="New Artists"></a>New Artists</h4><p>First of all, look at the number of artists added from the genre.According to the data given, the statistical changes of the number of people in the three schools from 1930 to 2010 are shown in Figure 12.</p><p><img src="https://bu.dusays.com/2022/08/29/630c81770320c.png" alt=""></p><p><font size=3 color="black">Figure 12: The number of artists in different musical genres</font></p><p>It can be seen from the figure that jazz, R&amp;B and country all rose in the United States in the 1930s. Jazz flourished from the 1930s to the 1950s, and the number of new artists reached 102 in the 1950s. After the 1950s, jazz began to decline rapidly, and the number continued to decline. By the 2010s, there was no jazz New artists are included in the statistics.</p><p>R&amp;B music developed slowly from 1930’s to 1940’s, and the number of artists increased slowly. However, as time went into 1940’s, R&amp;B music genres developed rapidly, and the number of new artists increased from 17 in 1940 to 104 in 1950 in just 10 years. When R&amp;B developed into 1950s, the number of artists still maintained a steady growth momentum, and reached the peak in 1960 From the 1960s to the 21st century, R&amp;B declined as a whole. The number of new artists has been decreasing except for the growth in the 1980s. In 2010, the number of new artists fell back to the level of the early 1940s.</p><p>The number of artists in the country music genre increased slowly, showing a fluctuating upward trend from 1930 to 1990. The number of new artists reached the peak of 74 in 1990. However, compared with the peak of jazz and R&amp; B genre, the number of artists was still small. After 90 years, the country music genre began to decline, and the number of new artists fell back to the<br>beginning of the genre The number of musicians.</p><h4 id="The-release-of-songs"><a href="#The-release-of-songs" class="headerlink" title="The release of songs"></a>The release of songs</h4><p>In terms of the number of songs released, in the data used, we exclude the songs jointly released by multiple artists, and all the songs included in the calculation are published by artists alone, which can avoid that a song may be released by artists of multiple genres.</p><p>According to the given data, the changes of the number of songs released by the three genres from 1920 to 2020 are counted, as shown in Figure 13.</p><p>Compared with the broken line trend in Figure 1 and Figure 2, we can conclude that the increase of new generation artists will significantly affect the number of songs released, and this effect is often ahead of the increase in the number of songs released, and there is a cumulative effect. Taking jazz music genre as an example, the number of new generation artists in the genre was at a high growth level from 1930 to 1960. In 1950, the number of new generation artists reached its peak. During this period, the genre accumulated a large number of excellent artists. These artists matured in the 1950s and 1960s, and a large number of music works emerged. During this period, the music circulation of jazz music was much higher than that of any other period, reaching the peak of 486 songs in 1957. The same is true of R&amp;B music. In 1960, the number of new generation artists<br>reached its peak, and then in 1972, the number of R&amp;B music released reached its peak of 339.</p><p><img src="https://bu.dusays.com/2022/08/29/630c817814094.png" alt=""></p><p><font size=3 color="black">Figure 13: Total number of songs released each year by different genre</font></p><h4 id="Genre-popularity"><a href="#Genre-popularity" class="headerlink" title="Genre popularity"></a>Genre popularity</h4><p>From the perspective of popularity, we first give the definition of genre popularity. Gene popularity: based on the active-Start, the arithmetic mean value of the popularity of all artists of the same genre is defined as genre popularity.</p><p>For example: jazz music genre, active- There are 45 artists who started in 1930, according to the data table-by-artist.csv We can know the popularity value of artists who meet the conditions, and we can get the popularity of jazz music in 1930 by taking the arithmetic average of these values.</p><p>According to the above definition, we can get the change curve of the three genres from 1930 to<br>2010, as shown in Figure 14.</p><p><img src="https://bu.dusays.com/2022/08/29/630c817923ba5.png" alt=""></p><p><font size=3 color="black">Figure 14: Grnre popularity</font></p><p>From the trend of the line chart, the value of Genre popularity is increasing over time. The reason for the decline of jazz curve from 2000 to 2010 is the lack of 2010 active-Start’s Jazz artist data, so its value was zero in 2010. Because the popularity of songs is calculated by algorithm, and the result largely depends on the total number of tracks played and the time of the most recent track played. Generally speaking, as time goes on, the frequency of playing the track will increase significantly, and it has a higher popularity. Therefore, the popularity of genres will increase with<br>the passage of time.</p><h3 id="The-influence-of-external-factors-on-the-development-trend-of-genres"><a href="#The-influence-of-external-factors-on-the-development-trend-of-genres" class="headerlink" title="The influence of external factors on the development trend of genres"></a>The influence of external factors on the development trend of genres</h3><p>We divide the ideal evolution of music genres into four stages: initial stage, development stage, booming stage and recession stage. We use the number of songs of a genre in a certain period to express the prosperity of the genre in that period. Based on the number curve of 10 genres, we use ARIMA model to fit an ideal evolution curve of genres.</p><p>Without the influence of social environment, scientific and technological development and other factors, genres will evolve according to the ideal curve.</p><p>ARIMA model(Auto regressive Integrated Moving Average model ) is one of the time series prediction methods. In ARIMA (P, D, q), AR is “autoregressive”, P is the number of autoregressive terms; Ma is “moving average”, q is the number of moving average terms, and D is the difference order of making it a stationary sequence.</p><p>The ideal evolution curve of genres fitted by this model is shown in the Figure 15 (the light blue area is the error range). Compared with the ideal evolution curve and the actual evolution curve of music genre, when there is a significant difference between the two in a certain period, it shows that there are social and cultural factors that have a great impact on the genre at this time.</p><p><img src="https://bu.dusays.com/2022/08/29/630c817a0e65b.png" alt=""></p><p><font size=3 color="black">Figure 15: The ideal evolution curve of genres</font></p><p>Taking electronic music as an example, the evolution curve of electronic music and the evolution curve of the genre in the ideal state are shown in the Figure 16.</p><p><img src="https://bu.dusays.com/2022/08/29/630c817ae4b92.png" alt=""></p><p><font size=3 color="black">Figure 16: The evolution curve of electronic</font></p><p>In fact,following the emergence of raving, pirate radios, and an upsurge of interest in club culture. Electronic music achieved widespread mainstream popularity in Europe. Meanwhile, MIDI devices, which has been the musical instrument industry standard interface since the 1980s through to the present day, became commercially available in 1980s.These cultural and technological factors have promoted the rapid development of electronic music.</p><h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>To understand and measure the influence of previously poducted music on new music and musical artists, we proposed a series of novel models to address the sub-issues from creating the influence network based on the similarity between artists. The proposed model achieves high accuracy and robustness.</p><ul><li>We create the directed network based on music influence and use BAmodel to explain how the influence network expend. Thenwe propose Influence-Index to analyze the influence between genres and artists. We select the most 10 influential artists and show their influence-index.</li><li>In order to analyze the similarity between genres, artists and music, we use correlation analysis to remove redundant attributes. We finally select 7 attributes which actually infect the similarity. They are danceability, energy, key, liveness, speechiness, mode and explicit. Based on these attributes, we propes similirity index.</li><li>To find the relationship between the music similarity and the influence, we propose a Optimized LOUVAIN-KMeans Algorithm and use it to community partition. By participating artists into different community through Louvain algorithm and KMeans algorithm, we can obtain 2 different partition. Then we use NMI to estimate the similarity between these 2 partions. Finally we get the conclusion that influencers actually affect the music created by followers.</li><li>We analyze the influence processes of musical evolution that occoured over time. We use ARIMA model to create an ideal evolution curve. By comparing with the ideal evolution curve, we can find how the social, culture and technology affect the music. We give an example about electronic music.</li></ul><h2 id="Strength-and-weaknesses"><a href="#Strength-and-weaknesses" class="headerlink" title="Strength and weaknesses"></a>Strength and weaknesses</h2><h3 id="Strengths"><a href="#Strengths" class="headerlink" title="Strengths"></a>Strengths</h3><p>-We creatively proposed Optimized LOUVAIN-KMeans Algorithm and use the partition to explain the relationship between the influence and music similarity. The community network is as a bridge between them, and the model can reflect the relationship effectively.</p><ul><li>We proposse Influence-Index to estimate the influence of the artists objectively. It’s not accuracy to use only the number of followers or the number of music.</li><li>When analyzing the similarity between music and artists, our model is simple and convenient. Among the 12 music attributes given in the original data set, we remove the redundant attributes through correlation analysis, so that our model can calculate the similarity more efficiently and maintain a higher accuracy.</li><li>We creatively use ARIMA model to generate an ideal evolution curce of the genre. It make our model more robust, and can be used in many other situation. The ideal evolution curve reveal the process of the evolution in these genres.</li></ul><h3 id="Weakness"><a href="#Weakness" class="headerlink" title="Weakness"></a>Weakness</h3><ul><li>We don’t consider the influence between genres when we construct the ideal evolution curve. And new genres will appear at any time, so the curve may not that accuracy.</li><li>The interpretability of the influence and similarity model is not strong. We only find the relationship between similarity and influence. But we don’t know how it operates indetail.</li><li>The amount of data of some minority genres is too small, the prediction result of the model for minority genres is not good.</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;We participated in ICM 2021 during Feb 5th - Feb 9th and finally honored &lt;strong&gt;Meritorious Winner(Top 7%)&lt;/strong&gt;😆. Because we 3 people are all music enthusiasts, we chose the problem D about data mining in music similarity and influence. We thought we could do better in this field. It turns out that we were right. Here is our thinking and solution to this problem.&lt;/p&gt;</summary>
    
    
    
    <category term="Social Network" scheme="https://enblog.crocodilezs.top/categories/Social-Network/"/>
    
    
  </entry>
  
  <entry>
    <title>A Renewable Energy Certificate Trading System Based on Blockchain</title>
    <link href="https://enblog.crocodilezs.top/202110/2021-10-02-REC-trading/"/>
    <id>https://enblog.crocodilezs.top/202110/2021-10-02-REC-trading/</id>
    <published>2020-10-02T09:32:11.000Z</published>
    <updated>2022-08-29T09:23:49.544Z</updated>
    
    <content type="html"><![CDATA[<p>I participated in this project during my internship at State Grid Blockchain Technology (Beijing) Co., LTD. We designed an energy trading system on blockchain.</p><span id="more"></span><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>At present, the cumbersome issuing process of renewable energy certificate (REC) and inflexible pricing mechanism consume a lot of manpower and material resources. In order to solve this problem, this paper proposes <u>a hybrid REC trading system based on Consortium Blockchain</u>. The paper introduces the operation mode of the system in detail and changes the view replacement protocol in the <code>Practical Byzantine Fault Tolerance (PBFT)</code> Algorithm to improve the stability of the system. It also introduces the bidding rules of <code>Continuous Double Auction (CDA)</code> used in the system, and designs the bidding strategies to maximize the user’s profit and the success rate of transaction. Finally, <code>ARIMA</code> model is also used to forecast the price of RECs to provide guidance for both buyers and sellers. </p><ul><li><strong>Index Terms</strong><br>REC transaction, Consortium Blockchain, Continuous Double Auction, ARIMA</li></ul><hr><p><a href="https://ieeexplore.ieee.org/abstract/document/9724399">Link</a><br>Accepted by TrustCom 2021: International Conference on Trust, Security and Privacy in Computing and Communications, Jul.2021.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;I participated in this project during my internship at State Grid Blockchain Technology (Beijing) Co., LTD. We designed an energy trading system on blockchain.&lt;/p&gt;</summary>
    
    
    
    <category term="Blockchain" scheme="https://enblog.crocodilezs.top/categories/Blockchain/"/>
    
    
  </entry>
  
  <entry>
    <title>Sentiment Analysis | Netizen Sentiment Recognition During COVID-19</title>
    <link href="https://enblog.crocodilezs.top/202006/2020-06-18-Netizen-Sentiment/"/>
    <id>https://enblog.crocodilezs.top/202006/2020-06-18-Netizen-Sentiment/</id>
    <published>2020-08-19T14:42:24.000Z</published>
    <updated>2022-08-29T09:24:26.149Z</updated>
    
    <content type="html"><![CDATA[<p>The emergence of the COVID-19 has disrupted our normal life. People’s psychological state can also receive a great negative impact. We chose such a topic with humanistic concern in <code>Data Warehouse and Data Mining</code> course, hoping that through Weibo posts we can gain more insight into people’s emotional state during the COVID-19. We are awarded the best project (1/23) in this course, and we think we could do more in psychological care during COVID-19.</p><span id="more"></span><h2 id="Data-Source-and-Data-Structure"><a href="#Data-Source-and-Data-Structure" class="headerlink" title="Data Source and Data Structure"></a>Data Source and Data Structure</h2><p>This data comes from a data mining competition organized by DataFountain. The data set is a collection of Weibo crawled during the COVID-19. The link to the dataset is <a href="https://www.datafountain.cn/competitions/423/datasets">here</a>.</p><blockquote><p>The dataset cannot be downloaded directly from the official website because the competition has ended, the dataset can also be found on <a href="https://www.kaggle.com/liangqingyuan/chinese-text-multi-classification?select=nCoV_100k_train.labled.csv">Kaggle</a>.</p></blockquote><p>We used 100,000 posts from the training set provided by the competition website, but considering the time and computational cost, I only used the first 10,000 posts with annotations, and divided them into a training set and a test set in a 7/3 ratio. This dataset was based on 230 keywords related to the topic of “新冠肺炎”, which means COVID-19 in Chinese. </p><p>1,000,000 Weibo posts were collected from January 1, 2020 to February 20, 2020, and 100,000 Weibo posts were labeled with three categories: 1 (positive), 0 (neutral) and -1 (negative). The data is stored in csv format in <code>nCoV-100k.labeled.csv</code> file. The original dataset contains 100,000 user-labeled posts in the following format: <code>[post id, posting time, posting account, content, photos, videos, sentiment]</code>.</p><p>The original dataset has six attributes: <code>post id</code> (hashcode), <code>posting time</code> (Date), <code>posting account</code> (String), <code>content</code> (String), <code>photos</code> (String), and <code>videos</code> (String). Predicting <code>sentiment</code>(Int) by the above attributes. The purpose of this project is to use <strong>word bag preprocessing</strong>, <strong>TF-IDF preprocessing</strong>, <strong>word2vec</strong> and compare their effects, focusing on text processing and text sentiment analysis. So we only chose <code>content</code> attribute to predict the sentiment. (It was hard for us to read the emotion from photos and videos.)</p><p>Here is the statistical information of the dataset from Kaggle.</p><p><img src="https://bu.dusays.com/2022/08/29/630c7e2449592.png" alt=""></p><p>The bar chart shows that the number of positive, neutral and negative posts varies considerably, with the highest number of neutral posts.</p><p>Here is the first post in dataset.</p><p><img src="https://bu.dusays.com/2022/08/29/630c7e2554514.png" alt=""></p><p>We found this posts in Weibo APP.</p><p><img src="https://bu.dusays.com/2022/08/29/630c7e12c779c.png" alt=""></p><h2 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h2><p>We Used Kaggle Kernel. Kaggle provides free access to the Nvidia K80 GPU in the kernel. This benchmark shows that using the GPU for your kernel can achieve a 12.5x speedup in the training of deep learning models.</p><h3 id="Data-import-and-turncut"><a href="#Data-import-and-turncut" class="headerlink" title="Data import and turncut"></a>Data import and turncut</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd filepath = <span class="string">&#x27;/kaggle/input/chinese-text-multi-classification/nCoV_100k_train.labled.cs v&#x27;</span> file_data = pd.read_csv(filepath)</span><br><span class="line">data = file_data.head(<span class="number">10000</span>)</span><br><span class="line"><span class="comment"># chose content and sentiment</span></span><br><span class="line">data = data[[<span class="string">&#x27;微博中⽂内容&#x27;</span>, <span class="string">&#x27;情感倾向&#x27;</span>]]</span><br></pre></td></tr></table></figure><h3 id="Handling-missing-values"><a href="#Handling-missing-values" class="headerlink" title="Handling missing values"></a>Handling missing values</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># handling missing values</span></span><br><span class="line">data.isnull().<span class="built_in">sum</span>()</span><br><span class="line">data = data.dropna()</span><br></pre></td></tr></table></figure><h3 id="Remove-meaningless-symbols"><a href="#Remove-meaningless-symbols" class="headerlink" title="Remove meaningless symbols"></a>Remove meaningless symbols</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clean_zh_text</span>(<span class="params">text</span>): </span><br><span class="line">    <span class="comment"># keep English, digital and Chinese </span></span><br><span class="line">    comp = re.<span class="built_in">compile</span>(<span class="string">&#x27;[^A-Z^a-z^0-9^\u4e00-\u9fa5]&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> comp.sub(<span class="string">&#x27;&#x27;</span>, text)</span><br><span class="line">data[<span class="string">&#x27;微博中⽂内容&#x27;</span>] = data.微博中⽂内容.apply(clean_zh_text)</span><br></pre></td></tr></table></figure><h3 id="Word-Cut"><a href="#Word-Cut" class="headerlink" title="Word Cut"></a>Word Cut</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># word cut </span></span><br><span class="line"><span class="keyword">import</span> jieba </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chinese_word_cut</span>(<span class="params">mytext</span>): </span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot; &quot;</span>.join(jieba.cut(mytext))</span><br><span class="line"></span><br><span class="line">data[<span class="string">&#x27;cut_comment&#x27;</span>] = data.微博中⽂内容.apply(chinese_word_cut)</span><br><span class="line"></span><br><span class="line"><span class="comment"># divided them into a training set and a test set in a 7/3 ratio.</span></span><br><span class="line">lentrain = <span class="built_in">int</span>((<span class="number">10000</span>-<span class="number">30</span>)*<span class="number">0.7</span>) </span><br><span class="line">lentest = <span class="built_in">int</span>((<span class="number">10000</span>-<span class="number">30</span>)*<span class="number">0.3</span>) </span><br><span class="line">x_train = data.head(lentrain)[<span class="string">&#x27;微博中⽂内容&#x27;</span>] </span><br><span class="line">y_train = data.head(lentrain)[<span class="string">&#x27;情感倾向&#x27;</span>] </span><br><span class="line">x_test = data.tail(lentest)[<span class="string">&#x27;微博中⽂内容&#x27;</span>]</span><br><span class="line">y_test = data.tail(lentest)[<span class="string">&#x27;情感倾向&#x27;</span>]</span><br></pre></td></tr></table></figure><p><img src="https://bu.dusays.com/2022/08/29/630c7e12a19c0.png" alt=""></p><p><img src="https://bu.dusays.com/2022/08/29/630c7e13823e5.png" alt=""></p><h3 id="Import-Stop-Words"><a href="#Import-Stop-Words" class="headerlink" title="Import Stop Words"></a>Import Stop Words</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import stop words</span></span><br><span class="line">stpwrdpath = <span class="string">&quot;/kaggle/input/stop-wordstxt/stop_words.txt&quot;</span> </span><br><span class="line">stpwrd_dic = <span class="built_in">open</span>(stpwrdpath, <span class="string">&#x27;rb&#x27;</span>) </span><br><span class="line">stpwrd_content = stpwrd_dic.read() </span><br><span class="line"></span><br><span class="line"><span class="comment">#transform into list </span></span><br><span class="line">stpwrdlst = stpwrd_content.splitlines()</span><br><span class="line">stpwrd_dic.close()</span><br></pre></td></tr></table></figure><h3 id="Word-Bag-Preprocess"><a href="#Word-Bag-Preprocess" class="headerlink" title="Word Bag Preprocess"></a>Word Bag Preprocess</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text </span><br><span class="line"><span class="keyword">import</span> CountVectorizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># CountVectorizer initialize</span></span><br><span class="line">count_vec = CountVectorizer(stop_words=stpwrdlst) </span><br><span class="line">x_train_list = x_train.tolist() </span><br><span class="line">x_train_cv = count_vec.fit_transform(x_train_list).toarray() </span><br><span class="line">x_test_list = x_test.tolist()</span><br><span class="line">x_test_cv = count_vec.fit_transform(x_test_list).toarray()</span><br></pre></td></tr></table></figure><p><img src="https://bu.dusays.com/2022/08/29/630c7e14a5617.png" alt=""></p><h3 id="TF-IDF-Preprocess"><a href="#TF-IDF-Preprocess" class="headerlink" title="TF-IDF Preprocess"></a>TF-IDF Preprocess</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer </span><br><span class="line">tfidf_vec = TfidfVectorizer(token_pattern=<span class="string">r&quot;(?u)\b\w+\b&quot;</span>, max_df=<span class="number">0.6</span>, stop_words=stpwr dlst) </span><br><span class="line">x_train_tiv = tfidf_vec.fit_transform(x_train_list).toarray()</span><br><span class="line">x_test_tiv = tfidf_vec.fit_transform(x_test_list).toarray()</span><br></pre></td></tr></table></figure><p><img src="https://bu.dusays.com/2022/08/29/630c7e1594d84.png" alt=""></p><h3 id="word2vec-embedding"><a href="#word2vec-embedding" class="headerlink" title="word2vec embedding"></a>word2vec embedding</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># word2vec </span></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec </span><br><span class="line">model = Word2Vec(x_train_list, hs=<span class="number">1</span>,min_count=<span class="number">1</span>,window=<span class="number">10</span>,size=<span class="number">100</span>)</span><br><span class="line"><span class="keyword">from</span> gensim.test.utils <span class="keyword">import</span> common_texts, get_tmpfile </span><br><span class="line">path = get_tmpfile(<span class="string">&quot;word2vec.model&quot;</span>) </span><br><span class="line">model.save(<span class="string">&quot;word2vec.model&quot;</span>)</span><br><span class="line"><span class="comment"># model = Word2Vec.load(&quot;word2vec.model&quot;)</span></span><br></pre></td></tr></table></figure><p><img src="https://bu.dusays.com/2022/08/29/630c7e1684769.png" alt=""></p><p>The corpus of 10,000 training data is still a bit small, but the results are slightly more productive. For example, if we look at the close synonyms of “开心”(happy), we can see that the results returned are more positive.</p><p><img src="https://bu.dusays.com/2022/08/29/630c7e1795f3a.png" alt=""></p><h2 id="Data-Mining-Algorithm"><a href="#Data-Mining-Algorithm" class="headerlink" title="Data Mining Algorithm"></a>Data Mining Algorithm</h2><p>In this section, we will use <code>SVM</code>, <code>decision tree</code> and <code>RNN</code> algorithms to achieve classification. The embedding obtained by <code>BOW</code> and <code>TF-IDF</code> will be classified by SVM and decision tree algorithms, respectively, while the embedding obtained by Word2Vec will be classified by RNN.</p><p>The embedding obtained from Word2Vec will be classified using RNN. The detailed algorithm flow is as follows.</p><p><img src="https://bu.dusays.com/2022/08/29/630c7e187204c.png" alt=""></p><h3 id="BOW-SVM"><a href="#BOW-SVM" class="headerlink" title="BOW + SVM"></a><code>BOW</code> + <code>SVM</code></h3><p>The 35596-dimensional embedding of the Weibo posts obtained by <code>BOW</code> is used as the input of the <code>SVM</code> in the <code>sklearn</code> package.<br>The parameters are as follows.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf = svm.SVC(C=<span class="number">0.8</span>, kernel=<span class="string">&#x27;rbf&#x27;</span>, gamma=<span class="number">20</span>, decision_function_shape=<span class="string">&#x27;ovr&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler </span><br><span class="line">scale = StandardScaler() </span><br><span class="line">scale_fit = scale.fit(x_cv) </span><br><span class="line"><span class="comment">#x = scale_fit.transform(x) </span></span><br><span class="line">lentrain = <span class="built_in">int</span>((<span class="number">10000</span>-<span class="number">30</span>)*<span class="number">0.7</span>) </span><br><span class="line">lentest = <span class="built_in">int</span>((<span class="number">10000</span>-<span class="number">30</span>)*<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">x_train_cv = scale_fit.transform(x_cv[:lentrain]) </span><br><span class="line">y_train = y[:lentrain] </span><br><span class="line">x_test_cv = scale_fit.transform(x_cv[(-<span class="number">1</span>)*lentest-<span class="number">1</span>:-<span class="number">1</span>]) </span><br><span class="line">y_test = y[(-<span class="number">1</span>)*lentest-<span class="number">1</span>:-<span class="number">1</span>] </span><br><span class="line"><span class="built_in">print</span>(x_train_cv.shape) </span><br><span class="line"><span class="built_in">print</span>(y_train.shape) </span><br><span class="line"><span class="built_in">print</span>(x_test_cv.shape)</span><br><span class="line"><span class="built_in">print</span>(y_test.shape)</span><br></pre></td></tr></table></figure><p><strong>TIPS:</strong></p><ul><li><code>SVM</code> and <code>Decision Tree</code> algorithms for 10,000 of 30,000-dimensional data can take a lot of time, and sklearn does not support GPU computing.</li><li>When you encounter a very large dataset, you should first use a small demo to check the correctness of the code, and then run a large demo with a large amount of data.</li><li><code>BOW</code> and <code>TF-IDF</code> should be used first before dividing the test and training sets, otherwise the test and training sets will not have the same dimensionality! It took a lot of time to fix this error.</li><li>Due to the excessive number of dimensions, remember to normalize the data before <code>SVM</code>.</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># prepare the data</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">scale = StandardScaler()</span><br><span class="line">scale_fit = scale.fit(x_cv)</span><br><span class="line"><span class="comment">#x = scale_fit.transform(x)</span></span><br><span class="line">lentrain = <span class="built_in">int</span>((<span class="number">10000</span>-<span class="number">30</span>)*<span class="number">0.7</span>)</span><br><span class="line">lentest = <span class="built_in">int</span>((<span class="number">10000</span>-<span class="number">30</span>)*<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">x_train_cv = scale_fit.transform(x_cv[:lentrain])</span><br><span class="line">y_train = y[:lentrain]</span><br><span class="line">x_test_cv = scale_fit.transform(x_cv[(-<span class="number">1</span>)*lentest-<span class="number">1</span>:-<span class="number">1</span>])</span><br><span class="line">y_test = y[(-<span class="number">1</span>)*lentest-<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(x_train_cv.shape)</span><br><span class="line"><span class="built_in">print</span>(y_train.shape)</span><br><span class="line"><span class="built_in">print</span>(x_test_cv.shape)</span><br><span class="line"><span class="built_in">print</span>(y_test.shape)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#test bow+svm</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x_train_cv.shape, <span class="string">&#x27;and &#x27;</span>, y_train.shape)</span><br><span class="line">clf = svm.SVC(C=<span class="number">0.8</span>, kernel=<span class="string">&#x27;rbf&#x27;</span>, gamma=<span class="number">20</span>, decision_function_shape=<span class="string">&#x27;ovr&#x27;</span>)</span><br><span class="line">cv_model = clf.fit(x_train_cv, y_train)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score, f1_score, accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Precision_score: &#x27;</span>, precision_score(y_hat_cv, y_test, average=<span class="string">&#x27;weighted&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Recall_score: &#x27;</span>, recall_score(y_hat_cv, y_test, average=<span class="string">&#x27;weighted&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;F1_score: &#x27;</span>, f1_score(y_hat_cv, y_test, average=<span class="string">&#x27;weighted&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy_score: &#x27;</span>, accuracy_score(y_hat_cv, y_test))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># roc_curve:真正率（True Positive Rate , TPR）或灵敏度（sensitivity）</span></span><br><span class="line"><span class="comment"># 横坐标：假正率（False Positive Rate , FPR）</span></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> interp</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> label_binarize</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> cycle</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve, auc</span><br><span class="line"></span><br><span class="line">nb_classes = <span class="number">3</span></span><br><span class="line"><span class="comment"># Binarize the output</span></span><br><span class="line">Y_valid = label_binarize(y_test, classes=[i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_classes)])</span><br><span class="line">Y_pred = label_binarize(y_hat_cv, classes=[i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_classes)])</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="comment"># Compute ROC curve and ROC area for each class</span></span><br><span class="line">fpr = <span class="built_in">dict</span>()</span><br><span class="line">tpr = <span class="built_in">dict</span>()</span><br><span class="line">roc_auc = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_classes):</span><br><span class="line">    fpr[i], tpr[i], _ = roc_curve(Y_valid[:, i], Y_pred[:, i])</span><br><span class="line">    roc_auc[i] = auc(fpr[i], tpr[i])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute micro-average ROC curve and ROC area</span></span><br><span class="line">fpr[<span class="string">&quot;micro&quot;</span>], tpr[<span class="string">&quot;micro&quot;</span>], _ = roc_curve(Y_valid.ravel(), Y_pred.ravel())</span><br><span class="line">roc_auc[<span class="string">&quot;micro&quot;</span>] = auc(fpr[<span class="string">&quot;micro&quot;</span>], tpr[<span class="string">&quot;micro&quot;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute macro-average ROC curve and ROC area</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># First aggregate all false positive rates</span></span><br><span class="line">all_fpr = np.unique(np.concatenate([fpr[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_classes)]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Then interpolate all ROC curves at this points</span></span><br><span class="line">mean_tpr = np.zeros_like(all_fpr)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_classes):</span><br><span class="line">    mean_tpr += interp(all_fpr, fpr[i], tpr[i])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Finally average it and compute AUC</span></span><br><span class="line">mean_tpr /= nb_classes</span><br><span class="line"></span><br><span class="line">fpr[<span class="string">&quot;macro&quot;</span>] = all_fpr</span><br><span class="line">tpr[<span class="string">&quot;macro&quot;</span>] = mean_tpr</span><br><span class="line">roc_auc[<span class="string">&quot;macro&quot;</span>] = auc(fpr[<span class="string">&quot;macro&quot;</span>], tpr[<span class="string">&quot;macro&quot;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot all ROC curves</span></span><br><span class="line">lw = <span class="number">2</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(fpr[<span class="string">&quot;micro&quot;</span>], tpr[<span class="string">&quot;micro&quot;</span>],</span><br><span class="line">         label=<span class="string">&#x27;micro-average ROC curve (area = &#123;0:0.2f&#125;)&#x27;</span></span><br><span class="line">               <span class="string">&#x27;&#x27;</span>.<span class="built_in">format</span>(roc_auc[<span class="string">&quot;micro&quot;</span>]),</span><br><span class="line">         color=<span class="string">&#x27;deeppink&#x27;</span>, linestyle=<span class="string">&#x27;:&#x27;</span>, linewidth=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(fpr[<span class="string">&quot;macro&quot;</span>], tpr[<span class="string">&quot;macro&quot;</span>],</span><br><span class="line">         label=<span class="string">&#x27;macro-average ROC curve (area = &#123;0:0.2f&#125;)&#x27;</span></span><br><span class="line">               <span class="string">&#x27;&#x27;</span>.<span class="built_in">format</span>(roc_auc[<span class="string">&quot;macro&quot;</span>]),</span><br><span class="line">         color=<span class="string">&#x27;navy&#x27;</span>, linestyle=<span class="string">&#x27;:&#x27;</span>, linewidth=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">colors = cycle([<span class="string">&#x27;aqua&#x27;</span>, <span class="string">&#x27;darkorange&#x27;</span>, <span class="string">&#x27;cornflowerblue&#x27;</span>])</span><br><span class="line"><span class="keyword">for</span> i, color <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(nb_classes), colors):</span><br><span class="line">    plt.plot(fpr[i], tpr[i], color=color, lw=lw,</span><br><span class="line">             label=<span class="string">&#x27;ROC curve of class &#123;0&#125; (area = &#123;1:0.2f&#125;)&#x27;</span></span><br><span class="line">             <span class="string">&#x27;&#x27;</span>.<span class="built_in">format</span>(i, roc_auc[i]))</span><br><span class="line"></span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">&#x27;k--&#x27;</span>, lw=lw)</span><br><span class="line">plt.xlim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.ylim([<span class="number">0.0</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;False Positive Rate&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;True Positive Rate&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Some extension of Receiver operating characteristic to multi-class&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;lower right&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>Here’s the result:</p><p><img src="https://bu.dusays.com/2022/08/29/630c7e194b1c6.png" alt=""></p><p><img src="https://bu.dusays.com/2022/08/29/630c7e1a3dde4.png" alt=""></p><p>The reason for the coincidence of the ROC curve and the X-axis is that most of the predictions are zero. The reasons are as follows: </p><ol><li>The two embedding methods, <code>BOW</code> and <code>TF-IDF</code>, do not work well as <code>SVM</code>, and even after normalizing the input word (frequency) vector matrix, most of the predictions are still the same. </li><li>The number of “neutral” labels in the sample is much higher than the number of “positive” and “negative” labels, which is also a problem in the sample selection process.</li><li>The parameters of <code>SVM</code> can be adjusted more precisely to make the classification better.</li></ol><p>Instead of further tuning this model, we tried other algorithms first.</p><h3 id="TF-IDF-SVM"><a href="#TF-IDF-SVM" class="headerlink" title="TF-IDF+SVM"></a><code>TF-IDF</code>+<code>SVM</code></h3><p>The 38473-dimensional embedding of the <code>TF-IDF</code> derived posts is used as the input to the <code>SVM</code> in the sklearn package.</p><p>The source code is similar to the model above, so I will not repeat it here. The results are shown below.</p><p><img src="https://bu.dusays.com/2022/08/29/630c7e1b2ae9b.png" alt=""></p><p>The parameters are as follows.<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf = svm.SVC(C=<span class="number">0.8</span>, kernel=<span class="string">&#x27;rbf&#x27;</span>, gamma=<span class="number">20</span>, decision_function_shape=<span class="string">&#x27;ovr&#x27;</span>)</span><br></pre></td></tr></table></figure></p><h3 id="BOW-Decision-Tree-amp-amp-TF-IDF-Decision-Tree"><a href="#BOW-Decision-Tree-amp-amp-TF-IDF-Decision-Tree" class="headerlink" title="BOW + Decision Tree &amp;&amp; TF-IDF + Decision Tree"></a><code>BOW</code> + <code>Decision Tree</code> &amp;&amp; <code>TF-IDF</code> + <code>Decision Tree</code></h3><p>The source code is similar to the model above, so I will not repeat it here.</p><p>Here’s the result for <code>BOW</code> + <code>Decision Tree</code>.</p><p><img src="https://bu.dusays.com/2022/08/29/630c7e1c1d200.png" alt=""></p><p>Here’s the result for <code>TF-IDF</code> + <code>Decision Tree</code></p><p><img src="https://bu.dusays.com/2022/08/29/630c7e1cef459.png" alt=""></p><h3 id="Word2Vec-RNN"><a href="#Word2Vec-RNN" class="headerlink" title="Word2Vec + RNN"></a><code>Word2Vec</code> + <code>RNN</code></h3><p>Embedding Weibo content using Word2Vec, then the resulting 400-dimensional vector is fed into a 10<em>200</em>1 recurrent neural grid with one hidden layer.</p><p>Parameters:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">100</span> </span><br><span class="line">n_iters = <span class="number">20000</span> </span><br><span class="line">seq_dim = <span class="number">20</span> </span><br><span class="line">input_dim = <span class="number">20</span> <span class="comment"># input dimension</span></span><br><span class="line">hidden_dim = <span class="number">200</span> <span class="comment"># hidden layer dimension </span></span><br><span class="line">layer_dim = <span class="number">1</span> <span class="comment"># number of hidden layers</span></span><br><span class="line">output_dim = <span class="number">3</span> <span class="comment"># output dimension</span></span><br></pre></td></tr></table></figure></p><p>So far we have obtained the embedding of all words, the key problem is how to represent the sentences. I referred to the information below and chose to try it with <code>word2vec</code> using Gensim.</p><p>At the first time we try, there is exploding gradient.</p><p><img src="https://bu.dusays.com/2022/08/29/630c7e1e7d6e7.png" alt=""></p><p>After adjusting the learning rate to 0.03, the results after 60,000 generations of training are shown below.</p><p><img src="https://bu.dusays.com/2022/08/29/630c7e1f6e018.png" alt=""></p><p><img src="https://bu.dusays.com/2022/08/29/630c7e2062341.png" alt=""></p><p>After adjusting hidden layers, the results after 20,000 generations are shown below.</p><p><img src="https://bu.dusays.com/2022/08/29/630c7e2146e9a.png" alt=""></p><p><img src="https://bu.dusays.com/2022/08/29/630c7e2253c14.png" alt=""></p><h2 id="Analysis-of-Results"><a href="#Analysis-of-Results" class="headerlink" title="Analysis of Results"></a>Analysis of Results</h2><p>This is a triple classification problem on the emotion of NLP. The results are shown as below.</p><p><img src="https://bu.dusays.com/2022/08/29/630c7e23458f0.png" alt=""></p><p>As can be seen from the above graphs, the <code>SVM</code> and <code>Decision Tree</code> algorithms have very little impact on the actual results, and the most important factor affecting the prediction results is the Embedding method. In this dataset, <code>TF-IDF</code> is more effective than <code>BOW</code>. In the end, the results of <code>TF-IDF</code>+<code>SVM</code>, <code>TF-IDF</code>+<code>Decision Tree</code> and <code>Word2Vec</code>+<code>RNN</code> are similar. The reasons for this result are as follows: </p><ol><li>The original dataset is not evenly distributed, and there are more “neutral” comments than “positive” and “negative” posts, so in the predictive classification process, most of the postss are not evenly distributed. The majority of posts tend to be classified as “neutral” in the prediction classification process, which is of course consistent with the actual situation. This is why the Embedding method has a greater impact on the results than the classification method.</li><li>The dataset is not large enough. I thought that a data set of about 10,000 would take a lot of training time, but Kaggle can use GPUs and the CPU speed of Kaggle is not slow, so I could have done it directly with the original data set of 10W, and the result would be better.</li><li>Parameter optimization. It is only fair to use the optimal parameters of each model for comparison of results.</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;The emergence of the COVID-19 has disrupted our normal life. People’s psychological state can also receive a great negative impact. We chose such a topic with humanistic concern in &lt;code&gt;Data Warehouse and Data Mining&lt;/code&gt; course, hoping that through Weibo posts we can gain more insight into people’s emotional state during the COVID-19. We are awarded the best project (1/23) in this course, and we think we could do more in psychological care during COVID-19.&lt;/p&gt;</summary>
    
    
    
    <category term="Natural Language Processing" scheme="https://enblog.crocodilezs.top/categories/Natural-Language-Processing/"/>
    
    <category term="Sentiment Analysis" scheme="https://enblog.crocodilezs.top/categories/Natural-Language-Processing/Sentiment-Analysis/"/>
    
    
    <category term="SVM" scheme="https://enblog.crocodilezs.top/tags/SVM/"/>
    
    <category term="Decision Tree" scheme="https://enblog.crocodilezs.top/tags/Decision-Tree/"/>
    
    <category term="RNN" scheme="https://enblog.crocodilezs.top/tags/RNN/"/>
    
    <category term="TF-IDF" scheme="https://enblog.crocodilezs.top/tags/TF-IDF/"/>
    
    <category term="Word2Vec" scheme="https://enblog.crocodilezs.top/tags/Word2Vec/"/>
    
  </entry>
  
  <entry>
    <title>MCM 2020 | Sentiment Analysis | Find the relationship between review and star ranking</title>
    <link href="https://enblog.crocodilezs.top/202003/2020-03-10-MCM-Grocery/"/>
    <id>https://enblog.crocodilezs.top/202003/2020-03-10-MCM-Grocery/</id>
    <published>2020-03-10T02:24:24.000Z</published>
    <updated>2022-08-29T09:32:32.572Z</updated>
    
    <content type="html"><![CDATA[<p>This is my second time for MCM &amp; ICM. I teamed up with two friends. This is how we divide the work: one of my friends collecting information and the other writing the essay. I was responsible for modeling and coding. We finally got <strong>Meritorious Winner(Top 5%)</strong>. I’m so proud of it.</p><span id="more"></span><h2 id="Topic-amp-Dataset"><a href="#Topic-amp-Dataset" class="headerlink" title="Topic &amp; Dataset"></a>Topic &amp; Dataset</h2><details class="tag-plugin folding" color="blue"><summary><span>Topic</span></summary><div class="body"><p>In the online marketplace it created, Amazon provides customers with an opportunity to rate and review purchases. Individual ratings - called “star ratings” - allow purchasers to express their level of satisfaction with a product using a scale of 1 (low rated, low satisfaction) to 5 (highly rated, high satisfaction). Additionally, customers can submit text-based messages - called “reviews” - that express further opinions and information about the product. Other customers can submit ratings on these reviews as being helpful or not - called a “helpfulness rating” - towards assisting their own product purchasing decision. Companies use these data to gain insights into the markets in which they participate, the timing of that participation, and the potential success of product design feature choices.</p><p>Sunshine Company is planning to introduce and sell three new products in the online marketplace: a microwave oven, a baby pacifier, and a hair dryer. They have hired your team as consultants to identify key patterns, relationships, measures, and parameters in past customer-supplied ratings and reviews associated with other competing products to 1) inform their online sales strategy and 2) identify potentially important design features that would enhance product desirability. Sunshine Company has used data to inform sales strategies in the past, but they have not previously used this particular combination and type of data. Of particular interest to Sunshine Companyare time-based patterns in these data, and whether they interact in ways that will help the company craft successful products.</p><p>To assist you, Sunshine’s data center has provided you with three data files for this project: hair_dryer.tsv, microwave.tsv, and pacifier.tsv. These data represent customer-supplied ratings and reviews for microwave ovens, baby pacifiers, and hair dryers sold in the Amazon marketplace over the time period(s) indicated in the data. A glossary of data label definitions is provided as well. </p></div></details><details class="tag-plugin folding" color="green"><summary><span>Dataset</span></summary><div class="body"><p>The three data sets provided contain product user ratings and reviews extracted from the Amazon Customer Reviews Dataset thru Amazon Simple Storage Service (Amazon S3).</p><ul><li><code>hair_dryer.tsv</code></li><li><code>microwave.tsv</code></li><li><code>pacifier.tsv</code></li></ul><p>Data Set Definitions: Each row represents data partitioned into the following columns.</p><ul><li>marketplace (string): 2 letter country code of the marketplace where the review was written.</li><li>customer_id (string): Random identifier that can be used to aggregate reviews written by a single author.</li><li>review_id (string): The unique ID of the review.</li><li>product_id (string): The unique Product ID the review pertains to.</li><li>product_parent (string): Random identifier that can be used to aggregate reviews for the same product.</li><li>product_title (string): Title of the product.</li><li>product_category (string): The major consumer category for the product.</li><li>star_rating (int): The 1-5 star rating of the review.</li><li>helpful_votes (int): Number of helpful votes.</li><li>total_votes (int): Number of total votes the review received.</li><li>vine (string): Customers are invited to become Amazon Vine Voices based on the trust that they have earned in the Amazon community for writing accurate and insightful reviews. Amazon provides Amazon Vine members with free copies of products that have been submitted to the program by vendors. Amazon doesn’t influence the opinions of Amazon Vine members, nor do they modify or edit reviews.</li><li>verified_purchase (string): A “Y” indicates Amazon verified that the person writing the review purchased the product at - Amazon and didn’t receive the product at a deep discount.</li><li>review_headline (string): The title of the review.</li><li>review_body (string): The review text.</li><li>review_date (bigint): The date the review was written.</li></ul></div></details><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><strong>Through data mining and modeling, we analyze the three types of products sales and provide a marketing strategy for Sunshine Company.</strong></p><p>First of all, we preprocess the data. We fully analyze the 15 attributes in microwaves, pacifiers and hair dryers, filter them and syntheze three new attributes: review_text, popularity and reputation. We deal with missing values in the data set and remove those meaningless reviews. Through the <u>Tokenization algorithm</u>, we cut the sentence of the review headline and the review body into single words, which is convenient for us to analyze the emotion of the user and understand the relationship between reviews and star ratings. </p><p>Furthermore, we use the <u>TF-IDF(Term FrequencyInverse Document Frequency) algorithm</u> and <u>MLP(Multi-layer Perceptron)</u> to build a model and try to discover the relationship between reviews and star ratings. We use reviews to evaluate the products and use helpful votes to evaluate reviews, through this way we build a multi-dimension evaluation system. The TF-IDF algorithm uses the segmented words to generate a word frequency matrix, which is used as the input of the MLP. And the products’ star ratings are used as the output. We build models for the three types of products respectively, for the reviews of the three types of products have some differences, which may affect the accuracy of the results. The data set is divided into training set and test set. When the model training is completed, the effect of the model is tested with the test set. After the practice, our prediction is very similar to the ground truth. Then we get the relationship between star ratings and reviews. The traditional MLP model has poor interpretability, so we continue to do <u>semantic analysis</u> on this basis, which enhances the interpretability of the whole model.</p><p>Eventually , we analyze the time-based popularity changes of the product, and make marketing suggestions to the Sunshine Company from the customer’s perspective. Through analysis, we found that the products’ popularity always reach the highest at the beginning and middle of each year, so the company could make promotional activities at that time to raise their profile. In this way, we found the relationship between period and popularity. From the perspective of customers, we observe that those users who score 5 stars or 1 star often quite emotional when giving reviews. We recommend three types of the most valuable users and select their reviews for the sunshine company. They are AmazonVine Voices Members, users who have purchased multiple similar products and users who have given one-star rate to the product. By analyzing the WordCloud of reviews of those who have given one-star rates, we can know how to improve the products.</p><h2 id="Preparation-for-the-modeling"><a href="#Preparation-for-the-modeling" class="headerlink" title="Preparation for the modeling"></a>Preparation for the modeling</h2><p>For the data mining problems that have large quantity and types of data, there are often a large number of default values, which may affect the efficiency and accuracy of the model. Therefore, the processing of these default values is of vital importance. In addition, in this data set, there is a large amount of text information in attributes such as prouct_title, and review_headline. Thus our team choose the tokenization algorithm to classify and organize the text information.</p><h3 id="Default-Value-Preprocessing"><a href="#Default-Value-Preprocessing" class="headerlink" title="Default Value Preprocessing"></a>Default Value Preprocessing</h3><p>Through our analysis, in all the given data sets, only the review_headline and review_body attributes are default. The Amazon website stipulates that when a review is submitted, its star_rating, review_headline and review_body must be filled in, otherwise the review cannot be submitted. so the lack of data is not caused by user behavior, but created during the collection, transmission or storage of these reviews. Therefore, the “default” here does not contain customers opinions towards the product.<br>In this case our team deal with the default records in this way.</p><ul><li>When only one of the review_headline or review_body is default, we will keep this data. Because it still contains a large amount of information;</li><li>When both of the review_headline and review_body are default, We will abandon this data. Because review is an essential part of our following analysis, in this case when both of them are default, this data is of little significance for data<br>mining.</li></ul><h3 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h3><p>The main goal of Tokenization is to normalize our texts, that is, to process a paragraph of text into short, non-redundant text information. Our tokenization process<br>includes the following steps:</p><p>a. Lowercase the values.<br>Because the text_market, product_title and other text attributes have a kind of phenomenon that some values with the same meaning have different capitalization. For example, the marketplace attribute contains ’US’ and ’us’, which is not convenient for our subsequent classification of products. Therefore, we convert all text to lower<br>case.<br>b. Break the sentences into token<br>c. Remove punctuation and stop words<br>In computing, stop words are words which are filtered out before or after processing of natural language data (text).</p><p>We give an example in Figure 1.</p><p><img src="https://bu.dusays.com/2022/08/29/630c6ec8b8d08.png" alt=""></p><p><font size=3 color="black">Figure 1: An example about Tokenization in Microwave Product Data Set</font></p><h3 id="Data-Selection-and-Synthesis"><a href="#Data-Selection-and-Synthesis" class="headerlink" title="Data Selection and Synthesis"></a>Data Selection and Synthesis</h3><p>In these 15 attributes that come from the provided data sets, some of them are not valuable. So we need to select the relatively more essential attributes. For the marketplace attribute, all evaluations in the dataset are from the US, so there is no reason to do data mining on this attribute, similarly in product_cetegory. Review_id is only used to distinguish different reviews, so we delete their corresponding data as well.</p><p>Before introducing product_id and product_parent, we need to learn more about parent-child relationships. </p><p>Each parent product may contain multiple child products, and each child parents may be different in sizes, colors, or prices. Each parent product corresponds to a product_parent, and each child product corresponds to a product_id. In the following data analysis, we mainly analyze the parent product and inspect these three types of products from a macro level. Take the microwave as an example, the parent-child product relationship as shown below. </p><p>In Figure 2 , we can find the four child products have different product_ids and they may have different sizes, colors or prices. However, they have the same product_parent. Please notice that we don’t know the specific information about these four child products.Their colors and sizes in Figure 2 are just for example. </p><p>We choose these following basic attributes in Table 1 to create the Customer Profile. Additionally, some synthetic variables are used in our model. These variables are described in Table 2.</p><p><img src="https://bu.dusays.com/2022/08/29/630c6ec98cacb.png" alt=""></p><p><font size=3 color="black">Figure 2: The Parent-Child Relationship in Microwave Products</font></p><p><img src="https://bu.dusays.com/2022/08/29/630c6ec5e59e1.png" alt=""></p><p><img src="https://bu.dusays.com/2022/08/29/630c6ec6a84d4.png" alt=""><br><br /></p><h2 id="Competing-Products-Analysis"><a href="#Competing-Products-Analysis" class="headerlink" title="Competing Products Analysis"></a>Competing Products Analysis</h2><p>First we visualize the data sets to show the profiles of the three products. The number of reviews and parent products are shown in Table 3. </p><p>Here is the products profile of microwave, the other two products are attached to the appendix. Figure 3 shows the star ratings and the number of reviews of every parent products. The horizontal axis reprensents the product titles. </p><p>For Microwaves, we select the product that has the most reviews - “Danby 0.7 cu.ft. Countertop Microwave” to analyze its reviews. Before we create the wordcloud of review_text, we take stemming operations. Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root formgenerally a written word form. </p><p>In Figure 5, High frequency words has bigger font size. In Figure 6, horizontal axis represents the review headline. </p><p>In addition to the above analysis from the perspective of the product itself, we also try to delete the click farmers or construct Customer Profile from the user’s perspective. Through the Customer Profile, we can have a better understanding toward the interests of the target users, and thus we could provide better product services.</p><p>We observe that a large number of users have bought more than one product. So we check out several reviews and found that many of them are click farmers, and their invalid reviews cannot be checked out through the helpful_votes, so eventually we manually deleted these reviews. For others who have really bought many products, we think their opinions are as valuable as those of Amazon Vine Voices Members. Therefore, we filtered out 53 reviews from 9 users who have purchased more than 5 products and sent these comments to Sunshine Company for their reference. These 9 user comments are detailed in the appendix.</p><p><img src="https://bu.dusays.com/2022/08/29/630c6ec7527c3.png" alt=""><br><br /></p><p><img src="https://bu.dusays.com/2022/08/29/630c6eca5224b.png" alt=""></p><p><font size=3 color="black">Figure 3: The Star Ratings and the Number of Reviews of Microwave</font></p><p><img src="https://bu.dusays.com/2022/08/29/630c6ecb036c4.png" alt=""></p><p><font size=3 color="black">Figure 4: The proportion of People Who Were in Amazon Vine and People Who Had Bought the Product</font></p><p><img src="https://bu.dusays.com/2022/08/29/630c6ebaee867.png" alt=""></p><p><font size=3 color="black">Figure 5: The Wordcloud of Microwave</font></p><p><img src="https://bu.dusays.com/2022/08/29/630c6ebae96fe.png" alt=""></p><p><font size=3 color="black">Figure 6: The Reviews' Total Votes and Helpful Votes of Danby 0.7 cu.ft. Countertop Microwave</font></p><h2 id="Model-Construction"><a href="#Model-Construction" class="headerlink" title="Model Construction"></a>Model Construction</h2><p>In the previous data preprocessing, we have processed a series of reviews_text through the Tokenization algorithm. Next, we will process the data through the TF-IDF (Term FrequencyInverse Document Frequency) algorithm and the MLP (Multi-Layer Perception) model to get the relationship between star_rating and review. </p><p>The keywords in different product reviews are quite different and will affect each other, so we build three prediction models for the products: microwave, pacifier, and hair dryer. We first use the TF-IDF algorithm to obtain the term frequency matrix. TF-IDF is a commonly used weighting technique for information retrieval and data mining. The algorithm can simply and efficiently find keywords in articles.</p><h3 id="Compute-the-TF-IDF-Matrix"><a href="#Compute-the-TF-IDF-Matrix" class="headerlink" title="Compute the TF-IDF Matrix"></a>Compute the TF-IDF Matrix</h3><p>TF-IDF consists of two parts, one is “Term Frequency” (abbreviated as TF), and the other is “Inverse Document Frequency” (abbreviated as IDF). </p><p>TF part will create a Term Frequency Matrix. We use the review_text in Figure 1 to<br>illustrate the process. The procession is shown in Figure 7.</p><p><img src="https://bu.dusays.com/2022/08/29/630c6ebba81fa.png" alt=""></p><p><font size=3 color="black">Figure 7: TF Procession</font></p><script type="math/tex; mode=display">f_i=\frac{c_i}{\sum^s_{i=1}c_i}\tag{1}</script><p>In Equation 1, \(f_i\) is the frequency of the \(i\)th word. \(c_i\) is the occurrence number in a review_text of the ith word. s is the amount of words in the review_text. we find out that in Figure 7 the word frequencies of ’look’, ’good’ and ’long’ are the highest. However, this does not mean these three words are equally important in the analysis process. We need to multiply this word frequency matrix by IDF (Inverse Document Frequency) to get the TF-IDF values of these words.</p><p>In Equation 2, \(d_i\) is the IDF value of the ith word. \(m\) is total number of articles in Corpus. \(n_i\) is the number of articles containing the ith word in Corpus. \(v_i\) is the TF-IDF value of the \(i\)th word.</p><script type="math/tex; mode=display">d_i=log \left( \frac{m}{n_i+1} \right)\tag{2}</script><script type="math/tex; mode=display">v_i=f_i*d_i\tag{3}</script><p>We give Figure 8 as an example for the TF-IDF values. After that, we will construct the TF-IDF matrix of the review based on the TF-IDF values of these words corresponding dictionaries. It is worth mentioning that we apply helpful<em>votes as a reference to the TF-IDF matrix. In Equation 4, \(M\) is the TF-IDF matrix for the product. The \(k\)th review’s \(i\)th word’s TF-IDF value is \(v</em>{ki}\).</p><script type="math/tex; mode=display">M_{ki}=v_{ki}\tag{4}</script><p><img src="https://bu.dusays.com/2022/08/29/630c6ebc78f39.png" alt=""></p><p><font size=3 color="black">Figure 8: TF-IDF Values</font></p><h3 id="Construct-the-MLP-Model"><a href="#Construct-the-MLP-Model" class="headerlink" title="Construct the MLP Model"></a>Construct the MLP Model</h3><p>We divide three data sets into training sets and test sets by a ratio of 3:1 respectively. Then we use the input and output of the training set to train MLP(Multi-Layer Perception). The core formulas of the MLP are show below.</p><p>Multi-Layer Perception made up of a lot of artificial neurons. The artificial neuron uses a nonlinear activation function to output an activity value.The training process of MLP can be divided into the following three steps:</p><ol><li>Calculate the state and activation value of each layer, until the last layer;</li><li>Calculate the error of each layer by backward propagation;</li><li>Calculate the partial derivative of each layer’s parameters, and update the parameters.</li></ol><p>Suppose neurons accept our TF-IDF Matrix \(X = (x1, x2, . . . , xn)\). State \(z\) is used to represent the weighted sum of input signal \(x\) obtained by a neuron, and the output is the activity value a of the neuron, which is defined as follows:</p><script type="math/tex; mode=display">z=W^TX+b\tag{5}</script><script type="math/tex; mode=display">a=f(z)\tag{6}</script><script type="math/tex; mode=display">z^{(l)}=W^{(l)} \cdot f_{l}\left(z^{(l-1)}\right)+b^{(l)}\tag{7}</script><p>In the process of forward propagation, we use notations as folloowing Table 4.</p><p><center><img src="https://bu.dusays.com/2022/08/29/630c6ec807b8d.png" alt="" width="100%"/></center><br><br /></p><script type="math/tex; mode=display">X=a^{(0)} \rightarrow z^{(1)} \rightarrow a^{(1)} \rightarrow z^{(2)} \rightarrow \cdots \rightarrow a^{(L-1)} \rightarrow z^{(L)} \rightarrow a^{(L)}=y\tag{8}</script><p>In the process of back propagation, given a set of samples \((X(i), y(i)), 1 ≤ i ≤ N\), the output of feedforward neural network whose objective function is \(f(X(i)|W, b)\) is as follows.</p><script type="math/tex; mode=display">\begin{aligned}J(W, b) &=\sum_{i=1}^{N} L\left(y^{(i)}, f\left(X^{(i)} \mid W, b\right)\right)+\frac{1}{2} \lambda\|W\|_{F}^{2} \\&=\sum_{i=1}^{N} J\left(W, b ; X(i), y^{(i)}\right)+\frac{1}{2} \lambda\|W\|_{F}^{2}\end{aligned}\tag{9}</script><script type="math/tex; mode=display">\|W\|_{F}^{2}=\sum_{l=1}^{L} \sum_{j=1}^{n^{l+1}} \sum_{1=1}^{n^{l}} W_{i j}^{(l)}\tag{10}</script><p>We use gradient descent to minimize the Equation 9, finally we get \(l\)th layer’s error in Equation 11.</p><script type="math/tex; mode=display">\begin{aligned}\delta^{(l)} & \triangleq \frac{J(W, b ; X, y)}{\partial z^{(l)}} \\&=\frac{\partial a^{(l)}}{\partial z^{(l)}} \cdot \frac{\partial z^{(l+1)}}{\partial a^{(l)}} \cdot \frac{J(W, b ; X, y)}{\partial z^{(l+1)}} \\&=\operatorname{diag}\left(f_{l}\left(z^{(l)}\right)\right) \cdot\left(W^{(l+1)}\right)^{T} \cdot \delta^{(l+1)} \\&=f_{l}\left(z^{(l)}\right) \odot\left(\left(W^{(l+1)}\right)^{T} \delta^{(l+1)}\right)\end{aligned}\tag{11}</script><p>Then we continue to iterate and get an error-stable model. Finally, we use the trained model to predict the test data. We set the fitting figure of Microwave in three products as Figure 9.</p><p><img src="https://bu.dusays.com/2022/08/29/630c6ebd3c476.png" alt=""></p><p><font size=3 color="black">Figure 9: The fitting Figure of Hair Dryer</font></p><h3 id="Time-Based-Analysis"><a href="#Time-Based-Analysis" class="headerlink" title="Time-Based Analysis"></a>Time-Based Analysis</h3><p>Product reputation is composed of star rating and popularity. Product quality is the determing factor of star rating and review content. We will analyze popularity based on time to provide suggestions for sunshine company.</p><p>We analyze the 8 products with the highest popularity, and observed their popularity changes over time. It can be seen from the Figure 10 that five of the eight products have peak popularity only at the beginning of each year, and the other three products will have peak popularity at the beginning and middle of each year. There are three reasons for this:</p><p>a. At the beginning of the year, it’s just the time for people to purchase in the new year. People’s desire for shopping is strong, while microwave, pacifier and hair dryer are durable products, which can be used for a long time. No one will buy twice in a short time.</p><p>b. Amazon or merchants have promotional activities at the beginning of the year to improve product popularity.</p><p>c. The data retrieving process is not objective, we only get the data at a certain period.</p><p><img src="https://bu.dusays.com/2022/08/29/630c6ebdf15b6.png" alt=""></p><p><font size=3 color="black">Figure 10: The Number of Reviews of the 10 Most Popularity Products</font></p><p>A products reputation consists the product’s praise and popularity, and the decisive factor that influences a products star ratings and the content of its reviews is its quality. In the following discussion we will offer a proposal for Sunshine Company from the perspective of the products popularity. </p><p>We analyze the top eight popular products and observe their popularity changes over time. To our surprise, 5 of these 8 products have popularity peaks only at the beginning of each year, and the remaining three products have popularity peaks at the beginning and middle of the year. There why This phenomenon happens are as follows:</p><ol><li>In the beginning of the year, everyone is buying goods for the coming of new year. So people’s desire for shopping is strong. On the other hand, the microwave, pacifier and hair dryer are all durable commodities, which can be used for a long time. So there will be no repurchase in a short time. Therefore, there will be peaks of popularity in only particular times.</li><li>Amazon or sellers have promotional campaign at the beginning of the year in order to increase the products popularity.</li><li>The data collection process is not objective. Only data of the beginning and middle of the year were collected.</li></ol><h2 id="Sales-Strategy"><a href="#Sales-Strategy" class="headerlink" title="Sales Strategy"></a>Sales Strategy</h2><p>Our goal is to allow Sunshine Company to expand market influence, continuously improve products and win a good corporate reputation among the public. We will make recommendations for Sunshine Company from two perspectives: How to Enhance the Reputation and How to Improve the Product.</p><h3 id="How-to-Enhance-the-Reputation"><a href="#How-to-Enhance-the-Reputation" class="headerlink" title="How to Enhance the Reputation"></a>How to Enhance the Reputation</h3><p>Reputation consists of two parts: popularity and star rating. First of all, from the perspective of popularity, we hope more people could pay attention to the company’s products. The most direct way to do this is to choose the right time to advertise and promote this product. we recommended that Sunshine Company are supposed to enhance propaganda and increase the discount at the beginning and middle of each year to increase product sales and popularity.</p><h3 id="How-to-Improve-the-Product"><a href="#How-to-Improve-the-Product" class="headerlink" title="How to Improve the Product"></a>How to Improve the Product</h3><p>On the other hand, for Star rating, the most fundamental influence of it is the product itself. By analyzing the content of the review, we could comprehend the relationship between star rating and review, furthermore, we could improve the product through the content review. Therefore, analyzing the content of the review is of vital importance. </p><p>From the previous model, we believe that there are three types of customers that worth pay attention to. Their reviews and attitude towards the product can effectively help us to improve the product.</p><ul><li>Amazon Vine Voices Members</li></ul><p>These people are selected by Amazon. They are the most trusted reviewers on Amazon to post opinions about new and pre-release items to help their fellow<br>customers make informed purchase decisions.</p><ul><li>People who have bought a lot of different products.</li></ul><p>These people have experienced more similar products than common customers, and they have a deeper understanding of this type of product. We have selected<br>nine such customers for Sunshine Company, their reviews are in the appendix.</p><ul><li>One-star rating customer</li></ul><p>our products or services may not meet the needs of all customers, so some of these customers give a one-star rating. Compared with five-star users who have very low-information reviews, these one-star rating users often directly mention the shortcomings of the product. So It is often helpful to look at these reviews<br>carefully.</p><p>we give the word cloud of one-star rating user of the microwave reviews below. There are words such as “service” and “warranty” in the word cloud. So we conclude that these users are dissatisfied with the product services and product warranty of the<br>microwave. In this way we can make specfic measures toward this issue. The word cloud of one-star rating users reviews by Pacifier and hair dryer is in the appendix. At the same time, we can find that users who give one star and users who give five stars tend to have more emotional reviews.</p><h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>Sunshine Company want to launch three new products in online marketplacemicrowave, pacifier and hair dryer. And they hire our team to help them to analyze the relevant products in the current market and make suggestions for their product sales.</p><p>At first,we process the dataset.We delete some data with missing values.At the same time,we find some data of click farmers.We also remove these useless data.We screen and integrate 15 attributes of the data set,leaving 11 basic attributes and 3 synthetic attributes.For a large number of texts in the review,we use the tokenization algorithm to cut the complex texts into simple words for our subsequent ananlysis.</p><p>Then,we analyze and visualize the attributes of the data set.In this way,we have a full understanding of the whole data set. We use the multi-demension evaluation system of “reviews evaluation products” and “helpful votes evaluation reviews”, through TF-IDF technology and MLP model to analyze the relationship between star rating, review and helpful votes,to help sunshine company to have a deeper understanding of products in the market.</p><p>Finally,on the basis of time ,we analyze the product heat,get a deeper understanding of customer review ,and provide a way for sunshine company to increase its products’popularity.</p><h2 id="Strenghths-and-Weaknesses"><a href="#Strenghths-and-Weaknesses" class="headerlink" title="Strenghths and Weaknesses"></a>Strenghths and Weaknesses</h2><h3 id="Strengths"><a href="#Strengths" class="headerlink" title="Strengths"></a>Strengths</h3><ul><li>Our model has strong interpretability</li></ul><p>The traditional MLP model has poor interpretability,and we continue to do lexical analysis and grammatical analysis on this basis, which enhances the interpretability of the whole model.</p><ul><li>We look at the product from the customer’s perspective</li></ul><p>We analyze the user’s emotion and build the user’s portrait through the user’s comments. At the same time, we find some customers who are meaningful to sunshine company.</p><h3 id="Weaknesses"><a href="#Weaknesses" class="headerlink" title="Weaknesses"></a>Weaknesses</h3><ul><li>There is a lack of analysis at the time level.</li></ul><p>At the time level, we only analyze the changes of product popularity, and there are a lot of contents that can be mined between time and review.</p><ul><li>We don’t build relationship among the three products.</li></ul><p>If our model can fully break the sales relationship among the three products in the market, it will be more helpful for sunshine company which sells the microwave, pacifier and hair dryer at the same time.</p><details class="tag-plugin folding" ><summary><span>Appendix: Product Profile</span></summary><div class="body"><p><img src="https://bu.dusays.com/2022/08/29/630c6ebeace1f.png" alt=""></p><p><font size=3 color="black">Figure 11: The Star Ratings and the Number of Reviews of Pacifier</font></p><p><img src="https://bu.dusays.com/2022/08/29/630c6ebf5d2db.png" alt=""></p><p><font size=3 color="black">Figure 12: The Proportion of People Who Were in Amazon Vine and People Who Had Bought the Product</font></p><p><img src="https://bu.dusays.com/2022/08/29/630c6ec0194ae.png" alt=""></p><p><font size=3 color="black">Figure 13: The Wordcloud of Pacifier</font></p><p><img src="https://bu.dusays.com/2022/08/29/630c6ec0c09a2.png" alt=""></p><p><font size=3 color="black">Figure 14: The Reviews’ Total Votes and Helpful Votes of a Pacifier Product</font></p><p><img src="https://bu.dusays.com/2022/08/29/630c6ec170423.png" alt=""></p><p><font size=3 color="black">Figure 15: The Star Ratings and the Number of Reviews of Pacifier</font></p><p><img src="https://bu.dusays.com/2022/08/29/630c6ec21d3e6.png" alt=""></p><p><font size=3 color="black">Figure 16: The Proportion of People Who Were in Amazon Vine and People Who Had Bought the Product</font></p><p><img src="https://bu.dusays.com/2022/08/29/630c6ec2d7c68.png" alt=""></p><p><font size=3 color="black">Figure 17: The Wordcloud of Pacifier</font></p><p><img src="https://bu.dusays.com/2022/08/29/630c6ec395936.png" alt=""></p><p><font size=3 color="black">Figure 18: The Reviews’ Total Votes and Helpful Votes of a Pacifier Product</font></p><p><img src="https://bu.dusays.com/2022/08/29/630c6ec454e05.png" alt=""></p><p><font size=3 color="black">Figure 19: The WordCloud of Reviews with One Star about Pacifier</font></p><p><img src="https://bu.dusays.com/2022/08/29/630c6ec5161fd.png" alt=""></p><p><font size=3 color="black">Figure 20: The WordCloud of Reviews with One Star about Hair Dryer</font></p></div></details>]]></content>
    
    
    <summary type="html">&lt;p&gt;This is my second time for MCM &amp;amp; ICM. I teamed up with two friends. This is how we divide the work: one of my friends collecting information and the other writing the essay. I was responsible for modeling and coding. We finally got &lt;strong&gt;Meritorious Winner(Top 5%)&lt;/strong&gt;. I’m so proud of it.&lt;/p&gt;</summary>
    
    
    
    <category term="Natural Language Processing" scheme="https://enblog.crocodilezs.top/categories/Natural-Language-Processing/"/>
    
    <category term="Sentiment Analysis" scheme="https://enblog.crocodilezs.top/categories/Natural-Language-Processing/Sentiment-Analysis/"/>
    
    
    <category term="Natural Language Processing" scheme="https://enblog.crocodilezs.top/tags/Natural-Language-Processing/"/>
    
    <category term="MLP" scheme="https://enblog.crocodilezs.top/tags/MLP/"/>
    
    <category term="ARIMA" scheme="https://enblog.crocodilezs.top/tags/ARIMA/"/>
    
  </entry>
  
  <entry>
    <title>Predicting the price by its description | ML Algorithms</title>
    <link href="https://enblog.crocodilezs.top/201911/2019-11-12-price-prediction/"/>
    <id>https://enblog.crocodilezs.top/201911/2019-11-12-price-prediction/</id>
    <published>2019-11-12T08:22:10.000Z</published>
    <updated>2022-08-29T09:34:25.345Z</updated>
    
    <content type="html"><![CDATA[<p>This is a task in the course <code>Introduction to Data Science</code>. Our team was going to predict the price of products by their description. </p><span id="more"></span><h2 id="Topic"><a href="#Topic" class="headerlink" title="Topic"></a>Topic</h2><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>Considering the number of products sold online, product pricing becomes more difficult at scale. Apparel has strong seasonal pricing trends and is heavily influenced by brands, while electronics prices fluctuate based on product specifications. It is a meaningful question to help merchants effectively sell their goods by making reasonable pricing based on past information.</p><h3 id="Target"><a href="#Target" class="headerlink" title="Target"></a>Target</h3><p>The product description, product category and brand information is given and combined with the product price from the training data to set the price for the new product.</p><p>Obviously Versace’s clothes should be much higher in price than Metersbonwe’s clothes, and in the description of the goods, you can find a slight difference between the two descriptions. </p><blockquote><p>This project aims to analyze the text information, extract the important information from the text information and derive the potential relationship with the price。 </p></blockquote><h3 id="Analysis-of-atributes"><a href="#Analysis-of-atributes" class="headerlink" title="Analysis of atributes"></a>Analysis of atributes</h3><center><img src="https://bu.dusays.com/2022/08/27/630a21a12578f.png" alt="" width="100%" /></center><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><ul><li><code>train.csv</code> training dataset (Include <code>price</code>)</li><li><code>test.csv</code> test dataset (Not include <code>price</code>) ; <code>label_test.csv</code> (Corresponding to the price of the test dataset)</li><li><code>f_test.csv</code> Final measurement data set (Not include <code>price</code>)</li></ul><h3 id="Evaluation-Indicators"><a href="#Evaluation-Indicators" class="headerlink" title="Evaluation Indicators"></a>Evaluation Indicators</h3><p>We used <code>Mean Squared Logarithmic Error</code> (MSLE) to evaluate the algorithm: </p><script type="math/tex; mode=display">MSLE = \cfrac{1}{n}\sum_{i=1}^n(log(p_i+1)-log(\alpha_i+1))^2\tag{1}</script><p>Which \(n\) means the number of samples in test dataset; \(p_i\)means the predicting price of sales; \(\alpha_i\) means the real price.</p><h2 id="Data-Process"><a href="#Data-Process" class="headerlink" title="Data Process"></a>Data Process</h2><h3 id="Learning-sample-code"><a href="#Learning-sample-code" class="headerlink" title="Learning sample code"></a>Learning sample code</h3><p>The sample code given was first tried to understand the general idea of solving this problem. The main processes to solve this price prediction problem are: importing data and data exploration, data pre-processing, model construction, price prediction and measurement.</p><h4 id="import-data-and-exploration"><a href="#import-data-and-exploration" class="headerlink" title="import data and exploration"></a>import data and exploration</h4><p>Import the data and get acknowledge with it.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data = pd.read_csv(<span class="string">&#x27;../data/4/train.csv&#x27;</span>, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">&#x27;../data/4/test.csv&#x27;</span>,sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">train_data.info()</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;class &#x27;pandas.core.frame.DataFrame&#x27;&gt;</span><br><span class="line">RangeIndex: 300000 entries, 0 to 299999</span><br><span class="line">Data columns (total 8 columns):</span><br><span class="line">train_id             300000 non-null int64</span><br><span class="line">name                 300000 non-null object</span><br><span class="line">item_condition_id    300000 non-null int64</span><br><span class="line">category_name        298719 non-null object</span><br><span class="line">brand_name           171929 non-null object</span><br><span class="line">price                300000 non-null float64</span><br><span class="line">shipping             300000 non-null int64</span><br><span class="line">item_description     300000 non-null object</span><br><span class="line">dtypes: float64(1), int64(3), object(4)</span><br><span class="line">memory usage: 18.3+ MB</span><br></pre></td></tr></table></figure><h4 id="Data-Preprocess"><a href="#Data-Preprocess" class="headerlink" title="Data Preprocess"></a>Data Preprocess</h4><p>First of all, we need to remove <code>price</code> from the training data, and then remove <code>train_id</code> or <code>test_id</code> which are not useful. By looking at the data attributes above, we can see that <code>category_name</code> and <code>brand_name</code> have missing data, so the sample code is filled with <code>missing</code> directly.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">featureProcessing</span>(<span class="params">df</span>):</span><br><span class="line">    <span class="comment"># delete the data that will not be used</span></span><br><span class="line">    df = df.drop([<span class="string">&#x27;price&#x27;</span>, <span class="string">&#x27;test_id&#x27;</span>, <span class="string">&#x27;train_id&#x27;</span>], axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># deal with the missing value with a default value</span></span><br><span class="line">    df[<span class="string">&#x27;category_name&#x27;</span>] = df[<span class="string">&#x27;category_name&#x27;</span>].fillna(<span class="string">&#x27;missing&#x27;</span>).astype(<span class="built_in">str</span>)</span><br><span class="line">    df[<span class="string">&#x27;brand_name&#x27;</span>] = df[<span class="string">&#x27;brand_name&#x27;</span>].fillna(<span class="string">&#x27;missing&#x27;</span>).astype(<span class="built_in">str</span>)</span><br><span class="line">    df[<span class="string">&#x27;item_description&#x27;</span>] = df[<span class="string">&#x27;item_description&#x27;</span>].fillna(<span class="string">&#x27;No&#x27;</span>)</span><br><span class="line">    <span class="comment"># convert the data : int -&gt; str</span></span><br><span class="line">    df[<span class="string">&#x27;shipping&#x27;</span>] = df[<span class="string">&#x27;shipping&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">    df[<span class="string">&#x27;item_condition_id&#x27;</span>] = df[<span class="string">&#x27;item_condition_id&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure><h4 id="Model-Construction"><a href="#Model-Construction" class="headerlink" title="Model Construction"></a>Model Construction</h4><p>First of all, the input of the model is done and the matrix of word frequencies is generated by <code>CountVectorizer</code> and <code>TfidfVectorizer</code>. <code>Tfidf</code> is better because the number of occurrences of each word in all field clocks is taken into account and the generated word frequency matrix is weighted.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vectorizer = FeatureUnion([</span><br><span class="line">    (<span class="string">&#x27;name&#x27;</span>, CountVectorizer(ngram_range=(<span class="number">1</span>, <span class="number">2</span>), max_features=<span class="number">50000</span>, preprocessor=build_preprocessor_1(<span class="string">&#x27;name&#x27;</span>))),</span><br><span class="line">    (<span class="string">&#x27;category_name&#x27;</span>, CountVectorizer(token_pattern=<span class="string">&#x27;.+&#x27;</span>, preprocessor=build_preprocessor_1(<span class="string">&#x27;category_name&#x27;</span>))),</span><br><span class="line">    (<span class="string">&#x27;brand_name&#x27;</span>, CountVectorizer(token_pattern=<span class="string">&#x27;.+&#x27;</span>, preprocessor=build_preprocessor_1(<span class="string">&#x27;brand_name&#x27;</span>))),</span><br><span class="line">    (<span class="string">&#x27;shipping&#x27;</span>, CountVectorizer(token_pattern=<span class="string">&#x27;\d+&#x27;</span>, preprocessor=build_preprocessor_1(<span class="string">&#x27;shipping&#x27;</span>))),</span><br><span class="line">    (<span class="string">&#x27;item_condition_id&#x27;</span>, CountVectorizer(token_pattern=<span class="string">&#x27;\d+&#x27;</span>, preprocessor=build_preprocessor_1(<span class="string">&#x27;item_condition_id&#x27;</span>))),</span><br><span class="line">    (<span class="string">&#x27;item_description&#x27;</span>, TfidfVectorizer(ngram_range=(<span class="number">1</span>, <span class="number">3</span>),max_features=<span class="number">100000</span>, preprocessor=build_preprocessor_1(<span class="string">&#x27;item_description&#x27;</span>))),</span><br><span class="line">])</span><br></pre></td></tr></table></figure><p>Predict the price by Ridge Regression.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ridgeClassify</span>(<span class="params">train_data, train_label</span>):</span><br><span class="line">    ridgeClf = Ridge(</span><br><span class="line">        solver=<span class="string">&#x27;auto&#x27;</span>,</span><br><span class="line">        fit_intercept=<span class="literal">True</span>,</span><br><span class="line">        alpha=<span class="number">0.5</span>,</span><br><span class="line">        max_iter=<span class="number">500</span>,</span><br><span class="line">        normalize=<span class="literal">False</span>,</span><br><span class="line">        tol=<span class="number">0.05</span>)</span><br><span class="line">    <span class="comment"># Training</span></span><br><span class="line">    ridgeClf.fit(train_data, train_label)</span><br><span class="line">    <span class="keyword">return</span> ridgeClf</span><br></pre></td></tr></table></figure><p>By understanding the dataset and studying the sample code, we learned that there are three angles to start with to optimize the answer to this question.</p><ol><li>data preprocessing: How to handle missing values? How should the attributes be combined?</li><li>optimization when forming the word frequency matrix: adjusting the parameters of <code>CountVectorizer</code> and <code>TfidfVectorizer</code>.</li><li>Model selection and optimization: try models other than ridge regression, adjust model parameters.</li></ol><h3 id="Try-more-models"><a href="#Try-more-models" class="headerlink" title="Try more models"></a>Try more models</h3><p>In the sample code above, the result obtained using the ridge regression model is about 3.01. After the hints from the previous class and the online search, we are ready to try the <code>MLP</code> model and the <code>Lgmb</code> model again. After roughly trying both models, we decided to further optimize the model using <code>MLP</code>.</p><h4 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a><code>MLP</code></h4><p>The result of <code>MLP</code> is 0.4430 (MSLE)</p><h4 id="LGBM"><a href="#LGBM" class="headerlink" title="LGBM"></a><code>LGBM</code></h4><p>The result of  <code>Lgbm</code> is 0.2688 (MSLE)</p><h4 id="MLP-Combined-with-LGBM"><a href="#MLP-Combined-with-LGBM" class="headerlink" title="MLP Combined with LGBM"></a><code>MLP</code> Combined with <code>LGBM</code></h4><p><1> Preprocessing</p><ul><li>Import the data</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train = pd.read_csv(<span class="string">&#x27;data/train.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">&#x27;data/test.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line"><span class="comment"># train and test data are handled together</span></span><br><span class="line">df = pd.concat([train, test], axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><ul><li>Handling missing value</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Handling missing values</span></span><br><span class="line">   df[<span class="string">&#x27;category_name&#x27;</span>] = df[<span class="string">&#x27;category_name&#x27;</span>].fillna(<span class="string">&#x27;MISS&#x27;</span>).astype(<span class="built_in">str</span>)</span><br><span class="line">   df[<span class="string">&#x27;brand_name&#x27;</span>] = df[<span class="string">&#x27;brand_name&#x27;</span>].fillna(<span class="string">&#x27;missing&#x27;</span>).astype(<span class="built_in">str</span>)</span><br><span class="line">   df[<span class="string">&#x27;item_description&#x27;</span>] = df[<span class="string">&#x27;item_description&#x27;</span>].fillna(<span class="string">&#x27;No&#x27;</span>)</span><br><span class="line">   <span class="comment"># modifying data structure</span></span><br><span class="line">   df[<span class="string">&#x27;shipping&#x27;</span>] = df[<span class="string">&#x27;shipping&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">   df[<span class="string">&#x27;item_condition_id&#x27;</span>] = df[<span class="string">&#x27;item_condition_id&#x27;</span>].astype(<span class="built_in">str</span>)</span><br></pre></td></tr></table></figure><ul><li>Feature vectorization</li></ul><p>Use the <code>CountVectorizer</code> class in the <code>sklearn</code> library to vectorize the text features and use <code>FeatureUnion</code> for feature union.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vectorizer = FeatureUnion([</span><br><span class="line">        (<span class="string">&#x27;name&#x27;</span>, CountVectorizer(</span><br><span class="line">            ngram_range=(<span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            max_features=<span class="number">100000</span>,</span><br><span class="line">            preprocessor=build_preprocessor(<span class="string">&#x27;name&#x27;</span>))),</span><br><span class="line">        (<span class="string">&#x27;category_name&#x27;</span>, CountVectorizer(</span><br><span class="line">            token_pattern=<span class="string">&#x27;.+&#x27;</span>,</span><br><span class="line">            preprocessor=build_preprocessor(<span class="string">&#x27;category_name&#x27;</span>))),</span><br><span class="line">        (<span class="string">&#x27;brand_name&#x27;</span>, CountVectorizer(</span><br><span class="line">            token_pattern=<span class="string">&#x27;.+&#x27;</span>,</span><br><span class="line">            preprocessor=build_preprocessor(<span class="string">&#x27;brand_name&#x27;</span>))),</span><br><span class="line">        (<span class="string">&#x27;shipping&#x27;</span>, CountVectorizer(</span><br><span class="line">            token_pattern=<span class="string">&#x27;\d+&#x27;</span>,</span><br><span class="line">            preprocessor=build_preprocessor(<span class="string">&#x27;shipping&#x27;</span>))),</span><br><span class="line">        (<span class="string">&#x27;item_condition_id&#x27;</span>, CountVectorizer(</span><br><span class="line">            token_pattern=<span class="string">&#x27;\d+&#x27;</span>,</span><br><span class="line">            preprocessor=build_preprocessor(<span class="string">&#x27;item_condition_id&#x27;</span>))),</span><br><span class="line">        (<span class="string">&#x27;item_description&#x27;</span>, TfidfVectorizer(</span><br><span class="line">            ngram_range=(<span class="number">1</span>, <span class="number">3</span>),</span><br><span class="line">            max_features=<span class="number">200000</span>,</span><br><span class="line">            preprocessor=build_preprocessor(<span class="string">&#x27;item_description&#x27;</span>),</span><br><span class="line">            stop_words=<span class="string">&#x27;english&#x27;</span>)),</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure><p><2> Model Construction</p><p>The features were trained using the ridge regression model, the <code>Lgbm</code> model and the <code>mlp</code> model, respectively, and the solutions obtained in local tests were 3.01, 3.00, and 0.26, respectively.</p><ul><li>Ridge Regression</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ridge_classify</span>(<span class="params">train_data,train_label</span>):</span><br><span class="line">    <span class="comment">#model</span></span><br><span class="line">    model = Ridge(</span><br><span class="line">            solver=<span class="string">&#x27;auto&#x27;</span>,</span><br><span class="line">            fit_intercept=<span class="literal">True</span>,</span><br><span class="line">            alpha=<span class="number">0.4</span>,</span><br><span class="line">            max_iter=<span class="number">100</span>,</span><br><span class="line">            normalize=<span class="literal">False</span>,</span><br><span class="line">            tol=<span class="number">0.05</span>)</span><br><span class="line">    <span class="comment">#training</span></span><br><span class="line">    model.fit(train_data, train_label)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><ul><li><code>lgbm</code> Model</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lgbm_classify</span>(<span class="params">train_data,train_label</span>):</span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.75</span>,</span><br><span class="line">        <span class="string">&#x27;application&#x27;</span>: <span class="string">&#x27;regression&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">3</span>,</span><br><span class="line">        <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">100</span>,</span><br><span class="line">        <span class="string">&#x27;verbosity&#x27;</span>: -<span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;RMSE&#x27;</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    train_X, valid_X, train_y, valid_y = train_test_split(train_data, train_label, test_size=<span class="number">0.1</span>, random_state=<span class="number">144</span>)</span><br><span class="line">    d_train = lgb.Dataset(train_X, label=train_y)</span><br><span class="line">    d_valid = lgb.Dataset(valid_X, label=valid_y)</span><br><span class="line">    watchlist = [d_train, d_valid]</span><br><span class="line"></span><br><span class="line">    model = lgb.train(params, train_set=d_train, num_boost_round=<span class="number">2200</span>, valid_sets=watchlist, \</span><br><span class="line">                      early_stopping_rounds=<span class="number">50</span>, verbose_eval=<span class="number">100</span>)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><ul><li><code>mlp</code> Model</li></ul><p>The <code>MLP</code> model consists of two fully connected layers and a dropout layer, which is essentially a network of multiple hidden layers.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mlp_model</span>(<span class="params">train_data,train_label,row_train</span>):</span><br><span class="line">    model = Sequential()</span><br><span class="line">    <span class="comment"># fully connected layer</span></span><br><span class="line">    model.add(Dense(<span class="number">64</span>, input_shape=(row_train,), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">    <span class="comment"># DropOut layer</span></span><br><span class="line">    model.add(Dropout(<span class="number">0.4</span>))</span><br><span class="line">    <span class="comment"># fully connected layer + classifier</span></span><br><span class="line">    model.add(Dense(<span class="number">1</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;mean_squared_logarithmic_error&#x27;</span>,</span><br><span class="line">                  optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">                  metrics=[<span class="string">&#x27;accuracy&#x27;</span>]</span><br><span class="line">                  )</span><br><span class="line"></span><br><span class="line">    model.fit(train_data, train_label,</span><br><span class="line">              batch_size=<span class="number">300</span>,</span><br><span class="line">              epochs=<span class="number">1</span>,</span><br><span class="line">              )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model.predict(X_test)</span><br></pre></td></tr></table></figure><h3 id="Optimization-when-forming-word-frequency-matrix"><a href="#Optimization-when-forming-word-frequency-matrix" class="headerlink" title="Optimization when forming word frequency matrix"></a>Optimization when forming word frequency matrix</h3><p>In the sample code we tried to replace all <code>CountVectorizer</code> with <code>TdidfVectorizer</code> and then use the Ridge model for prediction, but the result is not much optimized, only up to 2.9.<br>Later, when using the <code>MLP</code>, we completely discarded the <code>CountVectorizer</code> and used only the <code>TdidfVectorizer</code>.</p><h3 id="Optimize-the-data-pre-processing-process"><a href="#Optimize-the-data-pre-processing-process" class="headerlink" title="Optimize the data pre-processing process"></a>Optimize the data pre-processing process</h3><p>The way we optimize the <code>MLP</code>, which is basically perfected above, is to <strong>try different combinations of features</strong>.</p><h4 id="Analysis-of-the-attributes"><a href="#Analysis-of-the-attributes" class="headerlink" title="Analysis of the attributes"></a>Analysis of the attributes</h4><p>First I analyzed the attributes:<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">item_condition_id    300000 non-null int64</span><br><span class="line">shipping             300000 non-null int64</span><br><span class="line"></span><br><span class="line">name                 300000 non-null object</span><br><span class="line">category_name        298719 non-null object</span><br><span class="line">brand_name           171929 non-null object</span><br><span class="line">item_description     300000 non-null object</span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p><code>item_condition_id</code> and <code>shipping</code> are considered directly as inputs, while <code>name</code>, <code>category_name</code>, <code>brand_name</code>, <code>item_description</code> are considered for different combinations to try.</p><p>Before that, we found an example tutorial on data visualization to analyze the attributes of the data.<br>The optimal combination of inputs is obtained by observing the data.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train.head()</span><br></pre></td></tr></table></figure><center><img src="https://bu.dusays.com/2022/08/27/630a21a26d2eb.png" alt="" width="100%" /></center><ul><li><code>price</code><br>By looking at the data after visualization we know why we have to do <code>log1p</code> on <code>price</code> to make the distribution of <code>price</code> better.</li></ul><center><img src="https://bu.dusays.com/2022/08/27/630a21a3ad83d.png" alt="" width="100%" /></center><ul><li><code>category_name</code><br>Try to split the property into various subclasses and view the corresponding data.</li></ul><center><img src="https://bu.dusays.com/2022/08/27/630a21a4e3868.png" alt="" width="100%" /></center><ul><li><code>item_description</code><center><img src="https://bu.dusays.com/2022/08/27/630a21a7ac2b6.png" alt="" width="100%" /></center></li></ul><h4 id="Try-different-attributes-combination"><a href="#Try-different-attributes-combination" class="headerlink" title="Try different attributes combination"></a>Try different attributes combination</h4><ol><li>simply combine the attributes together in the sample code for text analysis, i.e. <code>name</code> + <code>item_condition_id</code> + <code>category_name</code> + <code>brand_name</code> + <code>shipping</code> + <code>item_description</code>. (6 inputs)</li><li>try <code>name</code>, <code>item_condition_id</code>, <code>shipping</code>,<code>category_name</code> + <code>item_description</code>, <code>brand_name</code>. (5 inputs)</li><li>try <code>name</code>, <code>item_condition_id</code>, <code>shipping</code>, <code>category_name</code> + <code>brand_name</code> + <code>item_description</code>. (4 inputs)</li><li>try <code>name</code>, <code>item_condition_id</code>, <code>shipping</code>, <code>name</code> + <code>category_name</code> + <code>brand_name</code> + <code>item_description</code>. (4 inputs)</li></ol><p>The results for the four combinations as input are very similar, except that combination 1 <code>MSLE</code> is around 0.4, combinations 2 and 3 are around 0.21, and combination 4 eventually runs to around 0.17. Combination 4 actually increases the weight of <code>name</code> to make the final result better.</p><h2 id="Final-source-code-and-experimental-results"><a href="#Final-source-code-and-experimental-results" class="headerlink" title="Final source code and experimental results"></a>Final source code and experimental results</h2><ol><li><p>Data preprocessing</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># data preprocessing</span></span><br><span class="line"><span class="comment"># There are 8 attributes, remove price, train_id will have no influence on the result.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_preprocess</span>(<span class="params">df</span>):</span><br><span class="line">    df[<span class="string">&#x27;name&#x27;</span>] = df[<span class="string">&#x27;name&#x27;</span>].fillna(<span class="string">&#x27;&#x27;</span>) + <span class="string">&#x27; &#x27;</span> + df[<span class="string">&#x27;brand_name&#x27;</span>].fillna(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    df[<span class="string">&#x27;text&#x27;</span>] = (df[<span class="string">&#x27;item_description&#x27;</span>].fillna(<span class="string">&#x27;&#x27;</span>) + <span class="string">&#x27; &#x27;</span> + df[<span class="string">&#x27;name&#x27;</span>] + <span class="string">&#x27; &#x27;</span> + df[<span class="string">&#x27;category_name&#x27;</span>].fillna(<span class="string">&#x27;&#x27;</span>))</span><br><span class="line">    <span class="keyword">return</span> df[[<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;text&#x27;</span>, <span class="string">&#x27;shipping&#x27;</span>, <span class="string">&#x27;item_condition_id&#x27;</span>]]</span><br></pre></td></tr></table></figure></li><li><p>Model Construction</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fit_predict</span>(<span class="params">xs, y_train</span>):</span><br><span class="line">    X_train, X_test = xs</span><br><span class="line">    <span class="comment"># Configure the operation method of tf.Session, such as gpu operation or cpu operation</span></span><br><span class="line">    config = tf.ConfigProto(</span><br><span class="line">        <span class="comment"># Set the number of threads for multiple operations in parallel</span></span><br><span class="line">        intra_op_parallelism_threads=<span class="number">1</span>, use_per_session_threads=<span class="number">1</span>, inter_op_parallelism_threads=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># Session provides the environment for Operation execution and Tensor evaluation.</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session(graph=tf.Graph(), config=config) <span class="keyword">as</span> sess, timer(<span class="string">&#x27;fit_predict&#x27;</span>):</span><br><span class="line">        ks.backend.set_session(sess)</span><br><span class="line">        model_in = ks.Input(shape=(X_train.shape[<span class="number">1</span>],), dtype=<span class="string">&#x27;float32&#x27;</span>, sparse=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># ks.layers.Dense means the dimension of output</span></span><br><span class="line">        <span class="comment"># Dense full connected layer, equals to add one layer directly.</span></span><br><span class="line">        <span class="comment"># activation is the activation function.</span></span><br><span class="line">        out = ks.layers.Dense(<span class="number">192</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(model_in)</span><br><span class="line">        out = ks.layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(out)</span><br><span class="line">        out = ks.layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(out)</span><br><span class="line">        out = ks.layers.Dense(<span class="number">1</span>)(out)</span><br><span class="line">        model = ks.Model(model_in, out)</span><br><span class="line">        model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;mean_squared_error&#x27;</span>, optimizer=ks.optimizers.Adam(lr=<span class="number">3e-3</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            <span class="keyword">with</span> timer(<span class="string">f&#x27;epoch <span class="subst">&#123;i + <span class="number">1</span>&#125;</span>&#x27;</span>):</span><br><span class="line">                model.fit(x=X_train, y=y_train, batch_size=<span class="number">2</span> ** (<span class="number">11</span> + i), epochs=<span class="number">1</span>, verbose=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> model.predict(X_test)[:, <span class="number">0</span>]</span><br></pre></td></tr></table></figure></li><li><p>Model training and prediction</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    vectorizer = make_union(<span class="comment"># Assemble all transformers into a FeatureUnion. n_jobs means it can be done simultaneously</span></span><br><span class="line">        <span class="comment"># FunctionTransformer implements a custom transformation with no input validation when validate=False</span></span><br><span class="line">        <span class="comment"># TfidfVectorizer function, consider only the words in the first max_feature bits by word frequency, token_pattern=&#x27;\w+&#x27; matches at least one word</span></span><br><span class="line">        make_pipeline(FunctionTransformer(itemgetter(<span class="string">&#x27;name&#x27;</span>), validate=<span class="literal">False</span>), TfidfVectorizer(max_features=<span class="number">100000</span>, token_pattern=<span class="string">&#x27;\w+&#x27;</span>)),</span><br><span class="line">        make_pipeline(FunctionTransformer(itemgetter(<span class="string">&#x27;text&#x27;</span>), validate=<span class="literal">False</span>), TfidfVectorizer(max_features=<span class="number">100000</span>, token_pattern=<span class="string">&#x27;\w+&#x27;</span>)),</span><br><span class="line">        make_pipeline(FunctionTransformer(itemgetter([<span class="string">&#x27;shipping&#x27;</span>, <span class="string">&#x27;item_condition_id&#x27;</span>]), validate=<span class="literal">False</span>),</span><br><span class="line">                      FunctionTransformer(to_records, validate=<span class="literal">False</span>), DictVectorizer()),</span><br><span class="line">        n_jobs=<span class="number">4</span>)</span><br><span class="line">    <span class="comment"># StandardScaler() performs data normalization. Save the parameters (mean, variance) from the training set directly using its object to transform the test set data.</span></span><br><span class="line">    y_scaler = StandardScaler()</span><br><span class="line">    <span class="comment"># The with statement is used when accessing resources to ensure that the necessary &quot;cleanup&quot; operations are performed to release resources regardless of exceptions during use, such as automatic closure of files after use, automatic acquisition and release of locks in threads, etc.</span></span><br><span class="line">    <span class="keyword">with</span> timer(<span class="string">&#x27;process train&#x27;</span>):</span><br><span class="line">        train = pd.read_csv(<span class="string">&#x27;train.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        test = pd.read_csv(<span class="string">&#x27;test.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        <span class="comment"># remove &#x27;price&#x27;</span></span><br><span class="line">        train = train[train[<span class="string">&#x27;price&#x27;</span>] &gt; <span class="number">0</span>].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># normalization of price</span></span><br><span class="line">        y_train = y_scaler.fit_transform(np.log1p(train[<span class="string">&#x27;price&#x27;</span>].values.reshape(-<span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line">        X_train = vectorizer.fit_transform(data_preprocess(train)).astype(np.float32)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;X_train: <span class="subst">&#123;X_train.shape&#125;</span> of <span class="subst">&#123;X_train.dtype&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> timer(<span class="string">&#x27;process valid&#x27;</span>):</span><br><span class="line">        X_test = vectorizer.transform(data_preprocess(test)).astype(np.float32)</span><br><span class="line">    <span class="keyword">with</span> ThreadPool(processes=<span class="number">4</span>) <span class="keyword">as</span> pool:</span><br><span class="line">        Xb_train, Xb_test = [x.astype(np.<span class="built_in">bool</span>).astype(np.float32) <span class="keyword">for</span> x <span class="keyword">in</span> [X_train, X_test]]</span><br><span class="line">        xs = [[Xb_train, Xb_test], [X_train, X_test]] * <span class="number">2</span></span><br><span class="line">        <span class="comment"># prediction</span></span><br><span class="line">        y_pred = np.mean(pool.<span class="built_in">map</span>(partial(fit_predict, y_train=y_train), xs), axis=<span class="number">0</span>)</span><br><span class="line">    y_pred = np.expm1(y_scaler.inverse_transform(y_pred.reshape(-<span class="number">1</span>, <span class="number">1</span>))[:, <span class="number">0</span>])</span><br><span class="line">    <span class="comment"># print(type(y_pred))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Export prediction results to csv</span></span><br><span class="line">    test_id = np.array(<span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(y_pred)))</span><br><span class="line">    dataframe = pd.DataFrame(&#123;<span class="string">&#x27;test_id&#x27;</span>: test_id, <span class="string">&#x27;price&#x27;</span>: y_pred&#125;)</span><br><span class="line">    dataframe.to_csv(<span class="string">&quot;res.csv&quot;</span>, index=<span class="literal">False</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(&#x27;Valid MSLE: &#123;:.4f&#125;&#x27;.format(mean_squared_log_error(valid[&#x27;price&#x27;], y_pred)))</span></span><br></pre></td></tr></table></figure></li></ol><p>The final experimental result reached 0.179.</p><h2 id="Other-optimization-directions-in-the-MLP-model"><a href="#Other-optimization-directions-in-the-MLP-model" class="headerlink" title="Other optimization directions in the MLP model"></a>Other optimization directions in the <code>MLP</code> model</h2><ol><li>It can be observed that in the word cloud of <code>item_desciption</code>, there are words such as <code>shipping</code> and <code>free</code>, which may stand for free shipping and other meanings, and there is some duplication with the <code>shipping</code> attribute, and using it as a feature word to train the model will cause interference.</li><li>The information contained in a single keyword may not be comprehensive, and there may be great correlation between keywords.</li><li>In the final model <code>MLP</code> uses a four-layer perceptron, and the number of layers of the perceptron and the input size of each layer can be further tuned.</li></ol><h2 id="Experience"><a href="#Experience" class="headerlink" title="Experience"></a>Experience</h2><p>This experiment was very difficult and I didn’t know where to start.</p><p>After carefully studying the sample code given in the course and the content of data visualization and analysis, I got a preliminary understanding of both the dataset and the method of prediction.  </p><p>Since I was very unfamiliar with models such as <code>MLP</code> and <code>Lightgbm</code>, I started from the input point of view and experimented with the combination of different attributes to get the final and better results.  </p><p>In the following study, we should learn and understand the model more deeply, and try to create the model independently, instead of modifying other models that have been written.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]. <a href="https://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html">https://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html</a></p><p>[2]. <a href="https://github.com/pjankiewicz/mercari-solution">https://github.com/pjankiewicz/mercari-solution</a></p><p>[3]. <a href="https://www.kaggle.com/thykhuely/mercari-interactive-eda-topic-modelling">https://www.kaggle.com/thykhuely/mercari-interactive-eda-topic-modelling</a></p><p>[4]. <a href="https://wklchris.github.io/Py3-pandas.html#统计信息dfdescribe-svalue_counts--unique">https://wklchris.github.io/Py3-pandas.html#统计信息dfdescribe-svalue_counts--unique</a></p><p>[5]. <a href="https://zh.wikipedia.org/wiki/多层感知器">https://zh.wikipedia.org/wiki/多层感知器</a></p><p>[6]. <a href="https://blog.csdn.net/weixin_39807102/article/details/81912566">https://blog.csdn.net/weixin_39807102/article/details/81912566</a></p><p>[7]. <a href="https://github.com/maiwen/NLP">https://github.com/maiwen/NLP</a></p><p>[8]. <a href="https://zh.wikipedia.org/wiki/正则表达式">https://zh.wikipedia.org/wiki/正则表达式</a></p><p>[9]. <a href="https://blog.csdn.net/u012609509/article/details/72911564">https://blog.csdn.net/u012609509/article/details/72911564</a></p><p>[10]. <a href="https://www.kaggle.com/tunguz/more-effective-ridge-lgbm-script-lb-0-44823">https://www.kaggle.com/tunguz/more-effective-ridge-lgbm-script-lb-0-44823</a></p><p>[11]. <a href="https://qiita.com/kazuhirokomoda/items/1e9b7ebcacf264b2d814">https://qiita.com/kazuhirokomoda/items/1e9b7ebcacf264b2d814</a></p><p>[12]. <a href="https://www.jianshu.com/p/c532424541ad">https://www.jianshu.com/p/c532424541ad</a></p><p>[13]. <a href="https://www.jiqizhixin.com/articles/2017-11-13-7">https://www.jiqizhixin.com/articles/2017-11-13-7</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;This is a task in the course &lt;code&gt;Introduction to Data Science&lt;/code&gt;. Our team was going to predict the price of products by their description. &lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://enblog.crocodilezs.top/categories/Machine-Learning/"/>
    
    
    <category term="Natural Language Processing" scheme="https://enblog.crocodilezs.top/tags/Natural-Language-Processing/"/>
    
    <category term="Machine Learning" scheme="https://enblog.crocodilezs.top/tags/Machine-Learning/"/>
    
    <category term="MLP" scheme="https://enblog.crocodilezs.top/tags/MLP/"/>
    
    <category term="LightGBM" scheme="https://enblog.crocodilezs.top/tags/LightGBM/"/>
    
  </entry>
  
  <entry>
    <title>Forward and backward propagation in neural networks</title>
    <link href="https://enblog.crocodilezs.top/201911/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BD%9C%E4%B8%9A%E6%8A%A5%E5%91%8A/"/>
    <id>https://enblog.crocodilezs.top/201911/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BD%9C%E4%B8%9A%E6%8A%A5%E5%91%8A/</id>
    <published>2019-10-28T17:32:10.000Z</published>
    <updated>2022-08-28T11:49:59.057Z</updated>
    
    <content type="html"><![CDATA[<p>Deduce forward propagation and back propagation algorithms of neural network with single hidden layer, and program (neural network in ‘Sklearn’ can be used).</p><ul><li><p>discuss the impact of 10,30,100,300,1000, different number of hidden nodes on network performance.</p></li><li><p>Explore the influence of different learning rate and iteration times on network performance.</p></li><li><p>Change the standardized method of data to explore the impact on training.</p></li></ul><span id="more"></span><h2 id="Derivation"><a href="#Derivation" class="headerlink" title="Derivation"></a>Derivation</h2><p><img src="https://bu.dusays.com/2022/08/28/630b53fc6a583.jpg" alt=""><br><img src="https://bu.dusays.com/2022/08/28/630b53fd672ca.jpg" alt=""><br><img src="https://bu.dusays.com/2022/08/28/630b53fe55420.jpg" alt=""><br><img src="https://bu.dusays.com/2022/08/28/630b53ff3b197.jpg" alt=""></p><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><h3 id="Load-data"><a href="#Load-data" class="headerlink" title="Load data"></a>Load data</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1、载入数据</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.examples.tutorials.mnist.input_data <span class="keyword">as</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取mnist数据</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">&#x27;MNIST_data/&#x27;</span>, one_hot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="Construct-network"><a href="#Construct-network" class="headerlink" title="Construct network"></a>Construct network</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.建立模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.1 构建输入层</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>], name=<span class="string">&#x27;X&#x27;</span>)</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>], name=<span class="string">&#x27;Y&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.2 构建隐藏层</span></span><br><span class="line"><span class="comment"># 隐藏层神经元数量(随意设置）</span></span><br><span class="line">H1_NN = <span class="number">256</span></span><br><span class="line"><span class="comment"># 权重</span></span><br><span class="line">W1 = tf.Variable(tf.random_normal([<span class="number">784</span>, H1_NN]))</span><br><span class="line"><span class="comment"># 偏置项</span></span><br><span class="line">b1 = tf.Variable(tf.zeros([H1_NN]))</span><br><span class="line"></span><br><span class="line">Y1 = tf.nn.relu(tf.matmul(x, W1) + b1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.3 构建输出层</span></span><br><span class="line">W2 = tf.Variable(tf.random_normal([H1_NN, <span class="number">10</span>]))</span><br><span class="line">b2 = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line">forward = tf.matmul(Y1, W2) + b2</span><br><span class="line">pred = tf.nn.softmax(forward)</span><br></pre></td></tr></table></figure><h3 id="Train-the-model"><a href="#Train-the-model" class="headerlink" title="Train the model"></a>Train the model</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.训练模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.1 定义损失函数</span></span><br><span class="line"><span class="comment"># tensorflow提供了下面的函数，用于避免log(0)值为Nan造成数据不稳定</span></span><br><span class="line">loss_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=forward, labels=y))</span><br><span class="line"><span class="comment"># # 交叉熵损失函数</span></span><br><span class="line"><span class="comment"># loss_function = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.2 设置训练参数</span></span><br><span class="line">train_epochs = <span class="number">40</span>  <span class="comment"># 训练轮数</span></span><br><span class="line">batch_size = <span class="number">50</span>  <span class="comment"># 单次训练样本数(批次大小)</span></span><br><span class="line"><span class="comment"># 一轮训练的批次数</span></span><br><span class="line">total_batch = <span class="built_in">int</span>(mnist.train.num_examples / batch_size)</span><br><span class="line">display_step = <span class="number">1</span>  <span class="comment"># 显示粒数</span></span><br><span class="line">learning_rate = <span class="number">0.01</span>  <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.2 选择优化器</span></span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss_function)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.3定义准确率</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(pred, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.4 模型的训练</span></span><br><span class="line"><span class="comment"># 记录训练开始的时间</span></span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">startTime = time()</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(train_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> <span class="built_in">range</span>(total_batch):</span><br><span class="line">        <span class="comment"># 读取批次训练数据</span></span><br><span class="line">        xs, ys = mnist.train.next_batch(batch_size)</span><br><span class="line">        <span class="comment"># 执行批次训练</span></span><br><span class="line">        sess.run(optimizer, feed_dict=&#123;x: xs, y: ys&#125;)</span><br><span class="line">    <span class="comment"># 在total_batch批次数据训练完成后，使用验证数据计算误差和准确率，验证集不分批</span></span><br><span class="line">    loss, acc = sess.run([loss_function, accuracy], feed_dict=&#123;x: mnist.validation.images, y: mnist.validation.labels&#125;)</span><br><span class="line">    <span class="comment"># 打印训练过程中的详细信息</span></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;训练轮次：&#x27;</span>, <span class="string">&#x27;%02d&#x27;</span> % (epoch + <span class="number">1</span>),</span><br><span class="line">              <span class="string">&#x27;损失：&#x27;</span>, <span class="string">&#x27;&#123;:.9f&#125;&#x27;</span>.<span class="built_in">format</span>(loss),</span><br><span class="line">              <span class="string">&#x27;准确率：&#x27;</span>, <span class="string">&#x27;&#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(acc))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练结束&#x27;</span>)</span><br><span class="line"><span class="comment"># 显示总运行时间</span></span><br><span class="line">duration = time() - startTime</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;总运行时间为：&quot;</span>, <span class="string">&quot;&#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(duration))</span><br></pre></td></tr></table></figure><h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 4.评估模型</span></span><br><span class="line">accu_test = sess.run(accuracy,</span><br><span class="line">                     feed_dict=&#123;x: mnist.test.images, y: mnist.test.labels&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试集准确率：&#x27;</span>, accu_test)</span><br></pre></td></tr></table></figure><h3 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 5.应用模型</span></span><br><span class="line">prediction_result = sess.run(tf.argmax(pred, <span class="number">1</span>), feed_dict=&#123;x: mnist.test.images&#125;)</span><br><span class="line"><span class="comment"># 查看预测结果的前10项</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;前10项的结果：&quot;</span>, prediction_result[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.1找出预测错误的样本</span></span><br><span class="line">compare_lists = prediction_result == np.argmax(mnist.test.labels, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(compare_lists)</span><br><span class="line">err_lists = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(compare_lists)) <span class="keyword">if</span> compare_lists[i] == <span class="literal">False</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;预测错误的图片：&#x27;</span>, err_lists)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;预测错误图片的总数：&#x27;</span>, <span class="built_in">len</span>(err_lists))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个输出错误分类的函数</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">print_predict_errs</span>(<span class="params">labels,  <span class="comment"># 标签列表</span></span></span><br><span class="line"><span class="params">                       prediction</span>):  <span class="comment"># 预测值列表</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    compare_lists = (prediction == np.argmax(labels, <span class="number">1</span>))</span><br><span class="line">    err_lists = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(compare_lists)) <span class="keyword">if</span> compare_lists[i] == <span class="literal">False</span>]</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> err_lists:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;index=&#x27;</span> + <span class="built_in">str</span>(x) + <span class="string">&#x27;标签值=&#x27;</span>, np.argmax(labels[x]), <span class="string">&#x27;预测值=&#x27;</span>, prediction[x])</span><br><span class="line">        count = count + <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;总计：&quot;</span> + <span class="built_in">str</span>(count))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print_predict_errs(labels=mnist.test.labels, prediction=prediction_result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_images_labels_prediction</span>(<span class="params">images,  <span class="comment"># 图像列表</span></span></span><br><span class="line"><span class="params">                                  labels,  <span class="comment"># 标签列表</span></span></span><br><span class="line"><span class="params">                                  predication,  <span class="comment"># 预测值列表</span></span></span><br><span class="line"><span class="params">                                  index,  <span class="comment"># 从第index个开始显示</span></span></span><br><span class="line"><span class="params">                                  num=<span class="number">10</span></span>):  <span class="comment"># 缺省一次显示10幅</span></span><br><span class="line">    fig = plt.gcf()  <span class="comment"># 获取当前图表，get current figure</span></span><br><span class="line">    fig.set_size_inches(<span class="number">10</span>, <span class="number">12</span>)  <span class="comment"># 设为英寸，1英寸=2.53厘米</span></span><br><span class="line">    <span class="keyword">if</span> num &gt; <span class="number">25</span>:</span><br><span class="line">        num = <span class="number">25</span>  <span class="comment"># 最多显示25个子图</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num):</span><br><span class="line">        ax = plt.subplot(<span class="number">5</span>, <span class="number">5</span>, i + <span class="number">1</span>)  <span class="comment"># 获取当前要处理的子图</span></span><br><span class="line">        <span class="comment"># 显示第index图像</span></span><br><span class="line">        ax.imshow(np.reshape(images[index], (<span class="number">28</span>, <span class="number">28</span>)), cmap=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建该图上显示的title</span></span><br><span class="line">        title = <span class="string">&#x27;label=&#x27;</span> + <span class="built_in">str</span>(np.argmax(labels[index]))</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(predication) &gt; <span class="number">0</span>:</span><br><span class="line">            title += <span class="string">&quot;,predict=&quot;</span> + <span class="built_in">str</span>(predication[index])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 显示图上的title信息</span></span><br><span class="line">        ax.set_title(title, fontsize=<span class="number">10</span>)</span><br><span class="line">        ax.set_xticks([])  <span class="comment"># 不显示坐标轴</span></span><br><span class="line">        ax.set_yticks([])</span><br><span class="line">        index += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plot_images_labels_prediction(mnist.test.images,</span><br><span class="line">                              mnist.test.labels,</span><br><span class="line">                              prediction_result, <span class="number">10</span>, <span class="number">25</span>)</span><br><span class="line">plot_images_labels_prediction(mnist.test.images,</span><br><span class="line">                              mnist.test.labels,</span><br><span class="line">                              prediction_result, <span class="number">610</span>, <span class="number">20</span>)</span><br></pre></td></tr></table></figure><h3 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h3><p>The number of hidden layer nodes: 256<br>Learning Rate: 0.01<br>Epochs: 40</p><p><img src="https://bu.dusays.com/2022/08/28/630b53f3722ee.jpg" alt=""></p><p><img src="https://bu.dusays.com/2022/08/28/630b53f4b3bb8.jpg" alt=""></p><p><img src="https://bu.dusays.com/2022/08/28/630b53f5a44f2.jpg" alt=""></p><h2 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h2><h3 id="The-number-of-hidden-layer-nodes"><a href="#The-number-of-hidden-layer-nodes" class="headerlink" title="The number of hidden layer nodes"></a>The number of hidden layer nodes</h3><p>The number of hidden layer nodes: 10</p><p><img src="https://bu.dusays.com/2022/08/28/630b53f03d7e8.jpg" alt=""></p><div class="table-container"><table><thead><tr><th style="text-align:center"># of hidden layer nodes</th><th style="text-align:center">Running Time/s</th><th style="text-align:center">False</th><th style="text-align:center">Acc.</th></tr></thead><tbody><tr><td style="text-align:center">10</td><td style="text-align:center">46.29</td><td style="text-align:center">736</td><td style="text-align:center">0.9264</td></tr><tr><td style="text-align:center">30</td><td style="text-align:center">43.46</td><td style="text-align:center">528</td><td style="text-align:center">0.9472</td></tr><tr><td style="text-align:center">100</td><td style="text-align:center">59.06</td><td style="text-align:center">343</td><td style="text-align:center">0.9657</td></tr><tr><td style="text-align:center">256</td><td style="text-align:center">84.48</td><td style="text-align:center">249</td><td style="text-align:center">0.9751</td></tr><tr><td style="text-align:center">300</td><td style="text-align:center">76.64</td><td style="text-align:center">269</td><td style="text-align:center">0.9731</td></tr><tr><td style="text-align:center">1000</td><td style="text-align:center">302.27</td><td style="text-align:center">240</td><td style="text-align:center">0.976</td></tr></tbody></table></div><p>As can be seen from the table, the accuracy increases with the increase of the number of hidden layer nodes, and the increase rate gradually decreases.</p><h3 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h3><div class="table-container"><table><thead><tr><th style="text-align:center">LR</th><th style="text-align:center">Running Time/s</th><th style="text-align:center">False</th><th style="text-align:center">Acc.</th></tr></thead><tbody><tr><td style="text-align:center">0.005</td><td style="text-align:center">78.81</td><td style="text-align:center">231</td><td style="text-align:center">0.9769</td></tr><tr><td style="text-align:center">0.01</td><td style="text-align:center">84.48</td><td style="text-align:center">249</td><td style="text-align:center">0.9751</td></tr><tr><td style="text-align:center">0.02</td><td style="text-align:center">69.72</td><td style="text-align:center">446</td><td style="text-align:center">0.9554</td></tr><tr><td style="text-align:center">0.1</td><td style="text-align:center">73.87</td><td style="text-align:center">2561</td><td style="text-align:center">0.7439</td></tr></tbody></table></div><p>As can be seen from the table, the accuracy decreases with the increase of the learning rate. When the learning rate is lower than 0.01, the rate of improvement of image classification accuracy is small.</p><h3 id="Epochs"><a href="#Epochs" class="headerlink" title="Epochs"></a>Epochs</h3><div class="table-container"><table><thead><tr><th style="text-align:center">Epochs</th><th style="text-align:center">Running Time/s</th><th style="text-align:center">False</th><th style="text-align:center">Acc.</th></tr></thead><tbody><tr><td style="text-align:center">20</td><td style="text-align:center">37.12</td><td style="text-align:center">307</td><td style="text-align:center">0.9693</td></tr><tr><td style="text-align:center">40</td><td style="text-align:center">84.48</td><td style="text-align:center">249</td><td style="text-align:center">0.9751</td></tr><tr><td style="text-align:center">100</td><td style="text-align:center">184.39</td><td style="text-align:center">239</td><td style="text-align:center">0.9761</td></tr></tbody></table></div><p>As can be seen from the table, the number of iterations has a great impact on the total running time, and the accuracy increases with the increase of the number of iterations, but the number of nodes in the hidden layer and the learning rate are the decisive factors for the accuracy.</p><h2 id="Implement-a-one-layer-network"><a href="#Implement-a-one-layer-network" class="headerlink" title="Implement a one-layer network"></a>Implement a one-layer network</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">layer_sizes</span>(<span class="params">X, Y</span>):</span><br><span class="line">    n_x = X.shape[<span class="number">0</span>] <span class="comment"># size of input layer</span></span><br><span class="line">    n_h = <span class="number">4</span> <span class="comment"># size of hidden layer</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>] <span class="comment"># size of output layer</span></span><br><span class="line">    <span class="keyword">return</span> (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_parameters</span>(<span class="params">n_x, n_h, n_y</span>):</span><br><span class="line">    W1 = np.random.randn(n_h, n_x)*<span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)*<span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>)) </span><br><span class="line">   </span><br><span class="line">    <span class="keyword">assert</span> (W1.shape == (n_h, n_x))    </span><br><span class="line">    <span class="keyword">assert</span> (b1.shape == (n_h, <span class="number">1</span>))    </span><br><span class="line">    <span class="keyword">assert</span> (W2.shape == (n_y, n_h))    </span><br><span class="line">    <span class="keyword">assert</span> (b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line">    parameters = &#123;<span class="string">&quot;W1&quot;</span>: W1, </span><br><span class="line">                  <span class="string">&quot;b1&quot;</span>: b1,                 </span><br><span class="line">                  <span class="string">&quot;W2&quot;</span>: W2,                  </span><br><span class="line">                  <span class="string">&quot;b2&quot;</span>: b2&#125;   </span><br><span class="line">                   </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_propagation</span>(<span class="params">X, parameters</span>):</span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary &quot;parameters&quot;</span></span><br><span class="line">    W1 = parameters[<span class="string">&#x27;W1&#x27;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&#x27;b1&#x27;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&#x27;W2&#x27;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&#x27;b2&#x27;</span>]    </span><br><span class="line">    <span class="comment"># Implement Forward Propagation to calculate A2 (probabilities)</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = np.tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2, Z1) + b2</span><br><span class="line">    A2 = sigmoid(Z2)    </span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = &#123;<span class="string">&quot;Z1&quot;</span>: Z1,                   </span><br><span class="line">             <span class="string">&quot;A1&quot;</span>: A1,                   </span><br><span class="line">             <span class="string">&quot;Z2&quot;</span>: Z2,                  </span><br><span class="line">             <span class="string">&quot;A2&quot;</span>: A2&#125;    </span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost</span>(<span class="params">A2, Y, parameters</span>):</span><br><span class="line">    m = Y.shape[<span class="number">1</span>] <span class="comment"># number of example</span></span><br><span class="line">    <span class="comment"># Compute the cross-entropy cost</span></span><br><span class="line">    logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(<span class="number">1</span>-A2), <span class="number">1</span>-Y)</span><br><span class="line">    cost = -<span class="number">1</span>/m * np.<span class="built_in">sum</span>(logprobs)</span><br><span class="line">    cost = np.squeeze(cost)     <span class="comment"># makes sure cost is the dimension we expect.</span></span><br><span class="line">    <span class="keyword">assert</span>(<span class="built_in">isinstance</span>(cost, <span class="built_in">float</span>))    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backward_propagation</span>(<span class="params">parameters, cache, X, Y</span>):</span><br><span class="line">    m = X.shape[<span class="number">1</span>]    </span><br><span class="line">    <span class="comment"># First, retrieve W1 and W2 from the dictionary &quot;parameters&quot;.</span></span><br><span class="line">    W1 = parameters[<span class="string">&#x27;W1&#x27;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&#x27;W2&#x27;</span>]    </span><br><span class="line">    <span class="comment"># Retrieve also A1 and A2 from dictionary &quot;cache&quot;.</span></span><br><span class="line">    A1 = cache[<span class="string">&#x27;A1&#x27;</span>]</span><br><span class="line">    A2 = cache[<span class="string">&#x27;A2&#x27;</span>]    </span><br><span class="line">    <span class="comment"># Backward propagation: calculate dW1, db1, dW2, db2. </span></span><br><span class="line">    dZ2 = A2-Y</span><br><span class="line">    dW2 = <span class="number">1</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1</span>/m * np.<span class="built_in">sum</span>(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dZ1 = np.dot(W2.T, dZ2)*(<span class="number">1</span>-np.power(A1, <span class="number">2</span>))</span><br><span class="line">    dW1 = <span class="number">1</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1</span>/m * np.<span class="built_in">sum</span>(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    grads = &#123;<span class="string">&quot;dW1&quot;</span>: dW1,</span><br><span class="line">             <span class="string">&quot;db1&quot;</span>: db1,                      </span><br><span class="line">             <span class="string">&quot;dW2&quot;</span>: dW2,             </span><br><span class="line">             <span class="string">&quot;db2&quot;</span>: db2&#125;   </span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">update_parameters</span>(<span class="params">parameters, grads, learning_rate = <span class="number">1.2</span></span>):</span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary &quot;parameters&quot;</span></span><br><span class="line">    W1 = parameters[<span class="string">&#x27;W1&#x27;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&#x27;b1&#x27;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&#x27;W2&#x27;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&#x27;b2&#x27;</span>]    </span><br><span class="line">    <span class="comment"># Retrieve each gradient from the dictionary &quot;grads&quot;</span></span><br><span class="line">    dW1 = grads[<span class="string">&#x27;dW1&#x27;</span>]</span><br><span class="line">    db1 = grads[<span class="string">&#x27;db1&#x27;</span>]</span><br><span class="line">    dW2 = grads[<span class="string">&#x27;dW2&#x27;</span>]</span><br><span class="line">    db2 = grads[<span class="string">&#x27;db2&#x27;</span>]    </span><br><span class="line">    <span class="comment"># Update rule for each parameter</span></span><br><span class="line">    W1 -= dW1 * learning_rate</span><br><span class="line">    b1 -= db1 * learning_rate</span><br><span class="line">    W2 -= dW2 * learning_rate</span><br><span class="line">    b2 -= db2 * learning_rate</span><br><span class="line">    parameters = &#123;<span class="string">&quot;W1&quot;</span>: W1, </span><br><span class="line">                  <span class="string">&quot;b1&quot;</span>: b1,            </span><br><span class="line">                  <span class="string">&quot;W2&quot;</span>: W2,   </span><br><span class="line">                  <span class="string">&quot;b2&quot;</span>: b2&#125;    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">nn_model</span>(<span class="params">X, Y, n_h, num_iterations = <span class="number">10000</span>, print_cost=<span class="literal">False</span></span>):</span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>]</span><br><span class="line">    n_y = layer_sizes(X, Y)[<span class="number">2</span>]    </span><br><span class="line">    <span class="comment"># Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: &quot;n_x, n_h, n_y&quot;. Outputs = &quot;W1, b1, W2, b2, parameters&quot;.</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">    W1 = parameters[<span class="string">&#x27;W1&#x27;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&#x27;b1&#x27;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&#x27;W2&#x27;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&#x27;b2&#x27;</span>]    </span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_iterations):        </span><br><span class="line">    <span class="comment"># Forward propagation. Inputs: &quot;X, parameters&quot;. Outputs: &quot;A2, cache&quot;.</span></span><br><span class="line">        A2, cache = forward_propagation(X, parameters)        </span><br><span class="line">        <span class="comment"># Cost function. Inputs: &quot;A2, Y, parameters&quot;. Outputs: &quot;cost&quot;.</span></span><br><span class="line">        cost = compute_cost(A2, Y, parameters)        </span><br><span class="line">        <span class="comment"># Backpropagation. Inputs: &quot;parameters, cache, X, Y&quot;. Outputs: &quot;grads&quot;.</span></span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)        </span><br><span class="line">        <span class="comment"># Gradient descent parameter update. Inputs: &quot;parameters, grads&quot;. Outputs: &quot;parameters&quot;.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate=<span class="number">1.2</span>)        </span><br><span class="line">        <span class="comment"># Print the cost every 1000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:            </span><br><span class="line">            <span class="built_in">print</span> (<span class="string">&quot;Cost after iteration %i: %f&quot;</span> %(i, cost))    </span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;Deduce forward propagation and back propagation algorithms of neural network with single hidden layer, and program (neural network in ‘Sklearn’ can be used).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;discuss the impact of 10,30,100,300,1000, different number of hidden nodes on network performance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Explore the influence of different learning rate and iteration times on network performance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Change the standardized method of data to explore the impact on training.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://enblog.crocodilezs.top/categories/Machine-Learning/"/>
    
    
    <category term="Neural Networks" scheme="https://enblog.crocodilezs.top/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>Finds and ID3 Algorithm</title>
    <link href="https://enblog.crocodilezs.top/201910/FINDS%E7%AE%97%E6%B3%95%E5%92%8CID3%E7%AE%97%E6%B3%95/"/>
    <id>https://enblog.crocodilezs.top/201910/FINDS%E7%AE%97%E6%B3%95%E5%92%8CID3%E7%AE%97%E6%B3%95/</id>
    <published>2019-10-28T04:21:10.000Z</published>
    <updated>2022-08-27T13:46:06.517Z</updated>
    
    <content type="html"><![CDATA[<p>Implement <code>FindS</code> and <code>ID3</code> algorithms.</p><span id="more"></span><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><h3 id="FINDS"><a href="#FINDS" class="headerlink" title="FINDS"></a><code>FINDS</code></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string"> Created on 2019/10/21 21:02</span></span><br><span class="line"><span class="string"> FINDS</span></span><br><span class="line"><span class="string"> @Author  : Zhouy</span></span><br><span class="line"><span class="string"> @Blog    : www.crocodilezs.top</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create dataset</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">CreateDataset</span>():</span><br><span class="line">    dataset = [[<span class="string">&#x27;Sunny&#x27;</span>, <span class="string">&#x27;Warm&#x27;</span>, <span class="string">&#x27;Normal&#x27;</span>, <span class="string">&#x27;Strong&#x27;</span>, <span class="string">&#x27;Warm&#x27;</span>, <span class="string">&#x27;Same&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>],</span><br><span class="line">               [<span class="string">&#x27;Sunny&#x27;</span>, <span class="string">&#x27;Warm&#x27;</span>, <span class="string">&#x27;High&#x27;</span>, <span class="string">&#x27;Strong&#x27;</span>, <span class="string">&#x27;Warm&#x27;</span>, <span class="string">&#x27;Same&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>],</span><br><span class="line">               [<span class="string">&#x27;Rainy&#x27;</span>, <span class="string">&#x27;Cold&#x27;</span>, <span class="string">&#x27;High&#x27;</span>, <span class="string">&#x27;Strong&#x27;</span>, <span class="string">&#x27;Warm&#x27;</span>, <span class="string">&#x27;Change&#x27;</span>, <span class="string">&#x27;No&#x27;</span>],</span><br><span class="line">               [<span class="string">&#x27;Sunny&#x27;</span>, <span class="string">&#x27;Warm&#x27;</span>, <span class="string">&#x27;High&#x27;</span>, <span class="string">&#x27;Strong&#x27;</span>, <span class="string">&#x27;Cold&#x27;</span>, <span class="string">&#x27;Change&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>]]</span><br><span class="line">    labels = [<span class="string">&#x27;Sky&#x27;</span>, <span class="string">&#x27;Temp&#x27;</span>, <span class="string">&#x27;Humidity&#x27;</span>, <span class="string">&#x27;Wind&#x27;</span>, <span class="string">&#x27;Water&#x27;</span>, <span class="string">&#x27;Forest&#x27;</span>, <span class="string">&#x27;OutdoorSport&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> dataset, labels</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find one version space by using FINDS</span></span><br><span class="line"><span class="comment"># &#x27;/&#x27; means null, and &#x27;*&#x27; means generalization</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">FINDS</span>(<span class="params">dataset</span>):</span><br><span class="line">    constraint = [<span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;/&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> dataset:</span><br><span class="line">        <span class="keyword">if</span> item[-<span class="number">1</span>] == <span class="string">&#x27;Yes&#x27;</span>:</span><br><span class="line">            <span class="comment"># only go through positive instances</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(item)-<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span>(item[i] != constraint[i] <span class="keyword">and</span> constraint[i] != <span class="string">&#x27;*&#x27;</span>):</span><br><span class="line">                    <span class="keyword">if</span>(constraint[i] == <span class="string">&#x27;/&#x27;</span>):</span><br><span class="line">                        constraint[i] = item[i]</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        constraint[i] = <span class="string">&#x27;*&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> constraint</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    dataset, labels = CreateDataset()</span><br><span class="line">    constraint = FINDS(dataset)</span><br><span class="line">    <span class="built_in">print</span>(constraint)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h3 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a><code>ID3</code></h3><p><code>myTrees.py</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string"> Created on 2019/10/22 11:59</span></span><br><span class="line"><span class="string"> myTrees</span></span><br><span class="line"><span class="string"> @Author  : Zhouy</span></span><br><span class="line"><span class="string"> @Blog    : www.crocodilezs.top</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 原始数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createDataSet</span>():</span><br><span class="line">    dataSet = [[<span class="number">1</span>, <span class="number">1</span>, <span class="string">&#x27;yes&#x27;</span>],</span><br><span class="line">               [<span class="number">1</span>, <span class="number">1</span>, <span class="string">&#x27;yes&#x27;</span>],</span><br><span class="line">               [<span class="number">1</span>, <span class="number">0</span>, <span class="string">&#x27;no&#x27;</span>],</span><br><span class="line">               [<span class="number">0</span>, <span class="number">1</span>, <span class="string">&#x27;no&#x27;</span>],</span><br><span class="line">               [<span class="number">0</span>, <span class="number">1</span>, <span class="string">&#x27;no&#x27;</span>]]</span><br><span class="line">    labels = [<span class="string">&#x27;no surfacing&#x27;</span>,<span class="string">&#x27;flippers&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> dataSet, labels</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多数表决器</span></span><br><span class="line"><span class="comment"># 列中相同值数量最多为结果</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">majorityCnt</span>(<span class="params">classList</span>):</span><br><span class="line">    classCounts = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> classList:</span><br><span class="line">        <span class="keyword">if</span> (value <span class="keyword">not</span> <span class="keyword">in</span> classCounts.keys()):</span><br><span class="line">            classCounts[value] = <span class="number">0</span></span><br><span class="line">        classCounts[value] += <span class="number">1</span></span><br><span class="line">    sortedClassCount = <span class="built_in">sorted</span>(classCounts.iteritems(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分数据集</span></span><br><span class="line"><span class="comment"># dataSet:原始数据集</span></span><br><span class="line"><span class="comment"># axis:进行分割的指定列索引</span></span><br><span class="line"><span class="comment"># value:指定列中的值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">splitDataSet</span>(<span class="params">dataSet, axis, value</span>):</span><br><span class="line">    retDataSet = []</span><br><span class="line">    <span class="keyword">for</span> featDataVal <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">if</span> featDataVal[axis] == value:</span><br><span class="line">            <span class="comment"># 下面两行去除某一项指定列的值，很巧妙有没有</span></span><br><span class="line">            reducedFeatVal = featDataVal[:axis]</span><br><span class="line">            reducedFeatVal.extend(featDataVal[axis + <span class="number">1</span>:])</span><br><span class="line">            retDataSet.append(reducedFeatVal)</span><br><span class="line">    <span class="keyword">return</span> retDataSet</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算香农熵</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcShannonEnt</span>(<span class="params">dataSet</span>):</span><br><span class="line">    <span class="comment"># 数据集总项数</span></span><br><span class="line">    numEntries = <span class="built_in">len</span>(dataSet)</span><br><span class="line">    <span class="comment"># 标签计数对象初始化</span></span><br><span class="line">    labelCounts = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> featDataVal <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="comment"># 获取数据集每一项的最后一列的标签值</span></span><br><span class="line">        currentLabel = featDataVal[-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 如果当前标签不在标签存储对象里，则初始化，然后计数</span></span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():</span><br><span class="line">            labelCounts[currentLabel] = <span class="number">0</span></span><br><span class="line">        labelCounts[currentLabel] += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 熵初始化</span></span><br><span class="line">    shannonEnt = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># 遍历标签对象，求概率，计算熵</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts.keys():</span><br><span class="line">        prop = labelCounts[key] / <span class="built_in">float</span>(numEntries)</span><br><span class="line">        shannonEnt -= prop * log(prop, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> shannonEnt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 选出最优特征列索引</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chooseBestFeatureToSplit</span>(<span class="params">dataSet</span>):</span><br><span class="line">    <span class="comment"># 计算特征个数，dataSet最后一列是标签属性，不是特征量</span></span><br><span class="line">    numFeatures = <span class="built_in">len</span>(dataSet[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">    <span class="comment"># 计算初始数据香农熵</span></span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)</span><br><span class="line">    <span class="comment"># 初始化信息增益，最优划分特征列索引</span></span><br><span class="line">    bestInfoGain = <span class="number">0.0</span></span><br><span class="line">    bestFeatureIndex = -<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numFeatures):</span><br><span class="line">        <span class="comment"># 获取每一列数据</span></span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">        <span class="comment"># 将每一列数据去重</span></span><br><span class="line">        uniqueVals = <span class="built_in">set</span>(featList)</span><br><span class="line">        newEntropy = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">            subDataSet = splitDataSet(dataSet, i, value)</span><br><span class="line">            <span class="comment"># 计算条件概率</span></span><br><span class="line">            prob = <span class="built_in">len</span>(subDataSet) / <span class="built_in">float</span>(<span class="built_in">len</span>(dataSet))</span><br><span class="line">            <span class="comment"># 计算条件熵</span></span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataSet)</span><br><span class="line">        <span class="comment"># 计算信息增益</span></span><br><span class="line">        infoGain = baseEntropy - newEntropy</span><br><span class="line">        <span class="keyword">if</span> (infoGain &gt; bestInfoGain):</span><br><span class="line">            bestInfoGain = infoGain</span><br><span class="line">            bestFeatureIndex = i</span><br><span class="line">    <span class="keyword">return</span> bestFeatureIndex</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 决策树创建</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createTree</span>(<span class="params">dataSet, labels</span>):</span><br><span class="line">    <span class="comment"># 获取标签属性，dataSet最后一列，区别于labels标签名称</span></span><br><span class="line">    classList = [example[-<span class="number">1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    <span class="comment"># 树极端终止条件判断</span></span><br><span class="line">    <span class="comment"># 标签属性值全部相同，返回标签属性第一项值</span></span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == <span class="built_in">len</span>(classList):</span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 只有一个特征（1列）</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(dataSet[<span class="number">0</span>]) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)</span><br><span class="line">    <span class="comment"># 获取最优特征列索引</span></span><br><span class="line">    bestFeatureIndex = chooseBestFeatureToSplit(dataSet)</span><br><span class="line">    <span class="comment"># 获取最优索引对应的标签名称</span></span><br><span class="line">    bestFeatureLabel = labels[bestFeatureIndex]</span><br><span class="line">    <span class="comment"># 创建根节点</span></span><br><span class="line">    myTree = &#123;bestFeatureLabel: &#123;&#125;&#125;</span><br><span class="line">    <span class="comment"># 去除最优索引对应的标签名，使labels标签能正确遍历</span></span><br><span class="line">    <span class="keyword">del</span> (labels[bestFeatureIndex])</span><br><span class="line">    <span class="comment"># 获取最优列</span></span><br><span class="line">    bestFeature = [example[bestFeatureIndex] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    uniquesVals = <span class="built_in">set</span>(bestFeature)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniquesVals:</span><br><span class="line">        <span class="comment"># 子标签名称集合</span></span><br><span class="line">        subLabels = labels[:]</span><br><span class="line">        <span class="comment"># 递归</span></span><br><span class="line">        myTree[bestFeatureLabel][value] = createTree(splitDataSet(dataSet, bestFeatureIndex, value), subLabels)</span><br><span class="line">    <span class="keyword">return</span> myTree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取分类结果</span></span><br><span class="line"><span class="comment"># inputTree:决策树字典</span></span><br><span class="line"><span class="comment"># featLabels:标签列表</span></span><br><span class="line"><span class="comment"># testVec:测试向量  例如：简单实例下某一路径 [1,1]  =&gt; yes（树干值组合，从根结点到叶子节点）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">classify</span>(<span class="params">inputTree, featLabels, testVec</span>):</span><br><span class="line">    <span class="comment"># 获取根结点名称，将dict转化为list</span></span><br><span class="line">    firstSide = <span class="built_in">list</span>(inputTree.keys())</span><br><span class="line">    <span class="comment"># 根结点名称String类型</span></span><br><span class="line">    firstStr = firstSide[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 获取根结点对应的子节点</span></span><br><span class="line">    secondDict = inputTree[firstStr]</span><br><span class="line">    <span class="comment"># 获取根结点名称在标签列表中对应的索引</span></span><br><span class="line">    featIndex = featLabels.index(firstStr)</span><br><span class="line">    <span class="comment"># 由索引获取向量表中的对应值</span></span><br><span class="line">    key = testVec[featIndex]</span><br><span class="line">    <span class="comment"># 获取树干向量后的对象</span></span><br><span class="line">    valueOfFeat = secondDict[key]</span><br><span class="line">    <span class="comment"># 判断是子结点还是叶子节点：子结点就回调分类函数，叶子结点就是分类结果</span></span><br><span class="line">    <span class="comment"># if type(valueOfFeat).__name__==&#x27;dict&#x27;: 等价 if isinstance(valueOfFeat, dict):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(valueOfFeat, <span class="built_in">dict</span>):</span><br><span class="line">        classLabel = classify(valueOfFeat, featLabels, testVec)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        classLabel = valueOfFeat</span><br><span class="line">    <span class="keyword">return</span> classLabel</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将决策树分类器存储在磁盘中，filename一般保存为txt格式</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">storeTree</span>(<span class="params">inputTree, filename</span>):</span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fw = <span class="built_in">open</span>(filename, <span class="string">&#x27;wb+&#x27;</span>)</span><br><span class="line">    pickle.dump(inputTree, fw)</span><br><span class="line">    fw.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将瓷盘中的对象加载出来，这里的filename就是上面函数中的txt文件</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">grabTree</span>(<span class="params">filename</span>):</span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fr = <span class="built_in">open</span>(filename, <span class="string">&#x27;rb&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> pickle.load(fr)</span><br></pre></td></tr></table></figure><p><code>treePlotter.py</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string"> Created on 2019/10/22 12:00</span></span><br><span class="line"><span class="string"> treePlotter</span></span><br><span class="line"><span class="string"> @Author  : Zhouy</span></span><br><span class="line"><span class="string"> @Blog    : www.crocodilezs.top</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">decisionNode = <span class="built_in">dict</span>(boxstyle=<span class="string">&quot;sawtooth&quot;</span>, fc=<span class="string">&quot;0.8&quot;</span>)</span><br><span class="line">leafNode = <span class="built_in">dict</span>(boxstyle=<span class="string">&quot;round4&quot;</span>, fc=<span class="string">&quot;0.8&quot;</span>)</span><br><span class="line">arrow_args = <span class="built_in">dict</span>(arrowstyle=<span class="string">&quot;&lt;-&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取树的叶子节点</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getNumLeafs</span>(<span class="params">myTree</span>):</span><br><span class="line">    numLeafs = <span class="number">0</span></span><br><span class="line">    <span class="comment"># dict转化为list</span></span><br><span class="line">    firstSides = <span class="built_in">list</span>(myTree.keys())</span><br><span class="line">    firstStr = firstSides[<span class="number">0</span>]</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="comment"># 判断是否是叶子节点（通过类型判断，子类不存在，则类型为str；子类存在，则为dict）</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(secondDict[</span><br><span class="line">                    key]).__name__ == <span class="string">&#x27;dict&#x27;</span>:  <span class="comment"># test to see if the nodes are dictonaires, if not they are leaf nodes</span></span><br><span class="line">            numLeafs += getNumLeafs(secondDict[key])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            numLeafs += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> numLeafs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取树的层数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getTreeDepth</span>(<span class="params">myTree</span>):</span><br><span class="line">    maxDepth = <span class="number">0</span></span><br><span class="line">    <span class="comment"># dict转化为list</span></span><br><span class="line">    firstSides = <span class="built_in">list</span>(myTree.keys())</span><br><span class="line">    firstStr = firstSides[<span class="number">0</span>]</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(secondDict[</span><br><span class="line">                    key]).__name__ == <span class="string">&#x27;dict&#x27;</span>:  <span class="comment"># test to see if the nodes are dictonaires, if not they are leaf nodes</span></span><br><span class="line">            thisDepth = <span class="number">1</span> + getTreeDepth(secondDict[key])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            thisDepth = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> thisDepth &gt; maxDepth: maxDepth = thisDepth</span><br><span class="line">    <span class="keyword">return</span> maxDepth</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotNode</span>(<span class="params">nodeTxt, centerPt, parentPt, nodeType</span>):</span><br><span class="line">    createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords=<span class="string">&#x27;axes fraction&#x27;</span>,</span><br><span class="line">                            xytext=centerPt, textcoords=<span class="string">&#x27;axes fraction&#x27;</span>,</span><br><span class="line">                            va=<span class="string">&quot;center&quot;</span>, ha=<span class="string">&quot;center&quot;</span>, bbox=nodeType, arrowprops=arrow_args)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotMidText</span>(<span class="params">cntrPt, parentPt, txtString</span>):</span><br><span class="line">    xMid = (parentPt[<span class="number">0</span>] - cntrPt[<span class="number">0</span>]) / <span class="number">2.0</span> + cntrPt[<span class="number">0</span>]</span><br><span class="line">    yMid = (parentPt[<span class="number">1</span>] - cntrPt[<span class="number">1</span>]) / <span class="number">2.0</span> + cntrPt[<span class="number">1</span>]</span><br><span class="line">    createPlot.ax1.text(xMid, yMid, txtString, va=<span class="string">&quot;center&quot;</span>, ha=<span class="string">&quot;center&quot;</span>, rotation=<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotTree</span>(<span class="params">myTree, parentPt, nodeTxt</span>):  <span class="comment"># if the first key tells you what feat was split on</span></span><br><span class="line">    numLeafs = getNumLeafs(myTree)  <span class="comment"># this determines the x width of this tree</span></span><br><span class="line">    depth = getTreeDepth(myTree)</span><br><span class="line">    firstSides = <span class="built_in">list</span>(myTree.keys())</span><br><span class="line">    firstStr = firstSides[<span class="number">0</span>]  <span class="comment"># the text label for this node should be this</span></span><br><span class="line">    cntrPt = (plotTree.xOff + (<span class="number">1.0</span> + <span class="built_in">float</span>(numLeafs)) / <span class="number">2.0</span> / plotTree.totalW, plotTree.yOff)</span><br><span class="line">    plotMidText(cntrPt, parentPt, nodeTxt)</span><br><span class="line">    plotNode(firstStr, cntrPt, parentPt, decisionNode)</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    plotTree.yOff = plotTree.yOff - <span class="number">1.0</span> / plotTree.totalD</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(secondDict[</span><br><span class="line">                    key]).__name__ == <span class="string">&#x27;dict&#x27;</span>:  <span class="comment"># test to see if the nodes are dictonaires, if not they are leaf nodes</span></span><br><span class="line">            plotTree(secondDict[key], cntrPt, <span class="built_in">str</span>(key))  <span class="comment"># recursion</span></span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># it&#x27;s a leaf node print the leaf node</span></span><br><span class="line">            plotTree.xOff = plotTree.xOff + <span class="number">1.0</span> / plotTree.totalW</span><br><span class="line">            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)</span><br><span class="line">            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, <span class="built_in">str</span>(key))</span><br><span class="line">    plotTree.yOff = plotTree.yOff + <span class="number">1.0</span> / plotTree.totalD</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># if you do get a dictonary you know it&#x27;s a tree, and the first element will be another dict</span></span><br><span class="line"><span class="comment"># 绘制决策树</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createPlot</span>(<span class="params">inTree</span>):</span><br><span class="line">    fig = plt.figure(<span class="number">1</span>, facecolor=<span class="string">&#x27;white&#x27;</span>)</span><br><span class="line">    fig.clf()</span><br><span class="line">    axprops = <span class="built_in">dict</span>(xticks=[], yticks=[])</span><br><span class="line">    createPlot.ax1 = plt.subplot(<span class="number">111</span>, frameon=<span class="literal">False</span>, **axprops)  <span class="comment"># no ticks</span></span><br><span class="line">    <span class="comment"># createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo puropses</span></span><br><span class="line">    plotTree.totalW = <span class="built_in">float</span>(getNumLeafs(inTree))</span><br><span class="line">    plotTree.totalD = <span class="built_in">float</span>(getTreeDepth(inTree))</span><br><span class="line">    plotTree.xOff = -<span class="number">0.5</span> / plotTree.totalW</span><br><span class="line">    plotTree.yOff = <span class="number">1.0</span></span><br><span class="line">    plotTree(inTree, (<span class="number">0.5</span>, <span class="number">1.0</span>), <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制树的根节点和叶子节点（根节点形状：长方形，叶子节点：椭圆形）</span></span><br><span class="line"><span class="comment"># def createPlot():</span></span><br><span class="line"><span class="comment">#    fig = plt.figure(1, facecolor=&#x27;white&#x27;)</span></span><br><span class="line"><span class="comment">#    fig.clf()</span></span><br><span class="line"><span class="comment">#    createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo puropses</span></span><br><span class="line"><span class="comment">#    plotNode(&#x27;a decision node&#x27;, (0.5, 0.1), (0.1, 0.5), decisionNode)</span></span><br><span class="line"><span class="comment">#    plotNode(&#x27;a leaf node&#x27;, (0.8, 0.1), (0.3, 0.8), leafNode)</span></span><br><span class="line"><span class="comment">#    plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">retrieveTree</span>(<span class="params">i</span>):</span><br><span class="line">    listOfTrees = [&#123;<span class="string">&#x27;no surfacing&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;no&#x27;</span>, <span class="number">1</span>: &#123;<span class="string">&#x27;flippers&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;no&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;yes&#x27;</span>&#125;&#125;&#125;&#125;,</span><br><span class="line">                   &#123;<span class="string">&#x27;no surfacing&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;no&#x27;</span>, <span class="number">1</span>: &#123;<span class="string">&#x27;flippers&#x27;</span>: &#123;<span class="number">0</span>: &#123;<span class="string">&#x27;head&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;no&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;yes&#x27;</span>&#125;&#125;, <span class="number">1</span>: <span class="string">&#x27;no&#x27;</span>&#125;&#125;&#125;&#125;</span><br><span class="line">                   ]</span><br><span class="line">    <span class="keyword">return</span> listOfTrees[i]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># thisTree = retrieveTree(0)</span></span><br><span class="line"><span class="comment"># createPlot(thisTree)</span></span><br><span class="line"><span class="comment"># createPlot()</span></span><br><span class="line"><span class="comment"># myTree = retrieveTree(0)</span></span><br><span class="line"><span class="comment"># numLeafs =getNumLeafs(myTree)</span></span><br><span class="line"><span class="comment"># treeDepth =getTreeDepth(myTree)</span></span><br><span class="line"><span class="comment"># print(u&quot;叶子节点数目：%d&quot;% numLeafs)</span></span><br><span class="line"><span class="comment"># print(u&quot;树深度：%d&quot;%treeDepth)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>testTrees_3.py</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string"> Created on 2019/10/22 12:00</span></span><br><span class="line"><span class="string"> testTrees_3</span></span><br><span class="line"><span class="string"> @Author  : Zhouy</span></span><br><span class="line"><span class="string"> @Blog    : www.crocodilezs.top</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> myTrees <span class="keyword">as</span> mt</span><br><span class="line"><span class="keyword">import</span> treePlotter <span class="keyword">as</span> tp</span><br><span class="line"><span class="comment">#测试</span></span><br><span class="line">dataSet, labels = mt.createDataSet()</span><br><span class="line"><span class="comment">#copy函数：新开辟一块内存，然后将list的所有值复制到新开辟的内存中</span></span><br><span class="line">labels1 = labels.copy()</span><br><span class="line"><span class="comment">#createTree函数中将labels1的值改变了，所以在分类测试时不能用labels1</span></span><br><span class="line">myTree = mt.createTree(dataSet,labels1)</span><br><span class="line"><span class="comment">#保存树到本地</span></span><br><span class="line">mt.storeTree(myTree,<span class="string">&#x27;myTree.txt&#x27;</span>)</span><br><span class="line"><span class="comment">#在本地磁盘获取树</span></span><br><span class="line">myTree = mt.grabTree(<span class="string">&#x27;myTree.txt&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">u&quot;决策树结构：%s&quot;</span>%myTree)</span><br><span class="line"><span class="comment">#绘制决策树</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">u&quot;绘制决策树：&quot;</span>)</span><br><span class="line">tp.createPlot(myTree)</span><br><span class="line">numLeafs =tp.getNumLeafs(myTree)</span><br><span class="line">treeDepth =tp.getTreeDepth(myTree)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">u&quot;叶子节点数目：%d&quot;</span>% numLeafs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">u&quot;树深度：%d&quot;</span>%treeDepth)</span><br><span class="line"><span class="comment">#测试分类 简单样本数据3列</span></span><br><span class="line">labelResult =mt.classify(myTree,labels,[<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">u&quot;[1,1] 测试结果为：%s&quot;</span>%labelResult)</span><br><span class="line">labelResult =mt.classify(myTree,labels,[<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">u&quot;[1,0] 测试结果为：%s&quot;</span>%labelResult)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;Implement &lt;code&gt;FindS&lt;/code&gt; and &lt;code&gt;ID3&lt;/code&gt; algorithms.&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://enblog.crocodilezs.top/categories/Machine-Learning/"/>
    
    
    <category term="FindS" scheme="https://enblog.crocodilezs.top/tags/FindS/"/>
    
    <category term="ID3" scheme="https://enblog.crocodilezs.top/tags/ID3/"/>
    
  </entry>
  
  <entry>
    <title>Implementation of KNN and Naive_Bayes</title>
    <link href="https://enblog.crocodilezs.top/201911/KNN%E4%B8%8ENaive_Bayes%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    <id>https://enblog.crocodilezs.top/201911/KNN%E4%B8%8ENaive_Bayes%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/</id>
    <published>2019-10-25T04:32:50.000Z</published>
    <updated>2022-08-29T09:33:45.810Z</updated>
    
    <content type="html"><![CDATA[<p>Implement kNN and Naive Bayes using Breast_cancer dataset.</p><span id="more"></span><h2 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h2><h3 id="Import-dataset"><a href="#Import-dataset" class="headerlink" title="Import dataset"></a>Import dataset</h3><p>Dataset: <a href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic">Breast_cancer</a>)</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> neighbors</span><br><span class="line"></span><br><span class="line">datasets = datasets.load_breast_cancer()</span><br><span class="line">X = datasets.data;</span><br><span class="line">y = datasets.target;</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = <span class="number">0.3</span>, random_state = <span class="number">0</span>)</span><br><span class="line">k = <span class="number">5</span></span><br><span class="line"><span class="comment"># print(datasets.DESCR)</span></span><br><span class="line"><span class="comment"># malignant - 0, benign - 1</span></span><br><span class="line">y_predict = []</span><br></pre></td></tr></table></figure><h3 id="KNN-implementation"><a href="#KNN-implementation" class="headerlink" title="KNN implementation"></a>KNN implementation</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">knn</span>(<span class="params">X_train, y_train, X_test, y_predict</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    对测试集的数据进行预测，得到的结果与y_test比较。用欧式距离进行计算。</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">for</span> test_data <span class="keyword">in</span> X_test:</span><br><span class="line">        first_k_instance = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X_train)):</span><br><span class="line">            distance = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> attributes_no <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X_train[<span class="number">0</span>])):</span><br><span class="line">                distance += (test_data[attributes_no] - X_train[i][attributes_no]) ** <span class="number">2</span></span><br><span class="line">            Euclid_distance = distance ** <span class="number">0.5</span></span><br><span class="line">            <span class="comment">#print(Euclid_distance)</span></span><br><span class="line">            <span class="keyword">if</span> i &lt; k:</span><br><span class="line">                first_k_instance.append((i, Euclid_distance))</span><br><span class="line">            <span class="keyword">elif</span> Euclid_distance &lt; first_k_instance[k-<span class="number">1</span>][<span class="number">1</span>]:</span><br><span class="line">                first_k_instance[k-<span class="number">1</span>] = (i, Euclid_distance)</span><br><span class="line">            first_k_instance = <span class="built_in">sorted</span>(first_k_instance, key = <span class="keyword">lambda</span> x:x[<span class="number">1</span>]) </span><br><span class="line">            <span class="comment">#print(first_k_instance)</span></span><br><span class="line">        <span class="comment"># 现在得到了距离测试点最近的k个点，用多数表决器来判断测试点是良性还是恶性</span></span><br><span class="line">        benign = <span class="number">0</span></span><br><span class="line">        malignant = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> instance <span class="keyword">in</span> first_k_instance:</span><br><span class="line">            <span class="keyword">if</span> y_train[instance[<span class="number">0</span>]] == <span class="number">0</span>:</span><br><span class="line">                malignant += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                benign += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> malignant &gt;= benign:</span><br><span class="line">            y_predict.append(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y_predict.append(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="Calculate-accuracy"><a href="#Calculate-accuracy" class="headerlink" title="Calculate accuracy"></a>Calculate accuracy</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_predict, y_test</span>):</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(y_predict)):</span><br><span class="line">        <span class="keyword">if</span> y_predict[i] == y_test[i]:</span><br><span class="line">            correct += <span class="number">1</span></span><br><span class="line">    accuracy_rate = correct / <span class="built_in">len</span>(y_predict)</span><br><span class="line">    <span class="keyword">return</span> correct, accuracy_rate</span><br></pre></td></tr></table></figure><h3 id="Main-fuction"><a href="#Main-fuction" class="headerlink" title="Main fuction"></a>Main fuction</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    knn(X_train, y_train, X_test, y_predict)</span><br><span class="line">    correct, accuracy_rate = accuracy(y_predict, y_test)</span><br><span class="line">    <span class="built_in">print</span>(y_predict)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;kNN模型测试集预测的准确率为：%.3f&quot;</span> % accuracy_rate);</span><br><span class="line">    KNN = neighbors.KNeighborsClassifier(n_neighbors = <span class="number">5</span>)</span><br><span class="line">    KNN.fit(X_train, y_train)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;sklearn库中kNN模型预测的准确率为：%.3f&quot;</span> % KNN.score(X_test, y_test));</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p> [0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1]<br> The prediction accuracy of kNN model test set is: 0.947<br> The prediction accuracy of kNN model in sklearn library is: 0.947</p><p>Through the results, it can be found that the kNN we implemented is consistent with the kNN provided in sklearn.</p><p>We can further optimize the accuracy by setting the value of k and transforming the strategy of finding similar samples (replacing the Euclidean distance with the matching coefficient or Jaccard, etc.).</p><h2 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive_Bayes"></a>Naive_Bayes</h2><h3 id="Import-Dataset"><a href="#Import-Dataset" class="headerlink" title="Import Dataset"></a>Import Dataset</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># load datasets</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> naive_bayes</span><br><span class="line"></span><br><span class="line">datasets = datasets.load_breast_cancer()</span><br><span class="line">X = datasets.data;</span><br><span class="line">y = datasets.target;</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = <span class="number">0.3</span>, random_state = <span class="number">0</span>)</span><br><span class="line"><span class="comment">#print(datasets.DESCR)</span></span><br><span class="line"><span class="comment">#malignant - 0, benign - 1</span></span><br><span class="line">y_predict = []</span><br></pre></td></tr></table></figure><p>Since all the 30 attributes are continuous values, we need to divide the range of attribute values into several intervals when using Naive Bayes, and calculate the probability of the instance falling in this interval. I’m dividing each property here by the average.<br><img src="https://bu.dusays.com/2022/08/28/630b5219d7e56.jpg" alt=""></p><h3 id="Divide-consecutive-attributes-into-intervals"><a href="#Divide-consecutive-attributes-into-intervals" class="headerlink" title="Divide consecutive attributes into intervals"></a>Divide consecutive attributes into intervals</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">distribution</span>(<span class="params">X_train, y_train</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    先把区间分好，然后再计算概率。</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment">#===============区间划分====================#</span></span><br><span class="line">    attributes_max_min_mean = []</span><br><span class="line">    <span class="comment"># 记录所有属性的最大值、最小值和平均值</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X_train[<span class="number">0</span>])):</span><br><span class="line">        <span class="comment">#属性循环</span></span><br><span class="line">        <span class="comment">#section = [max, min, mean]</span></span><br><span class="line">        section = [X_train[<span class="number">0</span>][i], X_train[<span class="number">0</span>][i], <span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> instance <span class="keyword">in</span> X_train:</span><br><span class="line">            <span class="comment">#训练样例循环</span></span><br><span class="line">            <span class="keyword">if</span> instance[i] &gt; section[<span class="number">0</span>]:</span><br><span class="line">                section[<span class="number">0</span>] = instance[i]</span><br><span class="line">            <span class="keyword">if</span> instance[i] &lt; section[<span class="number">1</span>]:</span><br><span class="line">                section[<span class="number">1</span>] = instance[i]</span><br><span class="line">            section[<span class="number">2</span>] += instance[i]</span><br><span class="line">        section[<span class="number">2</span>] /= <span class="built_in">len</span>(X_train)</span><br><span class="line">        attributes_max_min_mean.append(section)</span><br><span class="line">        </span><br><span class="line">    <span class="comment">#=========计算每个属性落在每个区间的样例个数=========#</span></span><br><span class="line">    instance_distribution = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X_train[<span class="number">0</span>])):</span><br><span class="line">        <span class="comment">#属性循环</span></span><br><span class="line">        smaller_benign = <span class="number">0</span></span><br><span class="line">        larger_benign = <span class="number">0</span></span><br><span class="line">        smaller_malignant = <span class="number">0</span></span><br><span class="line">        larger_malignant = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X_train)):</span><br><span class="line">            <span class="comment">#训练样例循环</span></span><br><span class="line">            <span class="keyword">if</span> X_train[j][i] &gt; attributes_max_min_mean[i][<span class="number">2</span>]:</span><br><span class="line">                <span class="keyword">if</span> y_train[j] == <span class="number">1</span>:</span><br><span class="line">                    larger_benign += <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    larger_malignant +=<span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> y_train[j] == <span class="number">1</span>:</span><br><span class="line">                smaller_benign += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                smaller_malignant += <span class="number">1</span>   </span><br><span class="line">        instance_distribution.append([smaller_benign, larger_benign, smaller_malignant, larger_malignant])</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> instance_distribution, attributes_max_min_mean</span><br></pre></td></tr></table></figure><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Naive_Bayes</span>(<span class="params">X_test, y_predict, instance_distribution,attributes_max_min_mean</span>):</span><br><span class="line">    <span class="keyword">for</span> test_data <span class="keyword">in</span> X_test:</span><br><span class="line">        <span class="comment">#测试样例循环</span></span><br><span class="line">        <span class="comment">#训练集中良性和恶性肿瘤的数量</span></span><br><span class="line">        malignant = instance_distribution[<span class="number">0</span>][<span class="number">2</span>] + instance_distribution[<span class="number">0</span>][<span class="number">3</span>]</span><br><span class="line">        benign = instance_distribution[<span class="number">0</span>][<span class="number">0</span>] + instance_distribution[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">        <span class="comment">#概率初始化，下面计算每个属性的概率</span></span><br><span class="line">        p_xc0 = <span class="number">1</span></span><br><span class="line">        p_xc1 = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X_train[<span class="number">0</span>])):</span><br><span class="line">            <span class="comment"># 属性循环</span></span><br><span class="line">            <span class="keyword">if</span> test_data[i] &gt; attributes_max_min_mean[i][<span class="number">2</span>]:</span><br><span class="line">                p_xc0 *= instance_distribution[i][<span class="number">3</span>] / malignant</span><br><span class="line">                p_xc1 *= instance_distribution[i][<span class="number">1</span>] / benign</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                p_xc0 *= instance_distribution[i][<span class="number">2</span>] / malignant</span><br><span class="line">                p_xc1 *= instance_distribution[i][<span class="number">0</span>] / benign</span><br><span class="line">        p0 = p_xc0 * malignant / (malignant + benign)</span><br><span class="line">        p1 = p_xc1 * benign / (malignant + benign)</span><br><span class="line">        <span class="keyword">if</span> p0 &gt; p1:</span><br><span class="line">            y_predict.append(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y_predict.append(<span class="number">1</span>)      </span><br></pre></td></tr></table></figure><h3 id="Calculate-accuracy-1"><a href="#Calculate-accuracy-1" class="headerlink" title="Calculate accuracy"></a>Calculate accuracy</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_predict, y_test</span>):</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(y_predict)):</span><br><span class="line">        <span class="keyword">if</span> y_predict[i] == y_test[i]:</span><br><span class="line">            correct += <span class="number">1</span></span><br><span class="line">    accuracy_rate = correct / <span class="built_in">len</span>(y_predict)</span><br><span class="line">    <span class="keyword">return</span> correct, accuracy_rate</span><br></pre></td></tr></table></figure><h3 id="Main-function"><a href="#Main-function" class="headerlink" title="Main function"></a>Main function</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    instance_distribution, attributes_max_min_mean = distribution(X_train, y_train)</span><br><span class="line">    Naive_Bayes(X_test, y_predict, instance_distribution, attributes_max_min_mean)</span><br><span class="line">    correct, accuracy_rate = accuracy(y_predict, y_test)</span><br><span class="line">    <span class="built_in">print</span>(y_predict)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Naive Bayes模型测试集预测的准确率为：%.3f&quot;</span> % accuracy_rate);</span><br><span class="line">    bayes = naive_bayes.GaussianNB()</span><br><span class="line">    bayes.fit(X_train, y_train)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;sklearn库中Naive Bayes模型预测的准确率为：%.3f&quot;</span> % bayes.score(X_test, y_test));</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p> [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1]<br> The predicted accuracy of Naive Bayes model test set is: 0.930<br> The prediction accuracy of Naive Bayes model in sklearn library is: 0.924</p><p>Through the experimental results, it can be found that the naive Bayes we implemented is better than the naive Bayes provided by sklearn.</p><p>We can further optimize the accuracy by trying different interval divisions for each attribute. The reason why the naive Bayes provided by sklearn may not work well is that the interval partition to convert continuous values to discrete values is not done well.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Implement kNN and Naive Bayes using Breast_cancer dataset.&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://enblog.crocodilezs.top/categories/Machine-Learning/"/>
    
    
    <category term="KNN" scheme="https://enblog.crocodilezs.top/tags/KNN/"/>
    
    <category term="Naive Bayes" scheme="https://enblog.crocodilezs.top/tags/Naive-Bayes/"/>
    
  </entry>
  
  <entry>
    <title>Fisher, SVM, K-Means and their optimization</title>
    <link href="https://enblog.crocodilezs.top/201911/Fisher%E7%AE%97%E6%B3%95&amp;SVM&amp;K-Means%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/"/>
    <id>https://enblog.crocodilezs.top/201911/Fisher%E7%AE%97%E6%B3%95&amp;SVM&amp;K-Means%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/</id>
    <published>2019-10-22T04:21:10.000Z</published>
    <updated>2022-08-28T11:22:01.312Z</updated>
    
    <content type="html"><![CDATA[<p>Implement Fisher, SVM and K-Means algorithms and optimize them.</p><span id="more"></span><h2 id="Fisher-algorithm-and-its-optimization"><a href="#Fisher-algorithm-and-its-optimization" class="headerlink" title="Fisher algorithm and its optimization"></a><code>Fisher</code> algorithm and its optimization</h2><p>Implement <code>Fisher</code> Algorithm and validate by random generating data.</p><h3 id="Generating-data"><a href="#Generating-data" class="headerlink" title="Generating data"></a>Generating data</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_multilabel_classification</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x, y = make_multilabel_classification(n_samples=<span class="number">200</span>, n_features=<span class="number">2</span>,</span><br><span class="line">                                      n_labels=<span class="number">1</span>, n_classes=<span class="number">1</span>,</span><br><span class="line">                                      random_state=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据类别分类</span></span><br><span class="line">index1 = np.array([index <span class="keyword">for</span> (index, value) <span class="keyword">in</span> <span class="built_in">enumerate</span>(y) <span class="keyword">if</span> value == <span class="number">0</span>])  <span class="comment"># 获取类别1的indexs</span></span><br><span class="line">index2 = np.array([index <span class="keyword">for</span> (index, value) <span class="keyword">in</span> <span class="built_in">enumerate</span>(y) <span class="keyword">if</span> value == <span class="number">1</span>])  <span class="comment"># 获取类别2的indexs</span></span><br><span class="line"></span><br><span class="line">c_1 = x[index1]   <span class="comment"># 类别1的所有数据(x1, x2) in X_1</span></span><br><span class="line">c_2 = x[index2]  <span class="comment"># 类别2的所有数据(x1, x2) in X_2</span></span><br></pre></td></tr></table></figure><p><a href="http://lijiancheng0614.github.io/scikit-learn/modules/generated/sklearn.datasets.make_multilabel_classification.html#sklearn.datasets.make_multilabel_classification">make_multilabel_classification</a><br><code>n_samples</code>: the number of samples<br><code>n_features</code>: the number of features.<br><code>n_labels</code>: the number of labels.<br><code>n_classes</code>: the number of classes.<br><code>random_state</code>: set the random seed to ensure generating the same data everytime.</p><p><a href="https://www.runoob.com/python/python-func-enumerate.html">enumerate()</a><br><code>enumerate()</code> is used to combine a traversable data object (such as a list, tuple, or string) into an index sequence, listing both the data and the data subscripts, typically in a <code>for</code> loop.</p><h3 id="The-implementation-of-Fisher"><a href="#The-implementation-of-Fisher" class="headerlink" title="The implementation of Fisher"></a>The implementation of <code>Fisher</code></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cal_cov_and_avg</span>(<span class="params">samples</span>):</span><br><span class="line">    u1 = np.mean(samples, axis=<span class="number">0</span>)</span><br><span class="line">    cov_m = np.zeros((samples.shape[<span class="number">1</span>], samples.shape[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> samples:</span><br><span class="line">        t = s - u1</span><br><span class="line">        cov_m += t * t.reshape(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> cov_m, u1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fisher</span>(<span class="params">c_1, c_2</span>):</span><br><span class="line">    cov_1, u1 = cal_cov_and_avg(c_1)</span><br><span class="line">    cov_2, u2 = cal_cov_and_avg(c_2)</span><br><span class="line">    s_w = cov_1 + cov_2</span><br><span class="line">    u, s, v = np.linalg.svd(s_w)  <span class="comment"># 奇异值分解</span></span><br><span class="line">    s_w_inv = np.dot(np.dot(v.T, np.linalg.inv(np.diag(s))), u.T)</span><br><span class="line">    <span class="keyword">return</span> np.dot(s_w_inv, u1 - u2)</span><br></pre></td></tr></table></figure><p><code>np.mean</code>: calculate the average on the specified axis<br><code>np.zeros</code>: generate an array of the given shape and filled with zeros.</p><h3 id="Determine-the-category"><a href="#Determine-the-category" class="headerlink" title="Determine the category"></a>Determine the category</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">judge</span>(<span class="params">sample, w, c_1, c_2</span>):</span><br><span class="line">    u1 = np.mean(c_1, axis=<span class="number">0</span>)</span><br><span class="line">    u2 = np.mean(c_2, axis=<span class="number">0</span>)</span><br><span class="line">    center_1 = np.dot(w.T, u1)</span><br><span class="line">    center_2 = np.dot(w.T, u2)</span><br><span class="line">    pos = np.dot(w.T, sample)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">abs</span>(pos - center_1) &lt; <span class="built_in">abs</span>(pos - center_2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">w = fisher(c_1, c_2)  <span class="comment"># 调用函数，得到参数w</span></span><br><span class="line">out = judge(c_1[<span class="number">1</span>], w, c_1, c_2)   <span class="comment"># 判断所属的类别</span></span><br><span class="line"><span class="comment"># print(out)</span></span><br></pre></td></tr></table></figure><h3 id="Plot"><a href="#Plot" class="headerlink" title="Plot"></a>Plot</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.scatter(c_1[:, <span class="number">0</span>], c_1[:, <span class="number">1</span>], c=<span class="string">&#x27;#99CC99&#x27;</span>)</span><br><span class="line">plt.scatter(c_2[:, <span class="number">0</span>], c_2[:, <span class="number">1</span>], c=<span class="string">&#x27;#FFCC00&#x27;</span>)</span><br><span class="line">line_x = np.arange(<span class="built_in">min</span>(np.<span class="built_in">min</span>(c_1[:, <span class="number">0</span>]), np.<span class="built_in">min</span>(c_2[:, <span class="number">0</span>])),</span><br><span class="line">                   <span class="built_in">max</span>(np.<span class="built_in">max</span>(c_1[:, <span class="number">0</span>]), np.<span class="built_in">max</span>(c_2[:, <span class="number">0</span>])),</span><br><span class="line">                   step=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">line_y = - (w[<span class="number">0</span>] * line_x) / w[<span class="number">1</span>]</span><br><span class="line">plt.plot(line_x, line_y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>&lt;Figure size 640x480 with 1 Axes&gt;</code></pre><h2 id="The-detailed-derivation-of-SVM-optimization-dual-problem"><a href="#The-detailed-derivation-of-SVM-optimization-dual-problem" class="headerlink" title="The detailed derivation of SVM optimization dual problem"></a>The detailed derivation of <code>SVM</code> optimization dual problem</h2><p><a href="https://zhuanlan.zhihu.com/p/49331510">reference: https://zhuanlan.zhihu.com/p/49331510</a>  </p><p><img src="https://s2.ax1x.com/2019/10/08/uWyoX4.png" alt=""><br><img src="https://s2.ax1x.com/2019/10/08/uWy4pT.png" alt=""><br><img src="https://s2.ax1x.com/2019/10/08/uWyIcF.png" alt=""><br><img src="https://s2.ax1x.com/2019/10/08/uWy51U.png" alt=""><br><img src="https://s2.ax1x.com/2019/10/08/uWyfhV.png" alt=""></p><h2 id="The-implementation-of-SVM"><a href="#The-implementation-of-SVM" class="headerlink" title="The implementation of SVM"></a>The implementation of <code>SVM</code></h2><p><a href="https://www.jb51.net/article/131580.htm">reference: https://www.jb51.net/article/131580.htm</a></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line">  </span><br><span class="line">np.random.seed(<span class="number">0</span>) </span><br><span class="line">x = np.r_[np.random.randn(<span class="number">100</span>,<span class="number">2</span>)-[<span class="number">2</span>,<span class="number">2</span>],np.random.randn(<span class="number">100</span>,<span class="number">2</span>)+[<span class="number">2</span>,<span class="number">2</span>]] <span class="comment">#正态分布来产生数字,20行2列*2 </span></span><br><span class="line">y = [<span class="number">0</span>]*<span class="number">100</span>+[<span class="number">1</span>]*<span class="number">100</span> <span class="comment">#100个class0，100个class1 </span></span><br><span class="line">  </span><br><span class="line">clf = svm.SVC(kernel=<span class="string">&#x27;linear&#x27;</span>) </span><br><span class="line">clf.fit(x,y) </span><br><span class="line">  </span><br><span class="line">w = clf.coef_[<span class="number">0</span>] <span class="comment">#获取w </span></span><br><span class="line">a = -w[<span class="number">0</span>]/w[<span class="number">1</span>] <span class="comment">#斜率 </span></span><br><span class="line"><span class="comment">#画图划线 </span></span><br><span class="line">xx = np.linspace(-<span class="number">5</span>,<span class="number">5</span>) <span class="comment">#(-5,5)之间x的值 </span></span><br><span class="line">yy = a*xx-(clf.intercept_[<span class="number">0</span>])/w[<span class="number">1</span>] <span class="comment">#xx带入y，截距 </span></span><br><span class="line">  </span><br><span class="line"><span class="comment">#画出与点相切的线 </span></span><br><span class="line">b = clf.support_vectors_[<span class="number">0</span>] </span><br><span class="line">yy_down = a*xx+(b[<span class="number">1</span>]-a*b[<span class="number">0</span>]) </span><br><span class="line">b = clf.support_vectors_[-<span class="number">1</span>] </span><br><span class="line">yy_up = a*xx+(b[<span class="number">1</span>]-a*b[<span class="number">0</span>]) </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W:&quot;</span>,w) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;a:&quot;</span>,a) </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;support_vectors_:&quot;</span>,clf.support_vectors_) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;clf.coef_:&quot;</span>,clf.coef_) </span><br><span class="line">  </span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">4</span>)) </span><br><span class="line">plt.plot(xx,yy) </span><br><span class="line">plt.plot(xx,yy_down) </span><br><span class="line">plt.plot(xx,yy_up) </span><br><span class="line">plt.scatter(clf.support_vectors_[:,<span class="number">0</span>],clf.support_vectors_[:,<span class="number">1</span>],s=<span class="number">80</span>) </span><br><span class="line">plt.scatter(x[:,<span class="number">0</span>],x[:,<span class="number">1</span>],c=y,cmap=plt.cm.Paired) <span class="comment">#[:，0]列切片，第0列 </span></span><br><span class="line">  </span><br><span class="line">plt.axis(<span class="string">&#x27;tight&#x27;</span>) </span><br><span class="line">  </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>W: [0.95070185 1.15607502]a: -0.8223530762163854support_vectors_: [[-0.51174781 -0.10411082] [ 0.16323595 -0.66347205] [ 2.39904635 -0.77259276] [ 0.66574153  0.65328249] [-0.25556423  0.97749316]]clf.coef_: [[0.95070185 1.15607502]]</code></pre><p><img src="https://bu.dusays.com/2022/08/28/630b4f4c4cc52.png" alt="png"></p><h2 id="The-implementation-of-k-means"><a href="#The-implementation-of-k-means" class="headerlink" title="The implementation of k-means"></a>The implementation of <code>k-means</code></h2><p><a href="https://cloud.tencent.com/developer/article/1465020">Reference1: https://cloud.tencent.com/developer/article/1465020</a><br><a href="https://blog.csdn.net/weixin_42029738/article/details/81978038">Reference2: https://blog.csdn.net/weixin_42029738/article/details/81978038</a>  </p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    K-Means clustering algorithms</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(__doc__)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> MiniBatchKMeans, KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> pairwise_distances_argmin</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Generate sample data</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">45</span></span><br><span class="line">centers = [[<span class="number">1</span>, <span class="number">1</span>], [-<span class="number">1</span>, -<span class="number">1</span>], [<span class="number">1</span>, -<span class="number">1</span>]] <span class="comment"># 初始化3个中心</span></span><br><span class="line">n_clusters = <span class="built_in">len</span>(centers) <span class="comment"># 聚类的数目为3</span></span><br><span class="line"><span class="comment"># 产生10000组二维数据，以上面三个点为中心，以(-10,10)为边界，数据集的标准差是0.7</span></span><br><span class="line">X, labels_true = make_blobs(n_samples=<span class="number">10000</span>, centers=centers, cluster_std=<span class="number">0.7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Compute clustering with Means</span></span><br><span class="line"></span><br><span class="line">k_means = KMeans(init=<span class="string">&#x27;k-means++&#x27;</span>, n_clusters=<span class="number">3</span>, n_init=<span class="number">10</span>)</span><br><span class="line">t0 = time.time()</span><br><span class="line">k_means.fit(X)</span><br><span class="line"><span class="comment"># 使用k-means对300组数据集训练算法的时间消耗</span></span><br><span class="line">t_batch = time.time() - t0</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Plot result</span></span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>, <span class="number">3</span>))</span><br><span class="line">fig.subplots_adjust(left=<span class="number">0.02</span>, right=<span class="number">0.98</span>, bottom=<span class="number">0.05</span>, top=<span class="number">0.9</span>)</span><br><span class="line">colors = [<span class="string">&#x27;#4EACC5&#x27;</span>, <span class="string">&#x27;#FF9C34&#x27;</span>, <span class="string">&#x27;#4E9A06&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># We want to have the same colors for the same cluster from the</span></span><br><span class="line"><span class="comment"># MiniBatchKMeans and the KMeans algorithm. Let&#x27;s pair the cluster centers per</span></span><br><span class="line"><span class="comment"># closest one.</span></span><br><span class="line">k_means_cluster_centers = np.sort(k_means.cluster_centers_, axis=<span class="number">0</span>)</span><br><span class="line">k_means_labels = pairwise_distances_argmin(X, k_means_cluster_centers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># KMeans</span></span><br><span class="line">ax = fig.add_subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> k, col <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(n_clusters), colors):</span><br><span class="line">    my_members = k_means_labels == k</span><br><span class="line">    cluster_center = k_means_cluster_centers[k]</span><br><span class="line">    ax.plot(X[my_members, <span class="number">0</span>], X[my_members, <span class="number">1</span>], <span class="string">&#x27;w&#x27;</span>,</span><br><span class="line">            markerfacecolor=col, marker=<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">    ax.plot(cluster_center[<span class="number">0</span>], cluster_center[<span class="number">1</span>], <span class="string">&#x27;o&#x27;</span>, markerfacecolor=col,</span><br><span class="line">            markeredgecolor=<span class="string">&#x27;k&#x27;</span>, markersize=<span class="number">6</span>)</span><br><span class="line">ax.set_title(<span class="string">&#x27;KMeans&#x27;</span>)</span><br><span class="line">ax.set_xticks(())</span><br><span class="line">ax.set_yticks(())</span><br><span class="line">plt.text(-<span class="number">3.5</span>, <span class="number">1.8</span>,  <span class="string">&#x27;train time: %.2fs\ninertia: %f&#x27;</span> % (</span><br><span class="line">    t_batch, k_means.inertia_))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>    K-Means clustering algorithms</code></pre><p><img src="https://bu.dusays.com/2022/08/28/630b4f4c5802b.png" alt="png"></p><h2 id="The-optimization-of-k-means"><a href="#The-optimization-of-k-means" class="headerlink" title="The optimization of k-means"></a>The optimization of <code>k-means</code></h2><pre><code>1. `k-means++`(Change the way you choose the center point)2. `elkan K-Means`(Reduce unnecessary distance calculations)3. `ISODATA` Algorithm (Adjust the number of clustering centers K according to the actual situation during operation)4. `Mini Batch k-means` Algorithm (Adopting partial samples and discarding some accuracy greatly accelerates the convergence speed)</code></pre><h3 id="k-means"><a href="#k-means" class="headerlink" title="k-means++"></a><code>k-means++</code></h3><p><a href="https://blog.csdn.net/github_39261590/article/details/76910689">Reference1: https://blog.csdn.net/github_39261590/article/details/76910689</a><br><a href="https://www.cnblogs.com/yszd/p/9672885.html">Reference2: https://www.cnblogs.com/yszd/p/9672885.html</a><br>The basic idea of the algorithm for selecting the initial seeds is: <strong> The initial cluster centers should be as far away from each other as possible. </strong> </p><p>Algorithm steps:</p><ol><li><p>Select a point randomly from the set of input data points as the first clustering center</p></li><li><p>For each point x in the data set, calculate the distance D(x) between it and the nearest cluster center (referring to the selected cluster center).</p></li><li><p>Select a new data point as the new cluster center. The selection principle is as follows: points with larger D(x) have a greater probability of being selected as the cluster center</p></li><li><p>Repeat 2 and 3 until K cluster centers are selected</p></li><li><p>Use these K initial cluster centers to run the standard k-means algorithm</p></li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets <span class="keyword">as</span> ds</span><br><span class="line"><span class="keyword">import</span> matplotlib.colors</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> MiniBatchKMeans</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">expand</span>(<span class="params">a, b</span>):</span><br><span class="line">    d = (b - a) * <span class="number">0.1</span></span><br><span class="line">    <span class="keyword">return</span> a-b, b+d</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    N = <span class="number">400</span></span><br><span class="line">    centers = <span class="number">4</span></span><br><span class="line">    data, y = ds.make_blobs(N, n_features=<span class="number">2</span>, centers=centers, random_state=<span class="number">2</span>)</span><br><span class="line">    data2, y2 = ds.make_blobs(N, n_features=<span class="number">2</span>, centers=centers, cluster_std=(<span class="number">1</span>, <span class="number">2.5</span>, <span class="number">0.5</span>, <span class="number">2</span>), random_state=<span class="number">2</span>)</span><br><span class="line">    <span class="comment"># 按行拼接numpy数组</span></span><br><span class="line">    data3 = np.vstack((data[y == <span class="number">0</span>][:], data[y == <span class="number">1</span>][:<span class="number">50</span>], data[y == <span class="number">2</span>][:<span class="number">20</span>], data[y == <span class="number">3</span>][:<span class="number">5</span>]))</span><br><span class="line">    y3 = np.array([<span class="number">0</span>] * <span class="number">100</span> + [<span class="number">1</span>] * <span class="number">50</span> + [<span class="number">2</span>] * <span class="number">20</span> + [<span class="number">3</span>] * <span class="number">5</span>)</span><br><span class="line">    cls = KMeans(n_clusters=<span class="number">4</span>, init=<span class="string">&#x27;k-means++&#x27;</span>)</span><br><span class="line">    y_hat = cls.fit_predict(data)</span><br><span class="line">    y2_hat = cls.fit_predict(data2)</span><br><span class="line">    y3_hat = cls.fit_predict(data3)</span><br><span class="line">    </span><br><span class="line">    m = np.array(((<span class="number">1</span>, <span class="number">1</span>),(<span class="number">1</span>, <span class="number">3</span>)))</span><br><span class="line">    data_r = data.dot(m)</span><br><span class="line">    y_r_hat = cls.fit_predict(data_r)</span><br><span class="line">    </span><br><span class="line">    matplotlib.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">u&#x27;SimHei&#x27;</span>]</span><br><span class="line">    matplotlib.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span></span><br><span class="line">    cm = matplotlib.colors.ListedColormap(<span class="built_in">list</span>(<span class="string">&#x27;rgbm&#x27;</span>))</span><br><span class="line">    plt.figure(figsize=(<span class="number">9</span>, <span class="number">10</span>), facecolor=<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">    plt.subplot(<span class="number">421</span>)</span><br><span class="line">    plt.title(<span class="string">u&#x27;原始数据&#x27;</span>)</span><br><span class="line">    plt.scatter(data[:, <span class="number">0</span>], data[:, <span class="number">1</span>], c=y, s=<span class="number">30</span>, cmap=cm, edgecolors=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    x1_min, x2_min = np.<span class="built_in">min</span>(data, axis=<span class="number">0</span>)</span><br><span class="line">    x1_max, x2_max = np.<span class="built_in">max</span>(data, axis=<span class="number">0</span>)</span><br><span class="line">    x1_min, x1_max = expand(x1_min, x1_max)</span><br><span class="line">    x2_min, x2_max = expand(x2_min, x2_max)</span><br><span class="line">    plt.xlim((x1_min, x1_max))</span><br><span class="line">    plt.ylim((x2_min, x2_max))</span><br><span class="line">    plt.grid(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">422</span>)</span><br><span class="line">    plt.title(<span class="string">u&#x27;KMeans++聚类&#x27;</span>)</span><br><span class="line">    plt.scatter(data[:, <span class="number">0</span>], data[:, <span class="number">1</span>], c=y_hat, s=<span class="number">30</span>, cmap=cm, edgecolors=<span class="string">&#x27;none&#x27;</span>)    </span><br><span class="line">    plt.xlim((x1_min, x1_max))</span><br><span class="line">    plt.ylim((x2_min, x2_max))</span><br><span class="line">    plt.grid(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><img src="https://bu.dusays.com/2022/08/28/630b4f4d4495f.png" alt="png"></p><h3 id="elkan-k-means"><a href="#elkan-k-means" class="headerlink" title="elkan k-means"></a><code>elkan k-means</code></h3><p><a href="https://blog.csdn.net/u014465639/article/details/71342072">Reference: https://blog.csdn.net/u014465639/article/details/71342072</a></p><p><code>Elkan k-means</code> uses the triangle property that the sum of the two sides is greater than or equal to the third side, and the difference between the two sides is less than the third side to reduce the distance calculation.</p><p>The first rule is for a sample point and two centers of mass. If we pre-calculate the distance between these two centers of mass, then if we find it, we know it immediately. Now we don’t have to calculate it anymore, so we’re saving a step.</p><p>The second rule is for a sample point and two centers of mass. We can get. This is also easy to get from the triangle property.</p><p>Using the above two rules, ELkan k-means has a great improvement in iteration speed compared with traditional k-means. However, if the features of our samples are sparse and there are missing values, this method will not be used. In this case, some distances cannot be calculated, so this algorithm cannot be used. </p><h3 id="ISODATA"><a href="#ISODATA" class="headerlink" title="ISODATA"></a><code>ISODATA</code></h3><p><a href="https://blog.csdn.net/houston11235/article/details/8511379">Reference1: https://blog.csdn.net/houston11235/article/details/8511379</a><br><a href="https://www.cnblogs.com/huadongw/p/4101422.html">Reference2: https://www.cnblogs.com/huadongw/p/4101422.html</a><br>One disadvantage of k-means is that you have to specify the number of clusters, which sometimes doesn’t work very well. Therefore, it is required that the number of this category can also be changed, which forms the ISOData method. By setting some conditions for category splitting and merging, the number of categories can be automatically increased or decreased in the process of clustering. The problem with this, of course, is that this condition is not always easy to give. Of course, ISODATA can still get more reliable results in many cases. </p><h3 id="Mini-Batch-k-means"><a href="#Mini-Batch-k-means" class="headerlink" title="Mini Batch k-means"></a><code>Mini Batch k-means</code></h3><p><a href="https://cloud.tencent.com/developer/article/1465020">Reference1: https://cloud.tencent.com/developer/article/1465020</a><br>Mini Batch KMeans algorithm is a clustering model ** that can keep the clustering accuracy as far as possible but can greatly reduce the computation time. It uses small batches of data subsets to reduce the computation time while still trying to optimize the objective function. The so-called Mini Batch KMeans algorithm refers to the data subset randomly selected during each training algorithm. Using these randomly selected data for training greatly reduces the computing time and reduces the convergence time of KMeans algorithm, but it is slightly worse than the standard algorithm. It is suggested that when the sample size is more than ten thousand for clustering, it is necessary to consider the selection of Mini Batch KMeans algorithm. </p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Comparison of the K-Means and MiniBatchKMeans clustering algorithms</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(__doc__)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> MiniBatchKMeans, KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> pairwise_distances_argmin</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Generate sample data</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">45</span></span><br><span class="line">centers = [[<span class="number">1</span>, <span class="number">1</span>], [-<span class="number">1</span>, -<span class="number">1</span>], [<span class="number">1</span>, -<span class="number">1</span>]] <span class="comment"># 初始化3个中心</span></span><br><span class="line">n_clusters = <span class="built_in">len</span>(centers) <span class="comment"># 聚类的数目为3</span></span><br><span class="line"><span class="comment"># 产生10000组二维数据，以上面三个点为中心，以(-10,10)为边界，数据集的标准差是0.7</span></span><br><span class="line">X, labels_true = make_blobs(n_samples=<span class="number">10000</span>, centers=centers, cluster_std=<span class="number">0.7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Compute clustering with Means</span></span><br><span class="line"></span><br><span class="line">k_means = KMeans(init=<span class="string">&#x27;k-means++&#x27;</span>, n_clusters=<span class="number">3</span>, n_init=<span class="number">10</span>)</span><br><span class="line">t0 = time.time()</span><br><span class="line">k_means.fit(X)</span><br><span class="line"><span class="comment"># 使用k-means对300组数据集训练算法的时间消耗</span></span><br><span class="line">t_batch = time.time() - t0</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Compute clustering with MiniBatchKMeans</span></span><br><span class="line"></span><br><span class="line">mbk = MiniBatchKMeans(init=<span class="string">&#x27;k-means++&#x27;</span>, n_clusters=<span class="number">3</span>, batch_size=batch_size,</span><br><span class="line">                      n_init=<span class="number">10</span>, max_no_improvement=<span class="number">10</span>, verbose=<span class="number">0</span>)</span><br><span class="line">t0 = time.time()</span><br><span class="line">mbk.fit(X)</span><br><span class="line"><span class="comment"># 使用MiniBatchKMeans对300组数据集训练算法的时间消耗</span></span><br><span class="line">t_mini_batch = time.time() - t0</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Plot result</span></span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>, <span class="number">3</span>))</span><br><span class="line">fig.subplots_adjust(left=<span class="number">0.02</span>, right=<span class="number">0.98</span>, bottom=<span class="number">0.05</span>, top=<span class="number">0.9</span>)</span><br><span class="line">colors = [<span class="string">&#x27;#4EACC5&#x27;</span>, <span class="string">&#x27;#FF9C34&#x27;</span>, <span class="string">&#x27;#4E9A06&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># We want to have the same colors for the same cluster from the</span></span><br><span class="line"><span class="comment"># MiniBatchKMeans and the KMeans algorithm. Let&#x27;s pair the cluster centers per</span></span><br><span class="line"><span class="comment"># closest one.</span></span><br><span class="line">k_means_cluster_centers = np.sort(k_means.cluster_centers_, axis=<span class="number">0</span>)</span><br><span class="line">mbk_means_cluster_centers = np.sort(mbk.cluster_centers_, axis=<span class="number">0</span>)</span><br><span class="line">k_means_labels = pairwise_distances_argmin(X, k_means_cluster_centers)</span><br><span class="line">mbk_means_labels = pairwise_distances_argmin(X, mbk_means_cluster_centers)</span><br><span class="line">order = pairwise_distances_argmin(k_means_cluster_centers,</span><br><span class="line">                                  mbk_means_cluster_centers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># KMeans</span></span><br><span class="line">ax = fig.add_subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> k, col <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(n_clusters), colors):</span><br><span class="line">    my_members = k_means_labels == k</span><br><span class="line">    cluster_center = k_means_cluster_centers[k]</span><br><span class="line">    ax.plot(X[my_members, <span class="number">0</span>], X[my_members, <span class="number">1</span>], <span class="string">&#x27;w&#x27;</span>,</span><br><span class="line">            markerfacecolor=col, marker=<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">    ax.plot(cluster_center[<span class="number">0</span>], cluster_center[<span class="number">1</span>], <span class="string">&#x27;o&#x27;</span>, markerfacecolor=col,</span><br><span class="line">            markeredgecolor=<span class="string">&#x27;k&#x27;</span>, markersize=<span class="number">6</span>)</span><br><span class="line">ax.set_title(<span class="string">&#x27;KMeans&#x27;</span>)</span><br><span class="line">ax.set_xticks(())</span><br><span class="line">ax.set_yticks(())</span><br><span class="line">plt.text(-<span class="number">3.5</span>, <span class="number">1.8</span>,  <span class="string">&#x27;train time: %.2fs\ninertia: %f&#x27;</span> % (</span><br><span class="line">    t_batch, k_means.inertia_))</span><br><span class="line"></span><br><span class="line"><span class="comment"># MiniBatchKMeans</span></span><br><span class="line">ax = fig.add_subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> k, col <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(n_clusters), colors):</span><br><span class="line">    my_members = mbk_means_labels == order[k]</span><br><span class="line">    cluster_center = mbk_means_cluster_centers[order[k]]</span><br><span class="line">    ax.plot(X[my_members, <span class="number">0</span>], X[my_members, <span class="number">1</span>], <span class="string">&#x27;w&#x27;</span>,</span><br><span class="line">            markerfacecolor=col, marker=<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">    ax.plot(cluster_center[<span class="number">0</span>], cluster_center[<span class="number">1</span>], <span class="string">&#x27;o&#x27;</span>, markerfacecolor=col,</span><br><span class="line">            markeredgecolor=<span class="string">&#x27;k&#x27;</span>, markersize=<span class="number">6</span>)</span><br><span class="line">ax.set_title(<span class="string">&#x27;MiniBatchKMeans&#x27;</span>)</span><br><span class="line">ax.set_xticks(())</span><br><span class="line">ax.set_yticks(())</span><br><span class="line">plt.text(-<span class="number">3.5</span>, <span class="number">1.8</span>, <span class="string">&#x27;train time: %.2fs\ninertia: %f&#x27;</span> %</span><br><span class="line">         (t_mini_batch, mbk.inertia_))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialise the different array to all False</span></span><br><span class="line">different = (mbk_means_labels == <span class="number">4</span>)</span><br><span class="line">ax = fig.add_subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(n_clusters):</span><br><span class="line">    different += ((k_means_labels == k) != (mbk_means_labels == order[k]))</span><br><span class="line"></span><br><span class="line">identic = np.logical_not(different)</span><br><span class="line">ax.plot(X[identic, <span class="number">0</span>], X[identic, <span class="number">1</span>], <span class="string">&#x27;w&#x27;</span>,</span><br><span class="line">        markerfacecolor=<span class="string">&#x27;#bbbbbb&#x27;</span>, marker=<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">ax.plot(X[different, <span class="number">0</span>], X[different, <span class="number">1</span>], <span class="string">&#x27;w&#x27;</span>,</span><br><span class="line">        markerfacecolor=<span class="string">&#x27;m&#x27;</span>, marker=<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">ax.set_title(<span class="string">&#x27;Difference&#x27;</span>)</span><br><span class="line">ax.set_xticks(())</span><br><span class="line">ax.set_yticks(())</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>Comparison of the K-Means and MiniBatchKMeans clustering algorithms</p><p><img src="https://bu.dusays.com/2022/08/28/630b4f4e40995.png" alt="png"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Implement Fisher, SVM and K-Means algorithms and optimize them.&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://enblog.crocodilezs.top/categories/Machine-Learning/"/>
    
    
    <category term="SVM" scheme="https://enblog.crocodilezs.top/tags/SVM/"/>
    
    <category term="Fisher" scheme="https://enblog.crocodilezs.top/tags/Fisher/"/>
    
    <category term="K-Means" scheme="https://enblog.crocodilezs.top/tags/K-Means/"/>
    
  </entry>
  
</feed>
