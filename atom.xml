<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yuyang&#39;s Blog</title>
  
  
  <link href="https://enblog.crocodilezs.top/atom.xml" rel="self"/>
  
  <link href="https://enblog.crocodilezs.top/"/>
  <updated>2022-05-23T17:36:55.902Z</updated>
  <id>https://enblog.crocodilezs.top/</id>
  
  <author>
    <name>CrocodileZS</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>åšå®¢å†…å®¹å½’æ¡£å’Œè‹±æ–‡åšå®¢</title>
    <link href="https://enblog.crocodilezs.top/202111/%E5%8D%9A%E5%AE%A2%E5%86%85%E5%AE%B9%E5%BD%92%E6%A1%A3%E5%92%8C%E8%8B%B1%E6%96%87%E5%8D%9A%E5%AE%A2/"/>
    <id>https://enblog.crocodilezs.top/202111/%E5%8D%9A%E5%AE%A2%E5%86%85%E5%AE%B9%E5%BD%92%E6%A1%A3%E5%92%8C%E8%8B%B1%E6%96%87%E5%8D%9A%E5%AE%A2/</id>
    <published>2021-11-20T10:08:21.000Z</published>
    <updated>2022-05-23T17:36:55.902Z</updated>
    
    <content type="html"><![CDATA[<div class="tag-plugin note" ><div class="body"><p>ä»2019å¹´å¼€å§‹å°è¯•æ­å»ºè‡ªå·±çš„åšå®¢ï¼Œåˆ°ç°åœ¨å·²ç»æœ¬ç§‘æ¯•ä¸šã€‚è¿™ä¸‰å¹´çš„æ—¶é—´è™½ç„¶æ²¡æœ‰åšæŒæ›´æ–°åšå®¢ï¼Œä½†æ˜¯ä¸€ç›´åœ¨åšæŒè®°å½•ã€‚æ— è®ºæ˜¯ç†è®ºçŸ¥è¯†çš„å­¦ä¹ ã€Debugçš„è®°å½•ï¼Œè¿˜æ˜¯è¯»ä¹¦å¥èº«ç¬”è®°å’Œæ„Ÿæ…¨ä¸‡åƒæ—¶çš„éšç¬”ï¼Œä»–ä»¬éƒ½æ•£è½åœ¨å„ç§åœ¨çº¿ç¬”è®°è½¯ä»¶å’Œæ‰‹æœºçš„å¤‡å¿˜å½•ä¸­ã€‚</p><p>éšç€æ—¶é—´çš„æ¨ç§»ï¼Œæˆ‘å¯¹åšå®¢çš„è®¤è¯†ä¹Ÿå‘ç”Ÿäº†å¾ˆå¤šå˜åŒ–ã€‚â€œè®°å½•â€å¯¹æˆ‘æ¥è¯´ä»ç„¶å…·æœ‰éå¸¸å¤§çš„æ„ä¹‰ï¼Œä½†æ˜¯â€œåšå®¢â€æœ€é‡è¦çš„ä»·å€¼ä¸åº”è¯¥æ˜¯â€œè®°å½•â€ï¼Œè€Œæ˜¯â€œåˆ†äº«â€â€”â€”ä¼ é€’æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚æˆ‘æŠŠå¹³æ—¶çš„æ—¥å¸¸ç”Ÿæ´»å’Œå­¦ä¹ ç»å†è®°å½•åœ¨å„ç§ç¬”è®°ä¸­ï¼Œè¿‡ä¸€æ®µæ—¶é—´å†å»å›é¡¾å’Œæ•´ç†ï¼Œé€‰å–å…¶ä¸­æœ‰ä»·å€¼çš„ä¸œè¥¿å»åˆ†äº«ã€‚</p><p>è¿‡å»çš„ä¸€å¹´å†æ¬¡å‚åŠ äº†ç¾èµ›ï¼ŒåŠ å…¥äº†å‡ ä¸ªå’ŒåŒºå—é“¾æœ‰å…³çš„é¡¹ç›®ï¼Œä¹Ÿå®Œæˆäº†ä¸å°‘ä¸“åˆ©å’Œè®ºæ–‡ã€‚æ›´å¤šçš„æ—¶é—´å¿™äºè¿ç”¨çŸ¥è¯†ï¼Œä»¥éœ€æ±‚ä¸ºå¯¼å‘è¿«ä½¿è‡ªå·±è¿›è¡Œå¹¿æ³›è€Œä¸æ·±åˆ»çš„å­¦ä¹ ã€‚å¯¹æˆ‘æ¥è¯´çœŸæ­£æœ‰ä»·å€¼çš„è¾“å…¥å˜å°‘äº†ï¼Œè¾“å‡ºå’Œè¡¨è¾¾çš„æ¬²æœ›ä¹Ÿé™ä½äº†å¾ˆå¤šã€‚</p><p>   æˆ‘åœ¨åŸå…ˆçš„åšå®¢åŸŸåä¹‹ä¸‹åˆå»ºç«‹äº†ä¸€ä¸ªè‹±æ–‡åšå®¢<a href="https://blog.crocodilezs.top/eysblog_en/">BLOG-EN</a>ï¼Œæ—¨åœ¨æ•´ç†è‡ªå·±è¿‡å»ä¸€å¹´ç»å†çš„å„ç§é¡¹ç›®ã€‚åŒæ—¶ä¹Ÿå°†åŸåšå®¢ä¸­çš„å†…å®¹è¿›è¡Œæ•´ç†å½’æ¡£ï¼Œä¾¿äºè‡ªå·±å’Œè®¿å®¢çš„æŸ¥é˜…ã€‚</p></div></div><span id="more"></span><h2 id="è¯¾ç¨‹è®¾è®¡ä¸å®éªŒ"><a href="#è¯¾ç¨‹è®¾è®¡ä¸å®éªŒ" class="headerlink" title="è¯¾ç¨‹è®¾è®¡ä¸å®éªŒ"></a>è¯¾ç¨‹è®¾è®¡ä¸å®éªŒ</h2><p><a href="/201911/Linuxå¼€å‘ç¯å¢ƒåŠåº”ç”¨ä½œä¸š%2020191031/" itemprop="url"> Linuxæ–‡æœ¬å¤„ç†ä½œä¸š </a> &emsp;&emsp;<br><a href="/categories/æ“ä½œç³»ç»Ÿ/" itemprop="url"> ã€Š30å¤©è‡ªåˆ¶æ“ä½œç³»ç»Ÿã€‹å®éªŒåˆè¾‘ </a> &emsp;&emsp;<br><a href="/201909/å‘¨å®‡æ´‹_ã€Œå­¦ç”Ÿå®¿èˆç®¡ç†ç³»ç»Ÿã€å®éªŒæŠ¥å‘Š/" itemprop="url"> å­¦ç”Ÿå®¿èˆç®¡ç†ç³»ç»ŸPythonå¼€å‘ </a> &emsp;&emsp;<br><a href="/201911/KNNä¸Naive_Bayesä»£ç å®ç°/" itemprop="url"> KNNå’Œæœ´ç´ è´å¶æ–¯çš„ä»£ç å®ç° </a> &emsp;&emsp;<br><a href="/201911/Price_Suggestion_Chanllenge/" itemprop="url"> å•†å“ä»·æ ¼é¢„æµ‹æŒ‘æˆ˜ </a> &emsp;&emsp;<br><a href="/201911/Fisherç®—æ³•&SVM&K-MeansåŠå…¶ä¼˜åŒ–/" itemprop="url"> Fisherç®—æ³• &amp; SVM &amp; K-Meansçš„å®ç°å’Œä¼˜åŒ– </a> &emsp;&emsp;<br><a href="/201910/FINDSç®—æ³•å’ŒID3ç®—æ³•/" itemprop="url"> FINDSç®—æ³•å’ŒID3ç®—æ³• </a> &emsp;&emsp;<br><a href="/201904/æ’å…¥æ’åºå½’å¹¶æ’åºå’Œå¿«é€Ÿæ’åº/" itemprop="url"> ç®—æ³•è®¾è®¡ä¹‹æ’åº </a> &emsp;&emsp;<br><a href="/201904/å¾ªç¯èµ›èµ›ç¨‹å®‰æ’/" itemprop="url"> ç®—æ³•è®¾è®¡ä¹‹å¾ªç¯èµ›èµ›ç¨‹å®‰æ’ </a> &emsp;&emsp;</p><h2 id="å±•ç¤ºå’Œæ±‡æŠ¥"><a href="#å±•ç¤ºå’Œæ±‡æŠ¥" class="headerlink" title="å±•ç¤ºå’Œæ±‡æŠ¥"></a>å±•ç¤ºå’Œæ±‡æŠ¥</h2><p><a href="/201904/åŸºäºé“¾æ¥å†…å®¹çš„ç¤¾åŒºå‘ç°ï¼ˆä¸€ï¼‰/" itemprop="url"> åŸºäºé“¾æ¥å†…å®¹çš„ç¤¾åŒºå‘ç°ï¼ˆä¸€ï¼‰ </a> &emsp;&emsp;<br><a href="/201904/åŸºäºé“¾æ¥å†…å®¹çš„ç¤¾åŒºå‘ç°ï¼ˆäºŒï¼‰/" itemprop="url"> åŸºäºé“¾æ¥å†…å®¹çš„ç¤¾åŒºå‘ç°ï¼ˆäºŒï¼‰ </a> &emsp;&emsp;</p><h2 id="å­¦ä¹ ç¬”è®°"><a href="#å­¦ä¹ ç¬”è®°" class="headerlink" title="å­¦ä¹ ç¬”è®°"></a>å­¦ä¹ ç¬”è®°</h2><p><a href="/201908/ã€Œè¿ç§»å­¦ä¹ ç®€æ˜æ‰‹å†Œã€å­¦ä¹ ç¬”è®°ï¼ˆ1ï¼‰/" itemprop="url"> ã€Šè¿ç§»å­¦ä¹ ç®€æ˜æ‰‹å†Œã€‹å­¦ä¹ ç¬”è®° </a> &emsp;&emsp;<br><a href="/201909/å®éªŒå®¤è‹¦é€¼æ¬ç –æš‘å‡ç”Ÿæ´»çºªå®/" itemprop="url"> ç”¨æˆ·å¯¹é½ï¼ˆå®éªŒå®¤æ¬ç –çºªå®ï¼‰ </a> &emsp;&emsp;</p><h2 id="è¯»ä¹¦ç¬”è®°"><a href="#è¯»ä¹¦ç¬”è®°" class="headerlink" title="è¯»ä¹¦ç¬”è®°"></a>è¯»ä¹¦ç¬”è®°</h2><p><a href="/202005/ã€Šè‹ä¸œå¡ä¼ ã€‹æ‘˜å½•/" itemprop="url"> ã€Šè‹ä¸œå¡ä¼ ã€‹ </a> &emsp;&emsp;<br><a href="/202002/ç¥­äº¡å¦»ç¨‹æ°æ–‡/" itemprop="url"> ã€Šç¥­äº¡å¦»ç¨‹æ°æ–‡ã€‹ </a> &emsp;&emsp; </p>]]></content>
    
    
    <summary type="html">&lt;div class=&quot;tag-plugin note&quot; &gt;&lt;div class=&quot;body&quot;&gt;&lt;p&gt;ä»2019å¹´å¼€å§‹å°è¯•æ­å»ºè‡ªå·±çš„åšå®¢ï¼Œåˆ°ç°åœ¨å·²ç»æœ¬ç§‘æ¯•ä¸šã€‚è¿™ä¸‰å¹´çš„æ—¶é—´è™½ç„¶æ²¡æœ‰åšæŒæ›´æ–°åšå®¢ï¼Œä½†æ˜¯ä¸€ç›´åœ¨åšæŒè®°å½•ã€‚æ— è®ºæ˜¯ç†è®ºçŸ¥è¯†çš„å­¦ä¹ ã€Debugçš„è®°å½•ï¼Œè¿˜æ˜¯è¯»ä¹¦å¥èº«ç¬”è®°å’Œæ„Ÿæ…¨ä¸‡åƒæ—¶çš„éšç¬”ï¼Œä»–ä»¬éƒ½æ•£è½åœ¨å„ç§åœ¨çº¿ç¬”è®°è½¯ä»¶å’Œæ‰‹æœºçš„å¤‡å¿˜å½•ä¸­ã€‚&lt;/p&gt;&lt;p&gt;éšç€æ—¶é—´çš„æ¨ç§»ï¼Œæˆ‘å¯¹åšå®¢çš„è®¤è¯†ä¹Ÿå‘ç”Ÿäº†å¾ˆå¤šå˜åŒ–ã€‚â€œè®°å½•â€å¯¹æˆ‘æ¥è¯´ä»ç„¶å…·æœ‰éå¸¸å¤§çš„æ„ä¹‰ï¼Œä½†æ˜¯â€œåšå®¢â€æœ€é‡è¦çš„ä»·å€¼ä¸åº”è¯¥æ˜¯â€œè®°å½•â€ï¼Œè€Œæ˜¯â€œåˆ†äº«â€â€”â€”ä¼ é€’æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚æˆ‘æŠŠå¹³æ—¶çš„æ—¥å¸¸ç”Ÿæ´»å’Œå­¦ä¹ ç»å†è®°å½•åœ¨å„ç§ç¬”è®°ä¸­ï¼Œè¿‡ä¸€æ®µæ—¶é—´å†å»å›é¡¾å’Œæ•´ç†ï¼Œé€‰å–å…¶ä¸­æœ‰ä»·å€¼çš„ä¸œè¥¿å»åˆ†äº«ã€‚&lt;/p&gt;&lt;p&gt;è¿‡å»çš„ä¸€å¹´å†æ¬¡å‚åŠ äº†ç¾èµ›ï¼ŒåŠ å…¥äº†å‡ ä¸ªå’ŒåŒºå—é“¾æœ‰å…³çš„é¡¹ç›®ï¼Œä¹Ÿå®Œæˆäº†ä¸å°‘ä¸“åˆ©å’Œè®ºæ–‡ã€‚æ›´å¤šçš„æ—¶é—´å¿™äºè¿ç”¨çŸ¥è¯†ï¼Œä»¥éœ€æ±‚ä¸ºå¯¼å‘è¿«ä½¿è‡ªå·±è¿›è¡Œå¹¿æ³›è€Œä¸æ·±åˆ»çš„å­¦ä¹ ã€‚å¯¹æˆ‘æ¥è¯´çœŸæ­£æœ‰ä»·å€¼çš„è¾“å…¥å˜å°‘äº†ï¼Œè¾“å‡ºå’Œè¡¨è¾¾çš„æ¬²æœ›ä¹Ÿé™ä½äº†å¾ˆå¤šã€‚&lt;/p&gt;&lt;p&gt;   æˆ‘åœ¨åŸå…ˆçš„åšå®¢åŸŸåä¹‹ä¸‹åˆå»ºç«‹äº†ä¸€ä¸ªè‹±æ–‡åšå®¢&lt;a href=&quot;https://blog.crocodilezs.top/eysblog_en/&quot;&gt;BLOG-EN&lt;/a&gt;ï¼Œæ—¨åœ¨æ•´ç†è‡ªå·±è¿‡å»ä¸€å¹´ç»å†çš„å„ç§é¡¹ç›®ã€‚åŒæ—¶ä¹Ÿå°†åŸåšå®¢ä¸­çš„å†…å®¹è¿›è¡Œæ•´ç†å½’æ¡£ï¼Œä¾¿äºè‡ªå·±å’Œè®¿å®¢çš„æŸ¥é˜…ã€‚&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>[ICM 2021] Music Never Stops: Cracking the Secret of Musical Influence</title>
    <link href="https://enblog.crocodilezs.top/202102/2021-02-10-2021ICM-Music/"/>
    <id>https://enblog.crocodilezs.top/202102/2021-02-10-2021ICM-Music/</id>
    <published>2021-02-10T10:24:24.000Z</published>
    <updated>2022-01-14T14:09:07.804Z</updated>
    
    <content type="html"><![CDATA[<p>We participated in ICM 2021 during Feb 5th - Feb 9th and finally honored <strong>Meritorious Winner(Top 7%)</strong>ğŸ˜†. Because we 3 people are all music enthusiasts, we choosed the problem D about data mining in music similarity and influence. We thought we could do better in this field. It turns out that we were right. Here is our thinking and solution to this problem.</p><h1 id="Topic-and-Datasets"><a href="#Topic-and-Datasets" class="headerlink" title="Topic and Datasets"></a>Topic and Datasets</h1><p>Here are <a href="https://www.mathmodels.org/Problems/2021/ICM-D/2021_ICM_Problem_D.pdf">the topic and the datasets</a>. You can also see them on <a href="https://www.mathmodels.org/Problems/2021/ICM-D/index.html">COMAP</a>.</p><p>In short, our team has been identified by an organization to develop a model that measures musical influence. This problem asks us to examine evolutionary and revolutionary trends of artists and genres. We has been given 4 data sets:</p><ol><li><code>influence_data.csv</code> represents musical influencers and followers, as reported by the artists themselves, as well as the opinions of industry experts. These data contains influencers and followers for 5,854 artists in the last 90 years.</li><li><code>full_music_data.csv</code> provides 16 variable entries, including musical features such as <code>danceability</code>, <code>tempo</code>, <code>loudness</code>, and <code>key</code>, along with <code>artist_name</code> and <code>artist_id</code> for each of 98,340 songs. These data are used to create two summary data sets, including: mean values by artist - <code>data_by_artist.csv</code>, means across years <code>data_by_year.csv</code>.</li></ol><h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><blockquote><p>Music has been a necessary part of human life and history since we human have consciousness. At the same time of human evolution, music is constantly changing new forms and contents. Music itself doesnâ€™t evolve, and thereâ€™s no doubt that itâ€™s done by a group of people called â€œartistsâ€. Itâ€™s a common sense that a new form of music occurs under the action of many factors, such as artistsâ€™ innate creativity, current social or political events, access to new instruments or tools, or other personal experiences.</p></blockquote><center><img src="/eysblog_en/imgsource/icm2021-figure-1.png" alt="The architecture of our model" width="80%"/><p><font size=3 color="black">Figure 1: The architecture of our model</font></p></center><p>Based on some basic musical attributes, our aim in this report is to build a model to quantify musical evolution. We are expected to provide a measurement mechanism to observe how the previous music affects the later music and musicians. So two major models are established to finish that job.</p><p>For the model 1, we analyzed <strong>the relationships</strong> between genre, artist and music from the perspective of influence and similarity. We use the <strong>BA model</strong> to explained the influence network, which connects the influencers and followers. Based on the influence relationships between artists, we use <strong>the directed edge of the model</strong> to construct an influence-index. And then based on this index, we use <strong>the improved Louvain algorithm</strong> to in <strong>community partition</strong> to get The partition based on influence.</p><p>On the other hand, from the perspective of the similarity between artists, we use <strong>Pearson correlation coefficient</strong> to remove the redundant of music attributes, and construct the measure index of similarity. Based on this index, we use <strong>the improved K-Means algorithm</strong> in community partition, and finally obtain The partition based on similarity.</p><p>We creatively proposed to <strong>combine these two networks</strong>, and <strong>the NMI index</strong> is used to analyze the relationship between similarity and influence. Finally, we prove that the influencers actually affect the music created by the followers.</p><p>For the model 2, we took <strong>time</strong> into consideration. In order to analyze the musical evolution process, We use <strong>ARIMA model</strong> to create an ideal evolution curve. We compare the real curve with the ideal evolution curve to find how the social, culture and technology affect the music. We give an example of electronic music. Symbols and Definitions show in the Table 1.</p><center><img src="/eysblog_en/imgsource/icm2021-table-1.png" alt="" width="100%" /></center><h1 id="ğŸŒŸModel-1ğŸŒŸ-BA-Model-and-Optimized-Louvain-KMeans-Algorithm"><a href="#ğŸŒŸModel-1ğŸŒŸ-BA-Model-and-Optimized-Louvain-KMeans-Algorithm" class="headerlink" title="ğŸŒŸModel 1ğŸŒŸ: BA Model and Optimized Louvain-KMeans Algorithm"></a><strong>ğŸŒŸModel 1ğŸŒŸ</strong>: BA Model and Optimized Louvain-KMeans Algorithm</h1><p>Model 1 we analyze social networks from the perspective of influence and similarity, and then analyze the influence and similarity relationship among genres, artists and music.</p><p>In this model, we use directed graph to construct influence network and BA model to explain the influence propagation relationship in the network. First, we analyzed the influence relationship between artists. We propose the influence index by using the indegree and outdegree parameters inthe directed graph, and use the improved Louvain algorithm in community partition to get the â€œThe partition based on influenceâ€.</p><p>Then we analyze the similarity among the music, we use <strong>Pearson correlation coefficient</strong> to delete redundant attributes, and propose the measure of â€œsimilarityâ€. Through similarity measurement, the improved KMeans algorithm is used in community partition to obtain â€œThe partition based on â€œsimilarityâ€.</p><p>After that, we innovatively combine the above two networks, and perfectly explain the relationship between music influence and similarity by using NMI parameters. We call this innovative method Optimized Louvain KMeans Algorithm.</p><h2 id="Analysis-of-Music-Influence"><a href="#Analysis-of-Music-Influence" class="headerlink" title="Analysis of Music Influence"></a>Analysis of Music Influence</h2><h3 id="The-influence-of-network-BA-model"><a href="#The-influence-of-network-BA-model" class="headerlink" title="The influence of network (BA model)"></a>The influence of network (BA model)</h3><p>In Model 1 we analyze social networks from the perspective of influence and similarity, and then analyze the influence and similarity relationship between genres, artists and music.</p><p><code>Influence_data.csv</code> includes 5854 artists in the past 90 years according to the artists themselves and industry experts. <strong>We construct 42770 influence relationships among the 5854 artists through the data set, and the directed edges in the network point from the follower to the influencer</strong>. In the Figure3.1.1, artists of different genres are represented by different colors. There are <strong>19 genres of music (except unknown) in the dataset</strong>.</p><p>Through the Figure2, we can roughly know the influence relationship between different genres. For example, we can easily see that Pop/Rock music has influence on all other genres, which is also determined by the characteristics of Pop/Rock music itself. Based on this network, the later content of this paper will further analyze the relationship between the influence of music characteristics and social network, age, policy and so on.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-2.png" alt="" width="80%"/><p><font size=3 color="black">Figure 2: Directed network of musical influence</font></p></center><p>In network theory, scale-free network is a kind of complex network. Its typical feature is that most nodes in the network are only connected with a few nodes, and a few nodes are connected with a lot of nodes. In reality, many networks have scale-free characteristics, such as Internet, financial system network, social network and so on.</p><p>According to the influence direct graph, we guess that the influence network is a scale-free network, and then we will verify it by BA model.This model is based on two assumptions.</p><ul><li>Growth model: many real networks are constantly expanding and growing, such as the birth of new web pages in the Internet, the publication of new papers and so on. Obviously, the musical influence network is also expanding through the increasing influence relationship.</li><li>Priority connection mode: when new nodes join, they tend to connect with nodes with more connections. For example, new web pages usually have connections to well-known Web sites, and new papers tend to cite well-known literatures that have been widely cited, etc. In reality, the same is true of influence. New musicians are more likely to be influenced by<br>influential influencers.</li></ul><p>Based on the above two hypotheses, we randomly selected 500 followers from the data set, removed them from the established social network, and then connected them back to the influence network by building a scale-free network with BA model.</p><p>There are \(N<em>a\) nodes in the original network, we will add a new artist \(a_i\) to the network. When \(a_i\) is a new node, \(m\) edges are connected from the new node to the original node, and the connection mode is the node with priority given to the high heights. For the original artist \(a_j\) in the network,the number of degree in the original network is recorded as \(idg</em>{a<em>j}\). Then the probability \(p</em>{a_i,a_j}\) of the new node can be canculate as follows:</p><script type="math/tex; mode=display">P_{a_i,a_j}=\frac{idg_{a_j}}{\sum_{K \in artist_{id}}idg_{a_k}}\tag{1}</script><p>By choosing the appropriate probability threshold, we add these 500 nodes back to the influence network, and create related directed edges. According to the real influence network, we calculate the connection accuracy of each new node(\(a_i\)) and the original node. The accuracy of the connection betweennew node(\(a_i\)) and the original node is 81.74%. This shows that the influence network basically conforms to the BA model.</p><p>According to the properties of BA model, we can know that the distribution of the number of the followers of artists can be approximately described by a power function with a power exponent of 3. For artist \(a_i\), the distribution function of the number of people affected is:</p><script type="math/tex; mode=display">p(idg_{a_i}) \propto 2idg^3_{a_i}\tag{2}</script><p>The distribution function can help us better explain the spread of influence, and can also be used to predict which artists are more attractive to the new artists.</p><h3 id="Analysis-of-music-influence-from-the-perspective-of-genre"><a href="#Analysis-of-music-influence-from-the-perspective-of-genre" class="headerlink" title="Analysis of music influence from the perspective of genre"></a>Analysis of music influence from the perspective of genre</h3><p>To quantify the influence of music genres, based on the previous social network, we counted the number of directed edges between genres as the influence parameter, and built the heat map between genres. In order to show the heat map better, we take logarithm of the number of directed edges.</p><p>As can be seen from the below Figure 3, the influence of all artists is mainly reflected in the genre itself. Jazz has an impact on all other genres of music. Pop / Rock, R&amp;B and Vocal also have a great impact on other fields.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-3.png" alt="" width="60%"/><p><font size=3 color="black">Figure 3: Heat map between genres</font></p></center><h3 id="Influence-index"><a href="#Influence-index" class="headerlink" title="Influence-index"></a>Influence-index</h3><p>In order to evaluate the music influence of artists better, we introduce the following evaluation measures. On the basis of the number of followers, we construct the comprehensive measure of influence index, which combines the influence of artists in and out of their genres to give an objective evaluation of the influence of artists.</p><script type="math/tex; mode=display">Influence-index_i=F_{in_i}+logG_i * F_{out_i}\tag{3}</script><p>We choose pop / rock artists as a sub network to investigate the influence of all artists. According to the influence index we set, we find out the top ten artists of this genre.The music genres and influence-index influenced by the ten artists are shown in the following table.</p><center><img src="/eysblog_en/imgsource/icm2021-table-2.png" alt="" width="100%"/></center><p>It is easy to check the influencer and followers of each artist in the network we built. Taking The Beatles as an example, the influence network chart of its music is shown in the Figure 4.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-4.png" alt="" width="90%"/><p><font size=3 color="black">Figure 4: The influence network chart of The Beatles</font></p></center><h2 id="Analysis-of-Music-Similarity"><a href="#Analysis-of-Music-Similarity" class="headerlink" title="Analysis of Music Similarity"></a>Analysis of Music Similarity</h2><h3 id="The-similarity-index"><a href="#The-similarity-index" class="headerlink" title="The similarity index"></a>The similarity index</h3><p>In order to establish the music similarity measurement model, we can compare the similarity degree of each parameter between different music. In this question, there are 7 characteristics of the music and 5 types of vocals, a total of 12 parameters to reflect the characteristics of music (<code>danceability</code>, <code>energy</code>, <code>valence</code>, <code>tempo</code>, <code>loudness</code>, <code>mode</code>, <code>key</code>, <code>acousticenss</code>, <code>instrumentalness</code>, <code>livenss</code>, <code>speechiness</code>, <code>explicit</code>).</p><p>By calculating Pearson correlation coefficients between 12 music parameters, the correlation co-efficient matrix is obtained, and the redundant parameters with high correlation can be eliminated.</p><p>Energy and valence data in <code>full-music-data.csv</code> were taken as an example to calculate the correlation coefficient, and the data could be expressed as: \( e:\{e<em>{m_1}, e</em>{m<em>2}, e</em>{m<em>3}, â€¦, e</em>{m<em>{N_n}}\} \), \( v:\{v</em>{m<em>1}, v</em>{m<em>2}, v</em>{m<em>3},â€¦, v</em>{m_{N_n}}\} \).</p><script type="math/tex; mode=display">\begin{align}&Mean Value: E(e) = \frac{\sum_{i=1}^ne_{m_i}}{n}, E(v)=\frac{\sum_{i=1}^nv_{m_i}}{n}, \\&Covariance: Cov(e, v) = \frac{\sum_{i=1}^n(e_{m_i}-E(e))(v_{m_i}-E(v))}{n}, \\&Standard Deviation: \delta_e=\sqrt{\frac{\sum_{i=1}^n(e_{m_i}-E(e))^2}{n}}, \delta_v=\sqrt{\frac{\sum_{i=1}^n(v_{m_i}-E(v))^2}{n}}, \\&Pearson = \rho_{ev} = \frac{Cov(e, v)}{\delta_e\delta_v}=\frac{\frac{\sum_{i=1}^n(e_{m_i}-E(e))(V_{m_i}-E(v))}{\delta_e\delta_v}}{n}\end{align}\tag{4}</script><p>Using the data in full-music-data.csv, the correlation coefficient between each index can be calculated. Its correlation coefficient matrix is shown in Figure 5.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-5.png" alt="" width="100%"/><p><font size=3 color="black">Figure 5: The correlation coefficient between each index</font></p></center><p>In order to reduce the influence of the correlation between indicators, indicators in each group with the absolute value of correlation coefficient greater than 0.3 were selected. Then, one of the two indicators of each group was selected, and the index used to evaluate music similarity was as follows:<code>danceability</code>, <code>energy</code>, <code>mode</code>, <code>key</code>, <code>liveness</code>, <code>speechiness</code> and <code>explicit</code>.</p><p>The values of <code>mode</code> and <code>explict</code> are Boolean values, so they are excluded from the music similarity model. The similarity of songs was evaluated by comparing the degree of differentiation of the remaining five indicators between different songs. We defined a numerical quantity called similarity to indicate the degree of difference between songs, and the higher the similarity value is, the more similar the songs are.</p><p>For example, calculate the similarity of song j and song k in full-musci-data file, and its data can be expressed as: \(M<em>j{d</em>{m<em>j}, e</em>{m<em>j}, l</em>{m<em>j}, s</em>{m<em>j}, k</em>{m<em>j}}, M_k{d</em>{m<em>k}, e</em>{m<em>k}, l</em>{m<em>k}, s</em>{m<em>k}, k</em>{m_k}}\).<br>The similarity of the two songs is defined as:</p><script type="math/tex; mode=display">def: Similarity = \frac{1}{\sqrt{(d_{m_j}-d_{m_k})^2+(e_{m_j}-e_{m_K})^2+...+(k_{m_j}-k_{m_k})^2}}\tag{5}</script><h3 id="Similarity-analysis-between-music-genres"><a href="#Similarity-analysis-between-music-genres" class="headerlink" title="Similarity analysis between music genres"></a>Similarity analysis between music genres</h3><p>We selected 12 music genres in the data set to analyze their music similarity. For each genre, we selected 50 artists with the highest influence index, and calculated the music style similarity between these 500 artists. Then the music similarity between different genres is calculated according to the same genre and different genres. The results are shown in the Figure 6 below.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-6.png" alt="" width="60%"/><p><font size=3 color="black">Figure 6: Music similarity between different genres</font></p></center><p>The darker the color in the figure 6 is, the higher the similarity between genres. For example, the similarity between Blues and Pop/Rock,jazz and Country is very high, while the similarity between R&amp;B and Vocal is very low. Similar conclusions can be drawn directly from the figure.</p><p>In order to better distinguish the characteristics of music genres, we use radar chart to intuitively show the prominent characteristics of each genre.The musical characteristics of these 12 genres are shown in the Figure 7.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-7.png" alt="" width="80%"/><p><font size=3 color="black">Figure 7: Music characteristics</font></p></center><h3 id="The-analysis-of-the-factors-that-influence-the-spread-of-music"><a href="#The-analysis-of-the-factors-that-influence-the-spread-of-music" class="headerlink" title="The analysis of the factors that influence the spread of music"></a>The analysis of the factors that influence the spread of music</h3><p>Using the data, the correlation coefficients between 12 indexes and the popularity of music are calculated.</p><p>In this question, we assume that if the absolute value of the correlation coefficient is greater than 0.3, we can think that there is a strong correlation between the data. When the absolute value of the correlation coefficient is between 0.1 and 0.3, we can think that there is a correlation between the data.</p><p>The characteristics of music communication can be reflected from the popularity of music, so from the correlation coefficient, it can be concluded that the communicability of music is positively correlated with the dancebility, energy, loudness and explicit, and negatively correlated with the instrumental, acoustic and other indicators, and is most affected by the dancebility, loudness and acoustic.</p><h2 id="Optimized-Louvain-KMeans-Algorithm-and-NMI"><a href="#Optimized-Louvain-KMeans-Algorithm-and-NMI" class="headerlink" title="Optimized Louvain-KMeans Algorithm and NMI"></a>Optimized Louvain-KMeans Algorithm and NMI</h2><p>Through the heat map of influence and similarity, we can qualitatively observe the relationship between music influence and similarity. </p><p>Next, we innovatively propose the optimized louvain-kmeans algorithm to quantitatively analyze the relationship between music influence and similarity with the help of NMI.The architecture of this model is shown in the figure 8.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-8.png" alt="" width="80%"/><p><font size=3 color="black">Figure 8: Flow chart of the model</font></p></center><h3 id="The-community-partition-based-on-influence"><a href="#The-community-partition-based-on-influence" class="headerlink" title="The community partition based on influence"></a>The community partition based on influence</h3><p>The Louvain method for community detection is a method to extract communities from large networks created by Blondel et al. from the University of Louvain (the source of this methodâ€™s name). The method is a greedy optimization method that appears to run in time \(O(n Â· log_2n)\) if \(n\) is the number of nodes in the network.We will use the Louvain algorithm to divide the influence network.</p><p>Before using Louvain, we need to introduce the concept of modularity. We use the Louvain algorithm to make as many edges as possible in the community and as few as possible between the communities. The measure of this index is modularity. </p><p>The number of nodes in the network is \(N<em>a\), the number of edges is \(N</em>{infl}\), and the indegree of node \(a<em>i\) is \(idg</em>{a<em>i}\). The adjacency matrix of the network is expressed as A, \(A</em>{a_i,a_j}=0\) means there is no edge between \(a_i\) and \(a_j\). \(AVW = 1\) means there are edges between the two nodes. </p><p>Define variable s, and \(s<em>{a_ia_j}\) means that \(a_i\) and \(a_j\) belong to the same partition. \(s</em>{a<em>ia_j}=-1\) means that the two nodes belong to different partition. Then we can use \(\delta</em>{a<em>ia_j} = 1/2(s</em>{a_ia_j} + 1) \) to verify if \(a_i\) and \(a_j\) belong to the same partition in a quantitive way. If the result equals one, we can say the two nodes belong to the same partition. If not, the result equals zaro. Then the probability expectation of modularity can be expressed as:</p><script type="math/tex; mode=display">\overline{Q} = 1/2\frac{\sum_{a_ia_j}A_{a_ia_j}}{N_{infl}} \delta_{a_ia_j}\tag{6}</script><p>Modularity can be finally expressed as:</p><script type="math/tex; mode=display">Q=\frac{1}{2m}\sum_{a_ia_j}(A_{a_ia_j}-\frac{idg_{a_i}idg_{a_j}}{2N_{infl}})\delta_{a_ia_j}\tag{7}</script><p>Through greedy algorithm, the modularity is continuously optimized to achieve â€œcommunity partition based on influenceâ€,The schematic diagram of the improved Louvain algorithm is shown in the Figure 9.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-9.png" alt="" width="80%"/><p><font size=3 color="black">Figure 9: Optimized louvain algorithm</font></p></center><p>In the process of optimization, we select the artist with the biggest influence factor among 19 music genres, and prevent them from being divided into a module, so as to improve the efficiency of community partition. The community partition results are shown in the Figure 10.(different colors represent different communities)</p><center><img src="/eysblog_en/imgsource/icm2021-figure-10.png" alt="" width="50%"/><p><font size=3 color="black">Figure 10: The partition based on influence</font></p></center><h3 id="The-communiy-partition-based-on-similarity"><a href="#The-communiy-partition-based-on-similarity" class="headerlink" title="The communiy partition based on similarity"></a>The communiy partition based on similarity</h3><p>We use KMeans to divide all the artist nodes into communities, so that the music similarity within the community is higher, and the music similarity between the communities is lower. Similar to the Louvain algorithm, we use the artist with the largest influence factor in the same 19 music genres, and prevent them from being divided into a module, so as to improve the efficiency of community partition.The community partition results are shown in the Figure 11(different colors represent different communities)</p><center><img src="/eysblog_en/imgsource/icm2021-figure-11.png" alt="" width="50%"/><p><font size=3 color="black">Figure 11: The partition based on similarity</font></p></center><h3 id="The-relationship-between-influence-and-similarity-was-analyzed-through-NMI"><a href="#The-relationship-between-influence-and-similarity-was-analyzed-through-NMI" class="headerlink" title="The relationship between influence and similarity was analyzed through NMI"></a>The relationship between influence and similarity was analyzed through NMI</h3><p>NMI is often used to detect the difference between the results of the partition and the true partition of the network and calculate the correct rate. Here we use the NMI index to evaluate the similarity between the influence-based and similarity-based social partitions.The higher the NMI index is, the more similar the two partitions are, which indicates that the influencers actually affect the music created by the followers.</p><p>In our model,\(P<em>{SC}\) stands for â€œsimilarity-based community partitionâ€ and \(P</em>{IC}\) for â€œinfluence-based community partitionâ€, and the NMI index of these two can be expressed by the following formula:</p><script type="math/tex; mode=display">\mathrm{NMI}=\frac{-2 \sum_{i=1}^{C_{P_{I C}}} \sum_{j=1}^{C_{P_{S C}}} C_{i j} \cdot \log \left(\frac{C_{i j} \cdot N}{C_{i} \cdot C_{j}}\right)}{\sum_{i=1}^{C_{P_{I C}}} C_{i} \cdot \log \left(\frac{C_{i}}{N}\right)+\sum_{j=1}^{C_{P S C}} C_{i j} \cdot \log \left(\frac{C_{j}}{N}\right)}\tag{8}</script><p>where N is the number of nodes, C is a confusion matrix, the element \(C<em>{ij}\) in the matrix indicates the number that the nodes belonging to the community i in the SC partition also belong to communities j in the IC partition. \(P</em>{IC}(P_{SC})\) is the number of communities in IC(SC) partition, \(C_i(C_j)\) is the sum of elements in matrix C. The grater the value of NMI,the more similarity between SC and IC partition, when the NMI value is 1, it indicates that SC and IC are the same partition of the network.</p><p>Finally, the NMI value we calculated is 0.6237, indicating that the influencers actually affect the music created by the followers.</p><h1 id="ğŸŒŸModel-2ğŸŒŸ-Time-Series-Analysis"><a href="#ğŸŒŸModel-2ğŸŒŸ-Time-Series-Analysis" class="headerlink" title="ğŸŒŸModel 2ğŸŒŸ: Time-Series Analysis"></a><strong>ğŸŒŸModel 2ğŸŒŸ</strong>: Time-Series Analysis</h1><p>It is a normal process that music genres emerge, evolve, and disappear. Our team member managed to observe big turns over time, and identify the key revolutionary artist of each genre. Whenever a music genre is about to leap, there will always be clues to change. As for a music genre, it is obvious that the explosive growth in the number of new artists and new songs indicates the prevalence and significant leap of the genre. So our team counted the number of artists and songs in the history of ten major music genres, and found the time of change in the visual image. According to the influence network, the most influential artists in these years were identified as the pioneers of the music revolution, that is, the so-called music revolutionaries. The details are shown below.</p><h2 id="The-evolution-of-genres-over-time"><a href="#The-evolution-of-genres-over-time" class="headerlink" title="The evolution of genres over time"></a>The evolution of genres over time</h2><p>We have studied the evolution of ten genres over time. In the text, we choose Jazz, R&amp;B and Country as three genres to illustrate their evolution over time.</p><h3 id="New-Artists"><a href="#New-Artists" class="headerlink" title="New Artists"></a>New Artists</h3><p>First of all, look at the number of artists added from the genre.According to the data given, the statistical changes of the number of people in the three schools from 1930 to 2010 are shown in Figure 12.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-12.png" alt="" width="60%"/><p><font size=3 color="black">Figure 12: The number of artists in different musical genres</font></p></center><p>It can be seen from the figure that jazz, R&amp;B and country all rose in the United States in the 1930s. Jazz flourished from the 1930s to the 1950s, and the number of new artists reached 102 in the 1950s. After the 1950s, jazz began to decline rapidly, and the number continued to decline. By the 2010s, there was no jazz New artists are included in the statistics.</p><p>R&amp;B music developed slowly from 1930â€™s to 1940â€™s, and the number of artists increased slowly. However, as time went into 1940â€™s, R&amp;B music genres developed rapidly, and the number of new artists increased from 17 in 1940 to 104 in 1950 in just 10 years. When R&amp;B developed into 1950s, the number of artists still maintained a steady growth momentum, and reached the peak in 1960 From the 1960s to the 21st century, R&amp;B declined as a whole. The number of new artists has been decreasing except for the growth in the 1980s. In 2010, the number of new artists fell back to the level of the early 1940s.</p><p>The number of artists in the country music genre increased slowly, showing a fluctuating upward trend from 1930 to 1990. The number of new artists reached the peak of 74 in 1990. However, compared with the peak of jazz and R&amp; B genre, the number of artists was still small. After 90 years, the country music genre began to decline, and the number of new artists fell back to the<br>beginning of the genre The number of musicians.</p><h3 id="The-release-of-songs"><a href="#The-release-of-songs" class="headerlink" title="The release of songs"></a>The release of songs</h3><p>In terms of the number of songs released, in the data used, we exclude the songs jointly released by multiple artists, and all the songs included in the calculation are published by artists alone, which can avoid that a song may be released by artists of multiple genres.</p><p>According to the given data, the changes of the number of songs released by the three genres from 1920 to 2020 are counted, as shown in Figure 13.</p><p>Compared with the broken line trend in Figure 1 and Figure 2, we can conclude that the increase of new generation artists will significantly affect the number of songs released, and this effect is often ahead of the increase in the number of songs released, and there is a cumulative effect. Taking jazz music genre as an example, the number of new generation artists in the genre was at a high growth level from 1930 to 1960. In 1950, the number of new generation artists reached its peak. During this period, the genre accumulated a large number of excellent artists. These artists matured in the 1950s and 1960s, and a large number of music works emerged. During this period, the music circulation of jazz music was much higher than that of any other period, reaching the peak of 486 songs in 1957. The same is true of R&amp;B music. In 1960, the number of new generation artists<br>reached its peak, and then in 1972, the number of R&amp;B music released reached its peak of 339.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-13.png" alt="" width="60%"/><p><font size=3 color="black">Figure 13: Total number of songs released each year by different genre</font></p></center><h3 id="Genre-popularity"><a href="#Genre-popularity" class="headerlink" title="Genre popularity"></a>Genre popularity</h3><p>From the perspective of popularity, we first give the definition of genre popularity. Gene popularity: based on the active-Start, the arithmetic mean value of the popularity of all artists of the same genre is defined as genre popularity.</p><p>For example: jazz music genre, active- There are 45 artists who started in 1930, according to the data table-by-artist.csv We can know the popularity value of artists who meet the conditions, and we can get the popularity of jazz music in 1930 by taking the arithmetic average of these values.</p><p>According to the above definition, we can get the change curve of the three genres from 1930 to<br>2010, as shown in Figure 14.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-14.png" alt="" width="60%"/><p><font size=3 color="black">Figure 14: Grnre popularity</font></p></center><p>From the trend of the line chart, the value of Genre popularity is increasing over time. The reason for the decline of jazz curve from 2000 to 2010 is the lack of 2010 active-Startâ€™s Jazz artist data, so its value was zero in 2010. Because the popularity of songs is calculated by algorithm, and the result largely depends on the total number of tracks played and the time of the most recent track played. Generally speaking, as time goes on, the frequency of playing the track will increase significantly, and it has a higher popularity. Therefore, the popularity of genres will increase with<br>the passage of time.</p><h2 id="The-influence-of-external-factors-on-the-development-trend-of-genres"><a href="#The-influence-of-external-factors-on-the-development-trend-of-genres" class="headerlink" title="The influence of external factors on the development trend of genres"></a>The influence of external factors on the development trend of genres</h2><p>We divide the ideal evolution of music genres into four stages: initial stage, development stage, booming stage and recession stage. We use the number of songs of a genre in a certain period to express the prosperity of the genre in that period. Based on the number curve of 10 genres, we use ARIMA model to fit an ideal evolution curve of genres.</p><p>Without the influence of social environment, scientific and technological development and other factors, genres will evolve according to the ideal curve.</p><p>ARIMA model(Auto regressive Integrated Moving Average model ) is one of the time series prediction methods. In ARIMA (P, D, q), AR is â€œautoregressiveâ€, P is the number of autoregressive terms; Ma is â€œmoving averageâ€, q is the number of moving average terms, and D is the difference order of making it a stationary sequence.</p><p>The ideal evolution curve of genres fitted by this model is shown in the Figure 15 (the light blue area is the error range). Compared with the ideal evolution curve and the actual evolution curve of music genre, when there is a significant difference between the two in a certain period, it shows that there are social and cultural factors that have a great impact on the genre at this time.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-15.png" alt="" width="60%"/><p><font size=3 color="black">Figure 15: The ideal evolution curve of genres</font></p></center><p>Taking electronic music as an example, the evolution curve of electronic music and the evolution curve of the genre in the ideal state are shown in the Figure 16.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-16.png" alt="" width="60%"/><p><font size=3 color="black">Figure 16: The evolution curve of electronic</font></p></center><p>In fact,following the emergence of raving, pirate radios, and an upsurge of interest in club culture. Electronic music achieved widespread mainstream popularity in Europe. Meanwhile, MIDI devices, which has been the musical instrument industry standard interface since the 1980s through to the present day, became commercially available in 1980s.These cultural and technological factors have promoted the rapid development of electronic music.</p><h1 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h1><p>To understand and measure the influence of previously poducted music on new music and musical artists, we proposed a series of novel models to address the sub-issues from creating the influence network based on the similarity between artists. The proposed model achieves high accuracy and robustness.</p><ul><li>We create the directed network based on music influence and use BAmodel to explain how the influence network expend. Thenwe propose Influence-Index to analyze the influence between genres and artists. We select the most 10 influential artists and show their influence-index.</li><li>In order to analyze the similarity between genres, artists and music, we use correlation analysis to remove redundant attributes. We finally select 7 attributes which actually infect the similarity. They are danceability, energy, key, liveness, speechiness, mode and explicit. Based on these attributes, we propes similirity index.</li><li>To find the relationship between the music similarity and the influence, we propose a Optimized LOUVAIN-KMeans Algorithm and use it to community partition. By participating artists into different community through Louvain algorithm and KMeans algorithm, we can obtain 2 different partition. Then we use NMI to estimate the similarity between these 2 partions. Finally we get the conclusion that influencers actually affect the music created by followers.</li><li>We analyze the influence processes of musical evolution that occoured over time. We use ARIMA model to create an ideal evolution curve. By comparing with the ideal evolution curve, we can find how the social, culture and technology affect the music. We give an example about electronic music.</li></ul><h1 id="Strength-and-weaknesses"><a href="#Strength-and-weaknesses" class="headerlink" title="Strength and weaknesses"></a>Strength and weaknesses</h1><h2 id="Strengths"><a href="#Strengths" class="headerlink" title="Strengths"></a>Strengths</h2><p>-We creatively proposed Optimized LOUVAIN-KMeans Algorithm and use the partition to explain the relationship between the influence and music similarity. The community network is as a bridge between them, and the model can reflect the relationship effectively.</p><ul><li>We proposse Influence-Index to estimate the influence of the artists objectively. Itâ€™s not accuracy to use only the number of followers or the number of music.</li><li>When analyzing the similarity between music and artists, our model is simple and convenient. Among the 12 music attributes given in the original data set, we remove the redundant attributes through correlation analysis, so that our model can calculate the similarity more efficiently and maintain a higher accuracy.</li><li>We creatively use ARIMA model to generate an ideal evolution curce of the genre. It make our model more robust, and can be used in many other situation. The ideal evolution curve reveal the process of the evolution in these genres.</li></ul><h2 id="Weakness"><a href="#Weakness" class="headerlink" title="Weakness"></a>Weakness</h2><ul><li>We donâ€™t consider the influence between genres when we construct the ideal evolution curve. And new genres will appear at any time, so the curve may not that accuracy.</li><li>The interpretability of the influence and similarity model is not strong. We only find the relationship between similarity and influence. But we donâ€™t know how it operates indetail.</li><li>The amount of data of some minority genres is too small, the prediction result of the model for minority genres is not good.</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;We participated in ICM 2021 during Feb 5th - Feb 9th and finally honored &lt;strong&gt;Meritorious Winner(Top 7%)&lt;/strong&gt;ğŸ˜†. Because we 3 peop</summary>
      
    
    
    
    <category term="Mathematical Modeling" scheme="https://enblog.crocodilezs.top/categories/Mathematical-Modeling/"/>
    
    
  </entry>
  
  <entry>
    <title>A Renewable Energy Certificate Trading System Based on Blockchain</title>
    <link href="https://enblog.crocodilezs.top/202110/2021-10-02-REC-trading/"/>
    <id>https://enblog.crocodilezs.top/202110/2021-10-02-REC-trading/</id>
    <published>2020-10-02T09:32:11.000Z</published>
    <updated>2022-08-25T13:24:55.975Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>At present, the cumbersome issuing process of renewable energy certificate (REC) and inflexible pricing mechanism consume a lot of manpower and material resources. In order to solve this problem, this paper proposes a hybrid REC trading system based on Consortium Blockchain. The paper introduces the operation mode of the system in detail and changes the view replacement protocol in the Practical Byzantine Fault Tolerance (PBFT) Algorithm to improve the stability of the system. It also introduces the bidding rules of Continuous Double Auction (CDA) used in the system, and designs the bidding strategies to maximize the userâ€™s profit and the success rate of transaction. Finally, ARIMA model is also used to forecast the price of RECs to provide guidance for both buyers and sellers. </p><ul><li><strong>Index Terms</strong><br>REC transaction, Consortium Blockchain, Continuous Double Auction, ARIMA</li></ul><h1 id="PDF"><a href="#PDF" class="headerlink" title="PDF"></a>PDF</h1><p><iframe style="height: 1000px; width: 100%; display: block" frameborder="0"  scrolling="no" src="/eysblog_en/objectsource/BDRA-13.pdf"> </iframe><br><br /></p><p>Accepted by TrustCom 2021: International Conference on Trust, Security and Privacy in Computing and Communications, Jul.2021.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h1&gt;&lt;p&gt;At present, the cumbersome issuing process of </summary>
      
    
    
    
    <category term="Blockchain" scheme="https://enblog.crocodilezs.top/categories/Blockchain/"/>
    
    
  </entry>
  
  <entry>
    <title>Netizen Sentiment Recognition During COVID-19</title>
    <link href="https://enblog.crocodilezs.top/202006/2020-06-18-Netizen-Sentiment/"/>
    <id>https://enblog.crocodilezs.top/202006/2020-06-18-Netizen-Sentiment/</id>
    <published>2020-08-19T14:42:24.000Z</published>
    <updated>2022-01-15T12:48:26.463Z</updated>
    
    <content type="html"><![CDATA[<p>The emergence of the COVID-19 has disrupted our normal life. Peopleâ€™s psychological state can also receive a great negative impact. We chose such a topic with humanistic concern in <code>Data Warehouse and Data Mining</code> course, hoping that through Weibo posts we can gain more insight into peopleâ€™s emotional state during the COVID-19. We are awarded the best project (1/23) in this course, and we think we could do more in psychological care during COVID-19.</p><h1 id="Data-Source-and-Data-Structure"><a href="#Data-Source-and-Data-Structure" class="headerlink" title="Data Source and Data Structure"></a>Data Source and Data Structure</h1><p>This data comes from a data mining competition organized by DataFountain. The data set is a collection of Weibo crawled during the COVID-19. The link to the dataset is <a href="https://www.datafountain.cn/competitions/423/datasets">here</a>.</p><blockquote><p>The dataset cannot be downloaded directly from the official website because the competition has ended, the dataset can also be found on <a href="https://www.kaggle.com/liangqingyuan/chinese-text-multi-classification?select=nCoV_100k_train.labled.csv">Kaggle</a>.</p></blockquote><p>We used 100,000 posts from the training set provided by the competition website, but considering the time and computational cost, I only used the first 10,000 posts with annotations, and divided them into a training set and a test set in a 7/3 ratio. This dataset was based on 230 keywords related to the topic of â€œæ–°å† è‚ºç‚â€, which means COVID-19 in Chinese. </p><p>1,000,000 Weibo posts were collected from January 1, 2020 to February 20, 2020, and 100,000 Weibo posts were labeled with three categories: 1 (positive), 0 (neutral) and -1 (negative). The data is stored in csv format in <code>nCoV-100k.labeled.csv</code> file. The original dataset contains 100,000 user-labeled posts in the following format: <code>[post id, posting time, posting account, content, photos, videos, sentiment]</code>.</p><p>The original dataset has six attributes: <code>post id</code> (hashcode), <code>posting time</code> (Date), <code>posting account</code> (String), <code>content</code> (String), <code>photos</code> (String), and <code>videos</code> (String). Predicting <code>sentiment</code>(Int) by the above attributes. The purpose of this project is to use <strong>word bag preprocessing</strong>, <strong>TF-IDF preprocessing</strong>, <strong>word2vec</strong> and compare their effects, focusing on text processing and text sentiment analysis. So we only chose <code>content</code> attribute to predict the sentiment. (It was hard for us to read the emotion from photos and videos.)</p><p>Here is the statistical information of the dataset from Kaggle.</p><center><img src="/eysblog_en/imgsource/covid-figure-1.png" alt="" width="80%"/></center><p>The bar chart shows that the number of positive, neutral and negative posts varies considerably, with the highest number of neutral posts.</p><p>Here is the first post in dataset.</p><center><img src="/eysblog_en/imgsource/covid-figure-2.png" alt="" width="80%"/></center><p>We found this posts in Weibo APP.</p><center><img src="/eysblog_en/imgsource/covid-figure-3.png" alt="" width="80%"/></center><h1 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h1><p>We Used Kaggle Kernel. Kaggle provides free access to the Nvidia K80 GPU in the kernel. This benchmark shows that using the GPU for your kernel can achieve a 12.5x speedup in the training of deep learning models.</p><p>ref: <a href="https://www.cnblogs.com/pinard/p/6744056.html">https://www.cnblogs.com/pinard/p/6744056.html</a></p><h2 id="Data-import-and-turncut"><a href="#Data-import-and-turncut" class="headerlink" title="Data import and turncut"></a>Data import and turncut</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd filepath = <span class="string">&#x27;/kaggle/input/chinese-text-multi-classification/nCoV_100k_train.labled.cs v&#x27;</span> file_data = pd.read_csv(filepath)</span><br><span class="line">data = file_data.head(<span class="number">10000</span>)</span><br><span class="line"><span class="comment"># chose content and sentiment</span></span><br><span class="line">data = data[[<span class="string">&#x27;å¾®åšä¸­â½‚å†…å®¹&#x27;</span>, <span class="string">&#x27;æƒ…æ„Ÿå€¾å‘&#x27;</span>]]</span><br></pre></td></tr></table></figure><h2 id="Handling-missing-values"><a href="#Handling-missing-values" class="headerlink" title="Handling missing values"></a>Handling missing values</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># handling missing values</span></span><br><span class="line">data.isnull().<span class="built_in">sum</span>()</span><br><span class="line">data = data.dropna()</span><br></pre></td></tr></table></figure><h2 id="Remove-meaningless-symbols"><a href="#Remove-meaningless-symbols" class="headerlink" title="Remove meaningless symbols"></a>Remove meaningless symbols</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clean_zh_text</span>(<span class="params">text</span>): </span><br><span class="line">    <span class="comment"># keep English, digital and Chinese </span></span><br><span class="line">    comp = re.<span class="built_in">compile</span>(<span class="string">&#x27;[^A-Z^a-z^0-9^\u4e00-\u9fa5]&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> comp.sub(<span class="string">&#x27;&#x27;</span>, text)</span><br><span class="line">data[<span class="string">&#x27;å¾®åšä¸­â½‚å†…å®¹&#x27;</span>] = data.å¾®åšä¸­â½‚å†…å®¹.apply(clean_zh_text)</span><br></pre></td></tr></table></figure><h2 id="Word-Cut"><a href="#Word-Cut" class="headerlink" title="Word Cut"></a>Word Cut</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># word cut </span></span><br><span class="line"><span class="keyword">import</span> jieba </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chinese_word_cut</span>(<span class="params">mytext</span>): </span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot; &quot;</span>.join(jieba.cut(mytext))</span><br><span class="line"></span><br><span class="line">data[<span class="string">&#x27;cut_comment&#x27;</span>] = data.å¾®åšä¸­â½‚å†…å®¹.apply(chinese_word_cut)</span><br><span class="line"></span><br><span class="line"><span class="comment"># divided them into a training set and a test set in a 7/3 ratio.</span></span><br><span class="line">lentrain = <span class="built_in">int</span>((<span class="number">10000</span>-<span class="number">30</span>)*<span class="number">0.7</span>) </span><br><span class="line">lentest = <span class="built_in">int</span>((<span class="number">10000</span>-<span class="number">30</span>)*<span class="number">0.3</span>) </span><br><span class="line">x_train = data.head(lentrain)[<span class="string">&#x27;å¾®åšä¸­â½‚å†…å®¹&#x27;</span>] </span><br><span class="line">y_train = data.head(lentrain)[<span class="string">&#x27;æƒ…æ„Ÿå€¾å‘&#x27;</span>] </span><br><span class="line">x_test = data.tail(lentest)[<span class="string">&#x27;å¾®åšä¸­â½‚å†…å®¹&#x27;</span>]</span><br><span class="line">y_test = data.tail(lentest)[<span class="string">&#x27;æƒ…æ„Ÿå€¾å‘&#x27;</span>]</span><br></pre></td></tr></table></figure><center><img src="/eysblog_en/imgsource/covid-figure-4.png" alt="" width="80%"/></center><center><img src="/eysblog_en/imgsource/covid-figure-5.png" alt="" width="80%"/></center><h2 id="Import-Stop-Words"><a href="#Import-Stop-Words" class="headerlink" title="Import Stop Words"></a>Import Stop Words</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import stop words</span></span><br><span class="line">stpwrdpath = <span class="string">&quot;/kaggle/input/stop-wordstxt/stop_words.txt&quot;</span> </span><br><span class="line">stpwrd_dic = <span class="built_in">open</span>(stpwrdpath, <span class="string">&#x27;rb&#x27;</span>) </span><br><span class="line">stpwrd_content = stpwrd_dic.read() </span><br><span class="line"></span><br><span class="line"><span class="comment">#transform into list </span></span><br><span class="line">stpwrdlst = stpwrd_content.splitlines()</span><br><span class="line">stpwrd_dic.close()</span><br></pre></td></tr></table></figure><h2 id="Word-Bag-Preprocess"><a href="#Word-Bag-Preprocess" class="headerlink" title="Word Bag Preprocess"></a>Word Bag Preprocess</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text </span><br><span class="line"><span class="keyword">import</span> CountVectorizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># CountVectorizer initialize</span></span><br><span class="line">count_vec = CountVectorizer(stop_words=stpwrdlst) </span><br><span class="line">x_train_list = x_train.tolist() </span><br><span class="line">x_train_cv = count_vec.fit_transform(x_train_list).toarray() </span><br><span class="line">x_test_list = x_test.tolist()</span><br><span class="line">x_test_cv = count_vec.fit_transform(x_test_list).toarray()</span><br></pre></td></tr></table></figure><center><img src="/eysblog_en/imgsource/covid-figure-6.png" alt="" width="80%"/></center><h2 id="TF-IDF-Preprocess"><a href="#TF-IDF-Preprocess" class="headerlink" title="TF-IDF Preprocess"></a>TF-IDF Preprocess</h2><p>ref:<a href="https://blog.csdn.net/blmoistawinde/article/details/80816179">https://blog.csdn.net/blmoistawinde/article/details/80816179</a></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer </span><br><span class="line">tfidf_vec = TfidfVectorizer(token_pattern=<span class="string">r&quot;(?u)\b\w+\b&quot;</span>, max_df=<span class="number">0.6</span>, stop_words=stpwr dlst) </span><br><span class="line">x_train_tiv = tfidf_vec.fit_transform(x_train_list).toarray()</span><br><span class="line">x_test_tiv = tfidf_vec.fit_transform(x_test_list).toarray()</span><br></pre></td></tr></table></figure><center><img src="/eysblog_en/imgsource/covid-figure-7.png" alt="" width="80%"/></center><h2 id="word2vec-embedding"><a href="#word2vec-embedding" class="headerlink" title="word2vec embedding"></a>word2vec embedding</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># word2vec </span></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec </span><br><span class="line">model = Word2Vec(x_train_list, hs=<span class="number">1</span>,min_count=<span class="number">1</span>,window=<span class="number">10</span>,size=<span class="number">100</span>)</span><br><span class="line"><span class="keyword">from</span> gensim.test.utils <span class="keyword">import</span> common_texts, get_tmpfile </span><br><span class="line">path = get_tmpfile(<span class="string">&quot;word2vec.model&quot;</span>) </span><br><span class="line">model.save(<span class="string">&quot;word2vec.model&quot;</span>)</span><br><span class="line"><span class="comment"># model = Word2Vec.load(&quot;word2vec.model&quot;)</span></span><br></pre></td></tr></table></figure><center><img src="/eysblog_en/imgsource/covid-figure-8.png" alt="" width="80%"/></center><p>The corpus of 10,000 training data is still a bit small, but the results are slightly more productive. For example, if we look at the close synonyms of â€œå¼€å¿ƒâ€(happy), we can see that the results returned are more positive.</p><center><img src="/eysblog_en/imgsource/covid-figure-9.png" alt="" width="80%"/></center><h1 id="Data-Mining-Algorithm"><a href="#Data-Mining-Algorithm" class="headerlink" title="Data Mining Algorithm"></a>Data Mining Algorithm</h1><p>In this section, we will use <code>SVM</code>, <code>decision tree</code> and <code>RNN</code> algorithms to achieve classification. The embedding obtained by <code>BOW</code> and <code>TF-IDF</code> will be classified by SVM and decision tree algorithms, respectively, while the embedding obtained by Word2Vec will be classified by RNN.</p><p>The embedding obtained from Word2Vec will be classified using RNN. The detailed algorithm flow is as follows.</p><center><img src="/eysblog_en/imgsource/covid-figure-10.png" alt="" width="80%"/></center><h2 id="BOW-SVM"><a href="#BOW-SVM" class="headerlink" title="BOW + SVM"></a><code>BOW</code> + <code>SVM</code></h2><p>The 35596-dimensional embedding of the Weibo posts obtained by <code>BOW</code> is used as the input of the <code>SVM</code> in the <code>sklearn</code> package.<br>The parameters are as follows.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf = svm.SVC(C=<span class="number">0.8</span>, kernel=<span class="string">&#x27;rbf&#x27;</span>, gamma=<span class="number">20</span>, decision_function_shape=<span class="string">&#x27;ovr&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler </span><br><span class="line">scale = StandardScaler() </span><br><span class="line">scale_fit = scale.fit(x_cv) </span><br><span class="line"><span class="comment">#x = scale_fit.transform(x) </span></span><br><span class="line">lentrain = <span class="built_in">int</span>((<span class="number">10000</span>-<span class="number">30</span>)*<span class="number">0.7</span>) </span><br><span class="line">lentest = <span class="built_in">int</span>((<span class="number">10000</span>-<span class="number">30</span>)*<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">x_train_cv = scale_fit.transform(x_cv[:lentrain]) </span><br><span class="line">y_train = y[:lentrain] </span><br><span class="line">x_test_cv = scale_fit.transform(x_cv[(-<span class="number">1</span>)*lentest-<span class="number">1</span>:-<span class="number">1</span>]) </span><br><span class="line">y_test = y[(-<span class="number">1</span>)*lentest-<span class="number">1</span>:-<span class="number">1</span>] </span><br><span class="line"><span class="built_in">print</span>(x_train_cv.shape) </span><br><span class="line"><span class="built_in">print</span>(y_train.shape) </span><br><span class="line"><span class="built_in">print</span>(x_test_cv.shape)</span><br><span class="line"><span class="built_in">print</span>(y_test.shape)</span><br></pre></td></tr></table></figure><p><strong>TIPS:</strong></p><ul><li><code>SVM</code> and <code>Decision Tree</code> algorithms for 10,000 of 30,000-dimensional data can take a lot of time, and sklearn does not support GPU computing.</li><li>When you encounter a very large dataset, you should first use a small demo to check the correctness of the code, and then run a large demo with a large amount of data.</li><li><code>BOW</code> and <code>TF-IDF</code> should be used first before dividing the test and training sets, otherwise the test and training sets will not have the same dimensionality! It took a lot of time to fix this error.</li><li>Due to the excessive number of dimensions, remember to normalize the data before <code>SVM</code>.</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># prepare the data</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">scale = StandardScaler()</span><br><span class="line">scale_fit = scale.fit(x_cv)</span><br><span class="line"><span class="comment">#x = scale_fit.transform(x)</span></span><br><span class="line">lentrain = <span class="built_in">int</span>((<span class="number">10000</span>-<span class="number">30</span>)*<span class="number">0.7</span>)</span><br><span class="line">lentest = <span class="built_in">int</span>((<span class="number">10000</span>-<span class="number">30</span>)*<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">x_train_cv = scale_fit.transform(x_cv[:lentrain])</span><br><span class="line">y_train = y[:lentrain]</span><br><span class="line">x_test_cv = scale_fit.transform(x_cv[(-<span class="number">1</span>)*lentest-<span class="number">1</span>:-<span class="number">1</span>])</span><br><span class="line">y_test = y[(-<span class="number">1</span>)*lentest-<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(x_train_cv.shape)</span><br><span class="line"><span class="built_in">print</span>(y_train.shape)</span><br><span class="line"><span class="built_in">print</span>(x_test_cv.shape)</span><br><span class="line"><span class="built_in">print</span>(y_test.shape)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#test bow+svm</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x_train_cv.shape, <span class="string">&#x27;and &#x27;</span>, y_train.shape)</span><br><span class="line">clf = svm.SVC(C=<span class="number">0.8</span>, kernel=<span class="string">&#x27;rbf&#x27;</span>, gamma=<span class="number">20</span>, decision_function_shape=<span class="string">&#x27;ovr&#x27;</span>)</span><br><span class="line">cv_model = clf.fit(x_train_cv, y_train)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score, f1_score, accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Precision_score: &#x27;</span>, precision_score(y_hat_cv, y_test, average=<span class="string">&#x27;weighted&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Recall_score: &#x27;</span>, recall_score(y_hat_cv, y_test, average=<span class="string">&#x27;weighted&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;F1_score: &#x27;</span>, f1_score(y_hat_cv, y_test, average=<span class="string">&#x27;weighted&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy_score: &#x27;</span>, accuracy_score(y_hat_cv, y_test))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># roc_curve:çœŸæ­£ç‡ï¼ˆTrue Positive Rate , TPRï¼‰æˆ–çµæ•åº¦ï¼ˆsensitivityï¼‰</span></span><br><span class="line"><span class="comment"># æ¨ªåæ ‡ï¼šå‡æ­£ç‡ï¼ˆFalse Positive Rate , FPRï¼‰</span></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> interp</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> label_binarize</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> cycle</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve, auc</span><br><span class="line"></span><br><span class="line">nb_classes = <span class="number">3</span></span><br><span class="line"><span class="comment"># Binarize the output</span></span><br><span class="line">Y_valid = label_binarize(y_test, classes=[i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_classes)])</span><br><span class="line">Y_pred = label_binarize(y_hat_cv, classes=[i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_classes)])</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="comment"># Compute ROC curve and ROC area for each class</span></span><br><span class="line">fpr = <span class="built_in">dict</span>()</span><br><span class="line">tpr = <span class="built_in">dict</span>()</span><br><span class="line">roc_auc = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_classes):</span><br><span class="line">    fpr[i], tpr[i], _ = roc_curve(Y_valid[:, i], Y_pred[:, i])</span><br><span class="line">    roc_auc[i] = auc(fpr[i], tpr[i])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute micro-average ROC curve and ROC area</span></span><br><span class="line">fpr[<span class="string">&quot;micro&quot;</span>], tpr[<span class="string">&quot;micro&quot;</span>], _ = roc_curve(Y_valid.ravel(), Y_pred.ravel())</span><br><span class="line">roc_auc[<span class="string">&quot;micro&quot;</span>] = auc(fpr[<span class="string">&quot;micro&quot;</span>], tpr[<span class="string">&quot;micro&quot;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute macro-average ROC curve and ROC area</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># First aggregate all false positive rates</span></span><br><span class="line">all_fpr = np.unique(np.concatenate([fpr[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_classes)]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Then interpolate all ROC curves at this points</span></span><br><span class="line">mean_tpr = np.zeros_like(all_fpr)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_classes):</span><br><span class="line">    mean_tpr += interp(all_fpr, fpr[i], tpr[i])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Finally average it and compute AUC</span></span><br><span class="line">mean_tpr /= nb_classes</span><br><span class="line"></span><br><span class="line">fpr[<span class="string">&quot;macro&quot;</span>] = all_fpr</span><br><span class="line">tpr[<span class="string">&quot;macro&quot;</span>] = mean_tpr</span><br><span class="line">roc_auc[<span class="string">&quot;macro&quot;</span>] = auc(fpr[<span class="string">&quot;macro&quot;</span>], tpr[<span class="string">&quot;macro&quot;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot all ROC curves</span></span><br><span class="line">lw = <span class="number">2</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(fpr[<span class="string">&quot;micro&quot;</span>], tpr[<span class="string">&quot;micro&quot;</span>],</span><br><span class="line">         label=<span class="string">&#x27;micro-average ROC curve (area = &#123;0:0.2f&#125;)&#x27;</span></span><br><span class="line">               <span class="string">&#x27;&#x27;</span>.<span class="built_in">format</span>(roc_auc[<span class="string">&quot;micro&quot;</span>]),</span><br><span class="line">         color=<span class="string">&#x27;deeppink&#x27;</span>, linestyle=<span class="string">&#x27;:&#x27;</span>, linewidth=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(fpr[<span class="string">&quot;macro&quot;</span>], tpr[<span class="string">&quot;macro&quot;</span>],</span><br><span class="line">         label=<span class="string">&#x27;macro-average ROC curve (area = &#123;0:0.2f&#125;)&#x27;</span></span><br><span class="line">               <span class="string">&#x27;&#x27;</span>.<span class="built_in">format</span>(roc_auc[<span class="string">&quot;macro&quot;</span>]),</span><br><span class="line">         color=<span class="string">&#x27;navy&#x27;</span>, linestyle=<span class="string">&#x27;:&#x27;</span>, linewidth=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">colors = cycle([<span class="string">&#x27;aqua&#x27;</span>, <span class="string">&#x27;darkorange&#x27;</span>, <span class="string">&#x27;cornflowerblue&#x27;</span>])</span><br><span class="line"><span class="keyword">for</span> i, color <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(nb_classes), colors):</span><br><span class="line">    plt.plot(fpr[i], tpr[i], color=color, lw=lw,</span><br><span class="line">             label=<span class="string">&#x27;ROC curve of class &#123;0&#125; (area = &#123;1:0.2f&#125;)&#x27;</span></span><br><span class="line">             <span class="string">&#x27;&#x27;</span>.<span class="built_in">format</span>(i, roc_auc[i]))</span><br><span class="line"></span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">&#x27;k--&#x27;</span>, lw=lw)</span><br><span class="line">plt.xlim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.ylim([<span class="number">0.0</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;False Positive Rate&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;True Positive Rate&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Some extension of Receiver operating characteristic to multi-class&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;lower right&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>Hereâ€™s the result:</p><center><img src="/eysblog_en/imgsource/covid-figure-11.png" alt="" width="80%"/></center><center><img src="/eysblog_en/imgsource/covid-figure-12.png" alt="" width="80%"/></center><p>The reason for the coincidence of the ROC curve and the X-axis is that most of the predictions are zero. The reasons are as follows: </p><ol><li>The two embedding methods, <code>BOW</code> and <code>TF-IDF</code>, do not work well as <code>SVM</code>, and even after normalizing the input word (frequency) vector matrix, most of the predictions are still the same. </li><li>The number of â€œneutralâ€ labels in the sample is much higher than the number of â€œpositiveâ€ and â€œnegativeâ€ labels, which is also a problem in the sample selection process.</li><li>The parameters of <code>SVM</code> can be adjusted more precisely to make the classification better.</li></ol><p>Instead of further tuning this model, we tried other algorithms first.</p><h2 id="TF-IDF-SVM"><a href="#TF-IDF-SVM" class="headerlink" title="TF-IDF+SVM"></a><code>TF-IDF</code>+<code>SVM</code></h2><p>The 38473-dimensional embedding of the <code>TF-IDF</code> derived posts is used as the input to the <code>SVM</code> in the sklearn package.</p><p>The source code is similar to the model above, so I will not repeat it here. The results are shown below.</p><center><img src="/eysblog_en/imgsource/covid-figure-13.png" alt="" width="80%"/></center><p>The parameters are as follows.<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf = svm.SVC(C=<span class="number">0.8</span>, kernel=<span class="string">&#x27;rbf&#x27;</span>, gamma=<span class="number">20</span>, decision_function_shape=<span class="string">&#x27;ovr&#x27;</span>)</span><br></pre></td></tr></table></figure></p><h2 id="BOW-Decision-Tree-amp-amp-TF-IDF-Decision-Tree"><a href="#BOW-Decision-Tree-amp-amp-TF-IDF-Decision-Tree" class="headerlink" title="BOW + Decision Tree &amp;&amp; TF-IDF + Decision Tree"></a><code>BOW</code> + <code>Decision Tree</code> &amp;&amp; <code>TF-IDF</code> + <code>Decision Tree</code></h2><p>The source code is similar to the model above, so I will not repeat it here.</p><p>Hereâ€™s the result for <code>BOW</code> + <code>Decision Tree</code>.</p><center><img src="/eysblog_en/imgsource/covid-figure-14.png" alt="" width="80%"/></center><p>Hereâ€™s the result for <code>TF-IDF</code> + <code>Decision Tree</code></p><center><img src="/eysblog_en/imgsource/covid-figure-15.png" alt="" width="80%"/></center><h2 id="Word2Vec-RNN"><a href="#Word2Vec-RNN" class="headerlink" title="Word2Vec + RNN"></a><code>Word2Vec</code> + <code>RNN</code></h2><p>Embedding Weibo content using Word2Vec, then the resulting 400-dimensional vector is fed into a 10<em>200</em>1 recurrent neural grid with one hidden layer.</p><p>Parameters:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">100</span> </span><br><span class="line">n_iters = <span class="number">20000</span> </span><br><span class="line">seq_dim = <span class="number">20</span> </span><br><span class="line">input_dim = <span class="number">20</span> <span class="comment"># input dimension</span></span><br><span class="line">hidden_dim = <span class="number">200</span> <span class="comment"># hidden layer dimension </span></span><br><span class="line">layer_dim = <span class="number">1</span> <span class="comment"># number of hidden layers</span></span><br><span class="line">output_dim = <span class="number">3</span> <span class="comment"># output dimension</span></span><br></pre></td></tr></table></figure></p><p>So far we have obtained the embedding of all words, the key problem is how to represent the sentences. I referred to the information below and chose to try it with <code>word2vec</code> using Gensim.</p><p>ref1: <a href="https://www.zhihu.com/question/29978268">https://www.zhihu.com/question/29978268</a></p><p>ref2: <a href="https://blog.csdn.net/John_xyz/article/details/79424284">https://blog.csdn.net/John_xyz/article/details/79424284</a></p><p>ref3: <a href="https://www.kaggle.com/kanncaa1/recurrent-neural-network-with-pytorch">https://www.kaggle.com/kanncaa1/recurrent-neural-network-with-pytorch</a></p><p>At the first time we try, there is exploding gradient.</p><center><img src="/eysblog_en/imgsource/covid-figure-16.png" alt="" width="80%"/></center><p>After adjusting the learning rate to 0.03, the results after 60,000 generations of training are shown below.</p><center><img src="/eysblog_en/imgsource/covid-figure-17.png" alt="" width="80%"/></center><center><img src="/eysblog_en/imgsource/covid-figure-18.png" alt="" width="80%"/></center><p>After adjusting hidden layers, the results after 20,000 generations are shown below.</p><center><img src="/eysblog_en/imgsource/covid-figure-19.png" alt="" width="80%"/></center><center><img src="/eysblog_en/imgsource/covid-figure-20.png" alt="" width="80%"/></center><h1 id="Analysis-of-Results"><a href="#Analysis-of-Results" class="headerlink" title="Analysis of Results"></a>Analysis of Results</h1><p>This is a triple classification problem on the emotion of NLP. The results are shown as follow.</p><center><img src="/eysblog_en/imgsource/covid-figure-21.png" alt="" width="80%"/></center><p>As can be seen from the above graphs, the <code>SVM</code> and <code>Decision Tree</code> algorithms have very little impact on the actual results, and the most important factor affecting the prediction results is the Embedding method. In this dataset, <code>TF-IDF</code> is more effective than <code>BOW</code>. In the end, the results of <code>TF-IDF</code>+<code>SVM</code>, <code>TF-IDF</code>+<code>Decision Tree</code> and <code>Word2Vec</code>+<code>RNN</code> are similar. The reasons for this result are as follows: </p><ol><li>The original dataset is not evenly distributed, and there are more â€œneutralâ€ comments than â€œpositiveâ€ and â€œnegativeâ€ posts, so in the predictive classification process, most of the postss are not evenly distributed. The majority of posts tend to be classified as â€œneutralâ€ in the prediction classification process, which is of course consistent with the actual situation. This is why the Embedding method has a greater impact on the results than the classification method.</li><li>The dataset is not large enough. I thought that a data set of about 10,000 would take a lot of training time, but Kaggle can use GPUs and the CPU speed of Kaggle is not slow, so I could have done it directly with the original data set of 10W, and the result would be better.</li><li>Parameter optimization. It is only fair to use the optimal parameters of each model for comparison of results.</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;The emergence of the COVID-19 has disrupted our normal life. Peopleâ€™s psychological state can also receive a great negative impact. We ch</summary>
      
    
    
    
    <category term="NLP (Natural Language Processing)" scheme="https://enblog.crocodilezs.top/categories/NLP-Natural-Language-Processing/"/>
    
    
  </entry>
  
  <entry>
    <title>[MCM 2020] Data Analysis for Sunshine Company&#39;s New Product</title>
    <link href="https://enblog.crocodilezs.top/202003/2020-03-10-MCM-Grocery/"/>
    <id>https://enblog.crocodilezs.top/202003/2020-03-10-MCM-Grocery/</id>
    <published>2020-03-10T02:24:24.000Z</published>
    <updated>2021-11-19T14:11:08.172Z</updated>
    
    <content type="html"><![CDATA[<p>This is my second time for MCM &amp; ICM. I teamed up with two friends respectively from computer science major and electronic information major. This is how we divide the work: one of my friends collecting information and the other writing the essay. I was responsible for modeling and coding. We finally got <strong>Meritorious Winner(Top 5%)</strong>. Iâ€™m so proud of it. This is our entry experience.</p><h1 id="Topic-amp-Dataset"><a href="#Topic-amp-Dataset" class="headerlink" title="Topic &amp; Dataset"></a>Topic &amp; Dataset</h1><blockquote><p>In the online marketplace it created, Amazon provides customers with an opportunity to rate and review purchases. Individual ratings - called â€œstar ratingsâ€ - allow purchasers to express their level of satisfaction with a product using a scale of 1 (low rated, low satisfaction) to 5 (highly rated, high satisfaction). Additionally, customers can submit text-based messages - called â€œreviewsâ€ - that express further opinions and information about the product. Other customers can submit ratings on these reviews as being helpful or not - called a â€œhelpfulness ratingâ€ - towards assisting their own product purchasing decision. Companies use these data to gain insights into the markets in which they participate, the timing of that participation, and the potential success of product design feature choices.</p><p>Sunshine Company is planning to introduce and sell three new products in the online marketplace: a microwave oven, a baby pacifier, and a hair dryer. They have hired your team as consultants to identify key patterns, relationships, measures, and parameters in past customer-supplied ratings and reviews associated with other competing products to 1) inform their online sales strategy and 2) identify potentially important design features that would enhance product desirability. Sunshine Company has used data to inform sales strategies in the past, but they have not previously used this particular combination and type of data. Of particular interest to Sunshine Companyare time-based patterns in these data, and whether they interact in ways that will help the company craft successful products.</p><p>To assist you, Sunshineâ€™s data center has provided you with three data files for this project: hair_dryer.tsv, microwave.tsv, and pacifier.tsv. These data represent customer-supplied ratings and reviews for microwave ovens, baby pacifiers, and hair dryers sold in the Amazon marketplace over the time period(s) indicated in the data. A glossary of data label definitions is provided as well. </p></blockquote><p>The three data sets provided contain product user ratings and reviews extracted from the Amazon Customer Reviews Dataset thru Amazon Simple Storage Service (Amazon S3).</p><ul><li><code>hair_dryer.tsv</code></li><li><code>microwave.tsv</code></li><li><code>pacifier.tsv</code></li></ul><p>Data Set Definitions: Each row represents data partitioned into the following columns.</p><pre><code>- marketplace (string): 2 letter country code of the marketplace where the review was written.- customer_id (string): Random identifier that can be used to aggregate reviews written by a single author.- review_id (string): The unique ID of the review.- product_id (string): The unique Product ID the review pertains to.- product_parent (string): Random identifier that can be used to aggregate reviews for the same product.- product_title (string): Title of the product.- product_category (string): The major consumer category for the product.- star_rating (int): The 1-5 star rating of the review.- helpful_votes (int): Number of helpful votes.- total_votes (int): Number of total votes the review received.- vine (string): Customers are invited to become Amazon Vine Voices based on the trust that they have earned in the Amazon community for writing accurate and insightful reviews. Amazon provides Amazon Vine members with free copies of products that have been submitted to the program by vendors. Amazon doesn&#39;t influence the opinions of Amazon Vine members, nor do they modify or edit reviews.- verified_purchase (string): A &quot;Y&quot; indicates Amazon verified that the person writing the review purchased the product at - Amazon and didn&#39;t receive the product at a deep discount.- review_headline (string): The title of the review.- review_body (string): The review text.- review_date (bigint): The date the review was written.</code></pre><h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>Through data mining and modeling, we analyze the three types of products and provide a marketing strategy for Sunshine Company.</p><p>First of all, we preprocess the data. We fully analyze the 15 attributes in microwaves, pacifiers and hair dryers, filter them and syntheze three new attributes: review_text, popularity and reputation. We deal with missing values in the data set and remove those meaningless reviews. Through the Tokenization algorithm, we cut the sentence of the review headline and the review body into single words, which is convenient for us to analyze the emotion of the user and understand the relationship between reviews and star ratings. </p><p>Furthermore, we use the TF-IDF(Term FrequencyInverse Document Frequency) algorithm and MLP(Multi-layer Perceptron) to build a model and try to discover the relationship between reviews and star ratings. We use reviews to evaluate the products and use helpful votes to evaluate reviews, through this way we build a multidimension evaluation system. The TF-IDF algorithm uses the segmented words to generate a word frequency matrix, which is used as the input of the MLP. And the productsâ€™ star ratings are used as the output. We build models for the three types of products respectively, for the reviews of the three types of products have some differences, which may affect the accuracy of the results. The data set is divided into training set and test set. When the model training is completed, the effect of the model is tested with the test set. After the practice, our prediction is very similar to the ground truth. Then we get the relationship between star ratings and reviews. The traditional MLP model has poor interpretability, so we continue to do lexical analysis and syntax analysis on this basis, which enhances the interpretability of the whole model.</p><p>Eventually , we analyze the time-based popularity changes of the product, and make marketing suggestions to the Sunshine Company from the customerâ€™s perspective. Through analysis, we found that the productsâ€™ popularity always reach the highest at the beginning and middle of each year, so the company could make promotional activities at that time to raise their profile. In this way, we found the relationship between period and popularity. From the perspective of customers, we observe that those users who score 5 stars or 1 star often quite emotional when giving reviews. We recommend three types of the most valuable users and select their reviews for the sunshine company. They are AmazonVine Voices Members, users who have purchased multiple similar products and users who have given one-star rate to the product. By analyzing the WordCloud of reviews of those who have given one-star rates, we can know how to improve the products.</p><h1 id="Preparation-for-the-modeling"><a href="#Preparation-for-the-modeling" class="headerlink" title="Preparation for the modeling"></a>Preparation for the modeling</h1><p>For the data mining problems that have large quantity and types of data, there are often a large number of default values, which may affect the efficiency and accuracy of the model. Therefore, the processing of these default values is of vital importance. In addition, in this data set, there is a large amount of text information in attributes such as prouct_title, and review_headline. Thus our team choose the tokenization algorithm to classify and organize the text information.</p><h2 id="Default-Value-Preprocessing"><a href="#Default-Value-Preprocessing" class="headerlink" title="Default Value Preprocessing"></a>Default Value Preprocessing</h2><p>Through our analysis, in all of the given data sets, only the review_headline and review_body attributes are default. The Amazon website stipulates that when a review is submitted, its star_rating, review_headline and review_body must be filled in, otherwise the review cannot be submitted. so the lack of data is not caused by user behavior, but created during the collection, transmission or storage of these reviews. Therefore the â€œdefaultâ€ here doesnt contain customers opinions towards the product.<br>In this case our team deal with the default records in this way.</p><ul><li>When only one of the review_headline or review_body is default, we will keep this data. Because it still contains a large amount of information;</li><li>When both of the review_headline and review_body are default, We will abandon this data. Because review is an essential part of our following analysis, in this case when both of them are default, this data is of little significance for data<br>mining.</li></ul><h2 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h2><p>The main goal of Tokenization is to normalize our texts, that is, to process a paragraph of text into short, non-redundant text information. Our tokenization process<br>includes the following steps:</p><p>a. Lowercase the values.<br>Because the text_market, product_title and other text attributes have a kind of phenomenon that some values with the same meaning have different capitalization. For example, the marketplace attribute contains â€™USâ€™ and â€™usâ€™, which is not convenient for our subsequent classification of products. Therefore we convert all text to lower<br>case.<br>b. Break the sentences into token<br>c. Remove punctuation and stop words<br>In computing, stop words are words which are filtered out before or after processing of natural language data (text).</p><p>We give an example in Figure 1.</p><center><img src="/eysblog_en/imgsource/mcm2020-figure-1.png" alt="" width="100%"/><p><font size=3 color="black">Figure 1: An example about Tokenization in Microwave Product Data Set</font></p></center><h2 id="Data-Selection-and-Synthesis"><a href="#Data-Selection-and-Synthesis" class="headerlink" title="Data Selection and Synthesis"></a>Data Selection and Synthesis</h2><p>In these 15 attributes that come from the provided data sets, some of them are not valuable. So we need to select the relatively more essential attributes. For the marketplace attribute, all evaluations in the dataset are from the US, so there is no reason to do data mining on this attribute, similarly in product_cetegory. Review_id is only used to distinguish different reviews, so we delete their corresponding data as well.</p><p>Before introducing product_id and product_parent, we need to learn more about parent-child relationships. </p><p>Each parent product may contain multiple child products, and each child parents may be different in sizes, colors, or prices. Each parent product corresponds to a product_parent, and each child product corresponds to a product_id. In the following data analysis, we mainly analyze the parent product and inspect these three types of products from a macro level. Take the microwave as an example, the parent-child product relationship as shown below. </p><p>In Figure 2 , we can find the four child products have different product_ids and they may have different sizes, colors or prices. However, they have the same product_parent. Please notice that we donâ€™t know the specific information about these four child products.Their colors and sizes in Figure 2 are just for example. </p><p>We choose these following basic attributes in Table 1 to create the Customer Profile. Additionally, some synthetic variables are used in our model. These variables are described in Table 2.</p><center><img src="/eysblog_en/imgsource/mcm2020-figure-2.png" alt="" width="70%"/><p><font size=3 color="black">Figure 2: The Parent-Child Relationship in Microwave Products</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-table-1.png" alt="" width="100%"/></center><center><img src="/eysblog_en/imgsource/mcm2020-table-2.png" alt="" width="100%"/></center><br /># Competing Products AnalysisFirst we visualize the data sets to show the profiles of the three products. The number of reviews and parent products are shown in Table 3. Here is the products profile of microwave, the other two products are attached to the appendix. Figure 3 shows the star ratings and the number of reviews of every parent products. The horizontal axis reprensents the product titles. For Microwaves, we select the product that has the most reviews - "Danby 0.7 cu.ft. Countertop Microwave" to analyze its reviews. Before we create the wordcloud of review_text, we take stemming operations. Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root formgenerally a written word form. In Figure 5, High frequency words has bigger font size. In Figure 6, horizontal axis represents the review headline. In addition to the above analysis from the perspective of the product itself, we also try to delete the click farmers or construct Customer Profile from the userâ€™s perspective. Through the Customer Profile, we can have a better understanding toward the interests of the target users, and thus we could provide better product services.We observe that a large number of users have bought more than one products. So we check out several reviews and found that many of them are click farmers, and their invalid reviews cannot be checked out through the helpful_votes, so eventually we manually deleted these reviews. For others who have really bought many products, we think their opinions are as valuable as those of Amazon Vine Voices Members. Therefore we filtered out 53 reviews from 9 users who have purchased more than 5 products and sent these comments to Sunshine Company for their reference. These 9 user comments are detailed in the appendix.<center><img src="/eysblog_en/imgsource/mcm2020-table-3.png" alt="" width="100%"/></center><br /><center><img src="/eysblog_en/imgsource/mcm2020-figure-3.png" alt="" width="80%"/><p><font size=3 color="black">Figure 3: The Star Ratings and the Number of Reviews of Microwave</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-figure-4.png" alt="" width="70%"/><p><font size=3 color="black">Figure 4: The proportion of People Who Were in Amazon Vine and People Who Had Bought the Product</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-figure-5.png" alt="" width="70%"/><p><font size=3 color="black">Figure 5: The Wordcloud of Microwave</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-figure-6.png" alt="" width="70%"/><p><font size=3 color="black">Figure 6: The Reviews' Total Votes and Helpful Votes of Danby 0.7 cu.ft. Countertop Microwave</font></p></center><h1 id="Model-Construction"><a href="#Model-Construction" class="headerlink" title="Model Construction"></a>Model Construction</h1><p>In the previous data preprocessing, we have processed a series of reviews_text through the Tokenization algorithm. Next, we will process the data through the TF-IDF (Term FrequencyInverse Document Frequency) algorithm and the MLP (Multi-Layer Perception) model to get the relationship between star_rating and review. </p><p>The keywords in different product reviews are quite different and will affect each other, so we build three prediction models for the products: microwave, pacifier, and hair dryer. We first use the TF-IDF algorithm to obtain the term frequency matrix. TF-IDF is a commonly used weighting technique for information retrieval and data mining. The algorithm can simply and efficiently find keywords in articles.</p><h2 id="Compute-the-TF-IDF-Matrix"><a href="#Compute-the-TF-IDF-Matrix" class="headerlink" title="Compute the TF-IDF Matrix"></a>Compute the TF-IDF Matrix</h2><p>TF-IDF consists of two parts, one is â€œTerm Frequencyâ€ (abbreviated as TF), and the other is â€œInverse Document Frequencyâ€ (abbreviated as IDF). </p><p>TF part will create a Term Frequency Matrix. We use the review_text in Figure 1 to<br>illustrate the process. The procession is shown in Figure 7.</p><center><img src="/eysblog_en/imgsource/mcm2020-figure-7.png" alt="" width="80%"/><p><font size=3 color="black">Figure 7: TF Procession</font></p></center><script type="math/tex; mode=display">f_i=\frac{c_i}{\sum^s_{i=1}c_i}\tag{1}</script><p>In Equation 1, \(f_i\) is the frequency of the \(i\)th word. \(c_i\) is the occurrence number in a review_text of the ith word. s is the amount of words in the review_text. we find out that in Figure 7 the word frequencies of â€™lookâ€™, â€™goodâ€™ and â€™longâ€™ are the highest. However, this does not mean these three words are equally important in the analysis process. We need to multiply this word frequency matrix by IDF (Inverse Document Frequency) to get the TF-IDF values of these words.</p><p>In Equation 2, \(d_i\) is the IDF value of the ith word. \(m\) is total number of articles in Corpus. \(n_i\) is the number of articles containing the ith word in Corpus. \(v_i\) is the TF-IDF value of the \(i\)th word.</p><script type="math/tex; mode=display">d_i=log \left( \frac{m}{n_i+1} \right)\tag{2}</script><script type="math/tex; mode=display">v_i=f_i*d_i\tag{3}</script><p>We give Figure 8 as an example for the TF-IDF values. After that, we will construct the TF-IDF matrix of the review based on the TF-IDF values of these words corresponding dictionaries. It is worth mentioning that we apply helpful<em>votes as a reference to the TF-IDF matrix. In Equation 4, \(M\) is the TF-IDF matrix for the product. The \(k\)th reviewâ€™s \(i\)th wordâ€™s TF-IDF value is \(v</em>{ki}\).</p><script type="math/tex; mode=display">M_{ki}=v_{ki}\tag{4}</script><center><img src="/eysblog_en/imgsource/mcm2020-figure-8.png" alt="" width="80%"/><p><font size=3 color="black">Figure 8: TF-IDF Values</font></p></center><h2 id="Contrust-the-MLP-Model"><a href="#Contrust-the-MLP-Model" class="headerlink" title="Contrust the MLP Model"></a>Contrust the MLP Model</h2><p>We divide three data sets into training sets and test sets by a ratio of 3:1 respectively. Then we use the input and output of the training set to train MLP(Multi-Layer Perception). The core formulas of the MLP are show below.</p><p>Multi-Layer Perception made up of a lot of artificial neurons. The artificial neuron uses a nonlinear activation function to output an activity value.The training process of MLP can be divided into the following three steps:</p><ol><li>Calculate the state and activation value of each layer, until the last layer;</li><li>Calculate the error of each layer by backward propagation;</li><li>Calculate the partial derivative of each layerâ€™s parameters, and update the parameters.</li></ol><p>Suppose neurons accept our TF-IDF Matrix \(X = (x1, x2, . . . , xn)\). State \(z\) is used to represent the weighted sum of input signal \(x\) obtained by a neuron, and the output is the activity value a of the neuron, which is defined as follows:</p><script type="math/tex; mode=display">z=W^TX+b\tag{5}</script><script type="math/tex; mode=display">a=f(z)\tag{6}</script><script type="math/tex; mode=display">z^{(l)}=W^{(l)} \cdot f_{l}\left(z^{(l-1)}\right)+b^{(l)}\tag{7}</script><p>In the process of forward propagation, we use notations as folloowing Table 4.</p><center><img src="/eysblog_en/imgsource/mcm2020-table-4.png" alt="" width="100%"/></center><br />$$X=a^{(0)} \rightarrow z^{(1)} \rightarrow a^{(1)} \rightarrow z^{(2)} \rightarrow \cdots \rightarrow a^{(L-1)} \rightarrow z^{(L)} \rightarrow a^{(L)}=y\tag{8}$$In the process of back propagation, given a set of samples \\((X(i), y(i)), 1 â‰¤ i â‰¤ N\\), the output of feedforward neural network whose objective function is \\(f(X(i)|W, b)\\) is as follows.$$\begin{aligned}J(W, b) &=\sum_{i=1}^{N} L\left(y^{(i)}, f\left(X^{(i)} \mid W, b\right)\right)+\frac{1}{2} \lambda\|W\|_{F}^{2} \\&=\sum_{i=1}^{N} J\left(W, b ; X(i), y^{(i)}\right)+\frac{1}{2} \lambda\|W\|_{F}^{2}\end{aligned}\tag{9}$$$$\|W\|_{F}^{2}=\sum_{l=1}^{L} \sum_{j=1}^{n^{l+1}} \sum_{1=1}^{n^{l}} W_{i j}^{(l)}\tag{10}$$We use gradient descent to minimize the Equation 9, finally we get \\(l\\)th layer's error in Equation 11.$$\begin{aligned}\delta^{(l)} & \triangleq \frac{J(W, b ; X, y)}{\partial z^{(l)}} \\&=\frac{\partial a^{(l)}}{\partial z^{(l)}} \cdot \frac{\partial z^{(l+1)}}{\partial a^{(l)}} \cdot \frac{J(W, b ; X, y)}{\partial z^{(l+1)}} \\&=\operatorname{diag}\left(f_{l}\left(z^{(l)}\right)\right) \cdot\left(W^{(l+1)}\right)^{T} \cdot \delta^{(l+1)} \\&=f_{l}\left(z^{(l)}\right) \odot\left(\left(W^{(l+1)}\right)^{T} \delta^{(l+1)}\right)\end{aligned}\tag{11}$$Then we continue to iterate and get an error-stable model. Finally, we use the trained model to predict the test data. We set the fitting figure of Microwave in three products as Figure 9.<center><img src="/eysblog_en/imgsource/mcm2020-figure-9.png" alt="" width="80%"/><p><font size=3 color="black">Figure 9: The fitting Figure of Hair Dryer</font></p></center><h2 id="Time-Based-Analysis"><a href="#Time-Based-Analysis" class="headerlink" title="Time-Based Analysis"></a>Time-Based Analysis</h2><p>Product reputation is composed of star rating and popularity. Product quality is the determing factor of star rating and review content. We will analyze popularity based on time to provide suggestions for sunshine company.</p><p>We analyze the 8 products with the highest popularity, and observed their popularity changes over time. It can be seen from the Figure 10 that five of the eight products have peak popularity only at the beginning of each year, and the other three products will have peak popularity at the beginning and middle of each year. There are three reasons for this:</p><p>a. At the beginning of the year, itâ€™s just the time for people to purchase in the new year. Peopleâ€™s desire for shopping is strong, while microwave, pacifier and hair dryer are durable products, which can be used for a long time. No one will buy twice in a short time.</p><p>b. Amazon or merchants have promotional activities at the beginning of the year to improve product popularity.</p><p>c. The data retrieving process is not objective, we only get the data at a certain period.</p><center><img src="/eysblog_en/imgsource/mcm2020-figure-10.png" alt="" width="80%"/><p><font size=3 color="black">Figure 10: The Number of Reviews of the 10 Most Popularity Products</font></p></center><p>A products reputation consists the productâ€™s praise and popularity, and the decisive factor that influences a products star ratings and the content of its reviews is its quality. In the following discussion we will offer a proposal for Sunshine Company from the perspective of the products popularity. </p><p>We analyze the top eight popular products and observe their popularity changes over time. To our surprise, 5 of these 8 products have popularity peaks only at the beginning of each year, and the remaining three products have popularity peaks at the beginning and middle of the year. There why This phenomenon happens are as follows:</p><ol><li>In the beginning of the year, everyone is buying goods for the coming of new year. So peopleâ€™s desire for shopping is strong. On the other hand, the microwave, pacifier and hair dryer are all durable commodities, which can be used for a long time. So there will be no repurchase in a short time. Therefore, there will be peaks of popularity in only particular times.</li><li>Amazon or sellers have promotional campaign at the beginning of the year in order to increase the products popularity.</li><li>The data collection process is not objective. Only data of the beginning and middle of the year were collected.</li></ol><h1 id="Sales-Strategy"><a href="#Sales-Strategy" class="headerlink" title="Sales Strategy"></a>Sales Strategy</h1><p>Our goal is to allow Sunshine Company to expand market influence, continuously improve products and win a good corporate reputation among the public. We will make recommendations for Sunshine Company from two perspectives: How to Enhance the Reputation and How to Improve the Product.</p><h2 id="How-to-Enhance-the-Reputation"><a href="#How-to-Enhance-the-Reputation" class="headerlink" title="How to Enhance the Reputation"></a>How to Enhance the Reputation</h2><p>Reputation consists of two parts: popularity and star rating. First of all, from the perspective of popularity, we hope more people could pay attention to the companyâ€™s products. The most direct way to do this is to choose the right time to advertise and promote this product. we recommended that Sunshine Company are supposed to enhance propaganda and increase the discount at the beginning and middle of each year to increase product sales and popularity.</p><h2 id="How-to-Improve-the-Product"><a href="#How-to-Improve-the-Product" class="headerlink" title="How to Improve the Product"></a>How to Improve the Product</h2><p>On the other hand, for Star rating, the most fundamental influence of it is the product itself. By analyzing the content of the review, we could comprehend the relationship between star rating and review, furthermore, we could improve the product through the content review. Therefore, analyzing the content of the review is of vital importance. </p><p>From the previous model, we believe that there are three types of customers that worth pay attention to. Their reviews and attitude towards the product can effectively help us to improve the product.</p><ul><li>Amazon Vine Voices Members</li></ul><p>These people are selected by Amazon. They are the most trusted reviewers on Amazon to post opinions about new and pre-release items to help their fellow<br>customers make informed purchase decisions.</p><ul><li>People who have bought a lot of different products.</li></ul><p>These people have experienced more similar products than common customers, and they have a deeper understanding of this type of product. We have selected<br>nine such customers for Sunshine Company, their reviews are in the appendix.</p><ul><li>One-star rating customer</li></ul><p>our products or services may not meet the needs of all customers, so some of these customers give a one-star rating. Compared with five-star users who have very low-information reviews, these one-star rating users often directly mention the shortcomings of the product. So It is often helpful to look at these reviews<br>carefully.</p><p>we give the word cloud of one-star rating user of the microwave reviews below. There are words such as â€œserviceâ€ and â€œwarrantyâ€ in the word cloud. So we conclude that these users are dissatisfied with the product services and product warranty of the<br>microwave. In this way we can make specfic measures toward this issue. The word cloud of one-star rating users reviews by Pacifier and hair dryer is in the appendix. At the same time, we can find that users who give one star and users who give five stars tend to have more emotional reviews.</p><h1 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h1><p>Sunshine Company want to launch three new products in online marketplacemicrowave, pacifier and hair dryer. And they hire our team to help them to analyze the relevant products in the current market and make suggestions for their product sales.</p><p>At first,we process the dataset.We delete some data with missing values.At the same time,we find some data of click farmers.We also remove these useless data.We screen and integrate 15 attributes of the data set,leaving 11 basic attributes and 3 synthetic attributes.For a large number of texts in the review,we use the tokenization algorithm to cut the complex texts into simple words for our subsequent ananlysis.</p><p>Then,we analyze and visualize the attributes of the data set.In this way,we have a full understanding of the whole data set. We use the multi-demension evaluation system of â€œreviews evaluation productsâ€ and â€œhelpful votes evaluation reviewsâ€, through TF-IDF technology and MLP model to analyze the relationship between star rating, review and helpful votes,to help sunshine company to have a deeper understanding of products in the market.</p><p>Finally,on the basis of time ,we analyze the product heat,get a deeper understanding of customer review ,and provide a way for sunshine company to increase its productsâ€™popularity.</p><h1 id="Strenghths-and-Weaknesses"><a href="#Strenghths-and-Weaknesses" class="headerlink" title="Strenghths and Weaknesses"></a>Strenghths and Weaknesses</h1><h2 id="Strengths"><a href="#Strengths" class="headerlink" title="Strengths"></a>Strengths</h2><ul><li>Our model has strong interpretability</li></ul><p>The traditional MLP model has poor interpretability,and we continue to do lexical analysis and grammatical analysis on this basis, which enhances the interpretability of the whole model.</p><ul><li>We look at the product from the customerâ€™s perspective</li></ul><p>We analyze the userâ€™s emotion and build the userâ€™s portrait through the userâ€™s comments. At the same time, we find some customers who are meaningful to sunshine company.</p><h2 id="Weaknesses"><a href="#Weaknesses" class="headerlink" title="Weaknesses"></a>Weaknesses</h2><ul><li>There is a lack of analysis at the time level.</li></ul><p>At the time level, we only analyze the changes of product popularity, and there are a lot of contents that can be mined between time and review.</p><ul><li>We donâ€™t build relationship among the three products.</li></ul><p>If our model can fully break the sales relationship among the three products in the market, it will be more helpful for sunshine company which sells the microwave, pacifier and hair dryer at the same time.</p><h1 id="Appendix-Product-Profile"><a href="#Appendix-Product-Profile" class="headerlink" title="Appendix: Product Profile"></a>Appendix: Product Profile</h1><center><img src="/eysblog_en/imgsource/mcm2020-figure-11.png" alt="" width="70%"/><p><font size=3 color="black">Figure 11: The Star Ratings and the Number of Reviews of Pacifier</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-figure-12.png" alt="" width="70%"/><p><font size=3 color="black">Figure 12: The Proportion of People Who Were in Amazon Vine and People Who Had Bought the Product</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-figure-13.png" alt="" width="70%"/><p><font size=3 color="black">Figure 13: The Wordcloud of Pacifier</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-figure-14.png" alt="" width="70%"/><p><font size=3 color="black">Figure 14: The Reviewsâ€™ Total Votes and Helpful Votes of a Pacifier Product</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-figure-15.png" alt="" width="70%"/><p><font size=3 color="black">Figure 15: The Star Ratings and the Number of Reviews of Pacifier</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-figure-16.png" alt="" width="70%"/><p><font size=3 color="black">Figure 16: The Proportion of People Who Were in Amazon Vine and People Who Had Bought the Product</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-figure-17.png" alt="" width="70%"/><p><font size=3 color="black">Figure 17: The Wordcloud of Pacifier</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-figure-18.png" alt="" width="70%"/><p><font size=3 color="black">Figure 18: The Reviewsâ€™ Total Votes and Helpful Votes of a Pacifier Product</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-figure-19.png" alt="" width="70%"/><p><font size=3 color="black">Figure 19: The WordCloud of Reviews with One Star about Pacifier</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-figure-20.png" alt="" width="70%"/><p><font size=3 color="black">Figure 20: The WordCloud of Reviews with One Star about Hair Dryer</font></p></center>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;This is my second time for MCM &amp;amp; ICM. I teamed up with two friends respectively from computer science major and electronic informatio</summary>
      
    
    
    
    <category term="Mathematical Modeling" scheme="https://enblog.crocodilezs.top/categories/Mathematical-Modeling/"/>
    
    <category term="NLP (Natural Language Processing)" scheme="https://enblog.crocodilezs.top/categories/Mathematical-Modeling/NLP-Natural-Language-Processing/"/>
    
    
  </entry>
  
  <entry>
    <title>Price Suggestion Challenge - A ML Algorithms Survey</title>
    <link href="https://enblog.crocodilezs.top/201911/2019-11-12-price-prediction/"/>
    <id>https://enblog.crocodilezs.top/201911/2019-11-12-price-prediction/</id>
    <published>2019-11-12T08:22:10.000Z</published>
    <updated>2022-08-25T13:24:40.556Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Topic"><a href="#Topic" class="headerlink" title="Topic"></a>Topic</h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>Considering the number of products sold online, product pricing becomes more difficult at scale. Apparel has strong seasonal pricing trends and is heavily influenced by brands, while electronics prices fluctuate based on product specifications. It is a meaningful question to help merchants effectively sell their goods by making reasonable pricing based on past information.</p><h2 id="Target"><a href="#Target" class="headerlink" title="Target"></a>Target</h2><p>The product description, product category and brand information is given and combined with the product price from the training data to set the price for the new product. For example,</p><center><img src="/eysblog_en/imgsource/img-price-prediction/1-1.png" alt="" width="100%" /></center><p>Obviously Versaceâ€™s clothes should be much higher in price than Metersbonweâ€™s clothes, and in the description of the goods, you can find a slight difference between the two descriptions. </p><blockquote><p>This project aims to analyze the text information, extract the important information from the text information and derive the potential relationship with the priceã€‚ </p></blockquote><h2 id="Analysis-of-atributes"><a href="#Analysis-of-atributes" class="headerlink" title="Analysis of atributes"></a>Analysis of atributes</h2><center><img src="/eysblog_en/imgsource/img-price-prediction/1-2.png" alt="" width="100%" /></center><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><ul><li><code>train.csv</code> training dataset (Include <code>price</code>)</li><li><code>test.csv</code> test dataset (Not include <code>price</code>) ; <code>label_test.csv</code> (Corresponding to the price of the test dataset)</li><li><code>f_test.csv</code> Final measurement data set (Not include <code>price</code>)</li></ul><h2 id="Evaluation-Indicators"><a href="#Evaluation-Indicators" class="headerlink" title="Evaluation Indicators"></a>Evaluation Indicators</h2><p>We used <code>Mean Squared Logarithmic Error</code> (MSLE) to evaluate the algorithm: </p><script type="math/tex; mode=display">MSLE = \cfrac{1}{n}\sum_{i=1}^n(log(p_i+1)-log(\alpha_i+1))^2\tag{1}</script><p>Which \(n\) means the number of samples in test dataset; \(p_i\)means the predicting price of sales; \(\alpha_i\) means the real price.</p><h1 id="Data-Process"><a href="#Data-Process" class="headerlink" title="Data Process"></a>Data Process</h1><h2 id="Learning-sample-code"><a href="#Learning-sample-code" class="headerlink" title="Learning sample code"></a>Learning sample code</h2><p>The sample code given was first tried to understand the general idea of solving this problem. The main processes to solve this price prediction problem are: importing data and data exploration, data pre-processing, model construction, price prediction and measurement.</p><h3 id="import-data-and-exploration"><a href="#import-data-and-exploration" class="headerlink" title="import data and exploration"></a>import data and exploration</h3><p>Import the data and get acknowledge with it.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data = pd.read_csv(<span class="string">&#x27;../data/4/train.csv&#x27;</span>, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">&#x27;../data/4/test.csv&#x27;</span>,sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">train_data.info()</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;class &#x27;pandas.core.frame.DataFrame&#x27;&gt;</span><br><span class="line">RangeIndex: 300000 entries, 0 to 299999</span><br><span class="line">Data columns (total 8 columns):</span><br><span class="line">train_id             300000 non-null int64</span><br><span class="line">name                 300000 non-null object</span><br><span class="line">item_condition_id    300000 non-null int64</span><br><span class="line">category_name        298719 non-null object</span><br><span class="line">brand_name           171929 non-null object</span><br><span class="line">price                300000 non-null float64</span><br><span class="line">shipping             300000 non-null int64</span><br><span class="line">item_description     300000 non-null object</span><br><span class="line">dtypes: float64(1), int64(3), object(4)</span><br><span class="line">memory usage: 18.3+ MB</span><br></pre></td></tr></table></figure><h3 id="Data-Preprocess"><a href="#Data-Preprocess" class="headerlink" title="Data Preprocess"></a>Data Preprocess</h3><p>First of all, we need to remove <code>price</code> from the training data, and then remove <code>train_id</code> or <code>test_id</code> which are not useful. By looking at the data attributes above, we can see that <code>category_name</code> and <code>brand_name</code> have missing data, so the sample code is filled with <code>missing</code> directly.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">featureProcessing</span>(<span class="params">df</span>):</span><br><span class="line">    <span class="comment"># delete the data that will not be used</span></span><br><span class="line">    df = df.drop([<span class="string">&#x27;price&#x27;</span>, <span class="string">&#x27;test_id&#x27;</span>, <span class="string">&#x27;train_id&#x27;</span>], axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># deal with the missing value with a default value</span></span><br><span class="line">    df[<span class="string">&#x27;category_name&#x27;</span>] = df[<span class="string">&#x27;category_name&#x27;</span>].fillna(<span class="string">&#x27;missing&#x27;</span>).astype(<span class="built_in">str</span>)</span><br><span class="line">    df[<span class="string">&#x27;brand_name&#x27;</span>] = df[<span class="string">&#x27;brand_name&#x27;</span>].fillna(<span class="string">&#x27;missing&#x27;</span>).astype(<span class="built_in">str</span>)</span><br><span class="line">    df[<span class="string">&#x27;item_description&#x27;</span>] = df[<span class="string">&#x27;item_description&#x27;</span>].fillna(<span class="string">&#x27;No&#x27;</span>)</span><br><span class="line">    <span class="comment"># convert the data : int -&gt; str</span></span><br><span class="line">    df[<span class="string">&#x27;shipping&#x27;</span>] = df[<span class="string">&#x27;shipping&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">    df[<span class="string">&#x27;item_condition_id&#x27;</span>] = df[<span class="string">&#x27;item_condition_id&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure><h3 id="Model-Construction"><a href="#Model-Construction" class="headerlink" title="Model Construction"></a>Model Construction</h3><p>First of all, the input of the model is done and the matrix of word frequencies is generated by <code>CountVectorizer</code> and <code>TfidfVectorizer</code>. <code>Tfidf</code> is better because the number of occurrences of each word in all field clocks is taken into account and the generated word frequency matrix is weighted.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vectorizer = FeatureUnion([</span><br><span class="line">    (<span class="string">&#x27;name&#x27;</span>, CountVectorizer(ngram_range=(<span class="number">1</span>, <span class="number">2</span>), max_features=<span class="number">50000</span>, preprocessor=build_preprocessor_1(<span class="string">&#x27;name&#x27;</span>))),</span><br><span class="line">    (<span class="string">&#x27;category_name&#x27;</span>, CountVectorizer(token_pattern=<span class="string">&#x27;.+&#x27;</span>, preprocessor=build_preprocessor_1(<span class="string">&#x27;category_name&#x27;</span>))),</span><br><span class="line">    (<span class="string">&#x27;brand_name&#x27;</span>, CountVectorizer(token_pattern=<span class="string">&#x27;.+&#x27;</span>, preprocessor=build_preprocessor_1(<span class="string">&#x27;brand_name&#x27;</span>))),</span><br><span class="line">    (<span class="string">&#x27;shipping&#x27;</span>, CountVectorizer(token_pattern=<span class="string">&#x27;\d+&#x27;</span>, preprocessor=build_preprocessor_1(<span class="string">&#x27;shipping&#x27;</span>))),</span><br><span class="line">    (<span class="string">&#x27;item_condition_id&#x27;</span>, CountVectorizer(token_pattern=<span class="string">&#x27;\d+&#x27;</span>, preprocessor=build_preprocessor_1(<span class="string">&#x27;item_condition_id&#x27;</span>))),</span><br><span class="line">    (<span class="string">&#x27;item_description&#x27;</span>, TfidfVectorizer(ngram_range=(<span class="number">1</span>, <span class="number">3</span>),max_features=<span class="number">100000</span>, preprocessor=build_preprocessor_1(<span class="string">&#x27;item_description&#x27;</span>))),</span><br><span class="line">])</span><br></pre></td></tr></table></figure><p>Predict the price by Ridge Regression.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ridgeClassify</span>(<span class="params">train_data, train_label</span>):</span><br><span class="line">    ridgeClf = Ridge(</span><br><span class="line">        solver=<span class="string">&#x27;auto&#x27;</span>,</span><br><span class="line">        fit_intercept=<span class="literal">True</span>,</span><br><span class="line">        alpha=<span class="number">0.5</span>,</span><br><span class="line">        max_iter=<span class="number">500</span>,</span><br><span class="line">        normalize=<span class="literal">False</span>,</span><br><span class="line">        tol=<span class="number">0.05</span>)</span><br><span class="line">    <span class="comment"># Training</span></span><br><span class="line">    ridgeClf.fit(train_data, train_label)</span><br><span class="line">    <span class="keyword">return</span> ridgeClf</span><br></pre></td></tr></table></figure><p>By understanding the dataset and studying the sample code, we learned that there are three angles to start with to optimize the answer to this question.</p><ol><li>data preprocessing: How to handle missing values? How should the attributes be combined?</li><li>optimization when forming the word frequency matrix: adjusting the parameters of <code>CountVectorizer</code> and <code>TfidfVectorizer</code>.</li><li>Model selection and optimization: try models other than ridge regression, adjust model parameters.</li></ol><h2 id="Try-more-models"><a href="#Try-more-models" class="headerlink" title="Try more models"></a>Try more models</h2><p>In the sample code above, the result obtained using the ridge regression model is about 3.01. After the hints from the previous class and the online search, we are ready to try the <code>MLP</code> model and the <code>Lgmb</code> model again. After roughly trying both models, we decided to further optimize the model using <code>MLP</code>.</p><h3 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a><code>MLP</code></h3><p>The result of <code>MLP</code> is shown below:</p><center><img src="/eysblog_en/imgsource/img-price-prediction/2-2.png" alt="" width="100%" /></center><h3 id="LGBM"><a href="#LGBM" class="headerlink" title="LGBM"></a><code>LGBM</code></h3><p>The result of  <code>Lgbm</code> is shown below: </p><center><img src="/eysblog_en/imgsource/img-price-prediction/2-1.png" alt="" width="100%" /></center><h3 id="MLP-Combined-with-LGBM"><a href="#MLP-Combined-with-LGBM" class="headerlink" title="MLP Combined with LGBM"></a><code>MLP</code> Combined with <code>LGBM</code></h3><p><1> Preprocessing</p><ul><li>Import the data</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train = pd.read_csv(<span class="string">&#x27;data/train.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">&#x27;data/test.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line"><span class="comment"># train and test data are handled together</span></span><br><span class="line">df = pd.concat([train, test], axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><ul><li>Handling missing value</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Handling missing values</span></span><br><span class="line">   df[<span class="string">&#x27;category_name&#x27;</span>] = df[<span class="string">&#x27;category_name&#x27;</span>].fillna(<span class="string">&#x27;MISS&#x27;</span>).astype(<span class="built_in">str</span>)</span><br><span class="line">   df[<span class="string">&#x27;brand_name&#x27;</span>] = df[<span class="string">&#x27;brand_name&#x27;</span>].fillna(<span class="string">&#x27;missing&#x27;</span>).astype(<span class="built_in">str</span>)</span><br><span class="line">   df[<span class="string">&#x27;item_description&#x27;</span>] = df[<span class="string">&#x27;item_description&#x27;</span>].fillna(<span class="string">&#x27;No&#x27;</span>)</span><br><span class="line">   <span class="comment"># modifying data structure</span></span><br><span class="line">   df[<span class="string">&#x27;shipping&#x27;</span>] = df[<span class="string">&#x27;shipping&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">   df[<span class="string">&#x27;item_condition_id&#x27;</span>] = df[<span class="string">&#x27;item_condition_id&#x27;</span>].astype(<span class="built_in">str</span>)</span><br></pre></td></tr></table></figure><ul><li>Feature vectorization</li></ul><p>Use the <code>CountVectorizer</code> class in the <code>sklearn</code> library to vectorize the text features and use <code>FeatureUnion</code> for feature union.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vectorizer = FeatureUnion([</span><br><span class="line">        (<span class="string">&#x27;name&#x27;</span>, CountVectorizer(</span><br><span class="line">            ngram_range=(<span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            max_features=<span class="number">100000</span>,</span><br><span class="line">            preprocessor=build_preprocessor(<span class="string">&#x27;name&#x27;</span>))),</span><br><span class="line">        (<span class="string">&#x27;category_name&#x27;</span>, CountVectorizer(</span><br><span class="line">            token_pattern=<span class="string">&#x27;.+&#x27;</span>,</span><br><span class="line">            preprocessor=build_preprocessor(<span class="string">&#x27;category_name&#x27;</span>))),</span><br><span class="line">        (<span class="string">&#x27;brand_name&#x27;</span>, CountVectorizer(</span><br><span class="line">            token_pattern=<span class="string">&#x27;.+&#x27;</span>,</span><br><span class="line">            preprocessor=build_preprocessor(<span class="string">&#x27;brand_name&#x27;</span>))),</span><br><span class="line">        (<span class="string">&#x27;shipping&#x27;</span>, CountVectorizer(</span><br><span class="line">            token_pattern=<span class="string">&#x27;\d+&#x27;</span>,</span><br><span class="line">            preprocessor=build_preprocessor(<span class="string">&#x27;shipping&#x27;</span>))),</span><br><span class="line">        (<span class="string">&#x27;item_condition_id&#x27;</span>, CountVectorizer(</span><br><span class="line">            token_pattern=<span class="string">&#x27;\d+&#x27;</span>,</span><br><span class="line">            preprocessor=build_preprocessor(<span class="string">&#x27;item_condition_id&#x27;</span>))),</span><br><span class="line">        (<span class="string">&#x27;item_description&#x27;</span>, TfidfVectorizer(</span><br><span class="line">            ngram_range=(<span class="number">1</span>, <span class="number">3</span>),</span><br><span class="line">            max_features=<span class="number">200000</span>,</span><br><span class="line">            preprocessor=build_preprocessor(<span class="string">&#x27;item_description&#x27;</span>),</span><br><span class="line">            stop_words=<span class="string">&#x27;english&#x27;</span>)),</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure><p><2> Model Construction</p><p>The features were trained using the ridge regression model, the <code>Lgbm</code> model and the <code>mlp</code> model, respectively, and the solutions obtained in local tests were 3.01, 3.00, and 0.26, respectively.</p><ul><li>Ridge Regression</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ridge_classify</span>(<span class="params">train_data,train_label</span>):</span><br><span class="line">    <span class="comment">#model</span></span><br><span class="line">    model = Ridge(</span><br><span class="line">            solver=<span class="string">&#x27;auto&#x27;</span>,</span><br><span class="line">            fit_intercept=<span class="literal">True</span>,</span><br><span class="line">            alpha=<span class="number">0.4</span>,</span><br><span class="line">            max_iter=<span class="number">100</span>,</span><br><span class="line">            normalize=<span class="literal">False</span>,</span><br><span class="line">            tol=<span class="number">0.05</span>)</span><br><span class="line">    <span class="comment">#training</span></span><br><span class="line">    model.fit(train_data, train_label)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><ul><li><code>lgbm</code> Model</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lgbm_classify</span>(<span class="params">train_data,train_label</span>):</span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.75</span>,</span><br><span class="line">        <span class="string">&#x27;application&#x27;</span>: <span class="string">&#x27;regression&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">3</span>,</span><br><span class="line">        <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">100</span>,</span><br><span class="line">        <span class="string">&#x27;verbosity&#x27;</span>: -<span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;RMSE&#x27;</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    train_X, valid_X, train_y, valid_y = train_test_split(train_data, train_label, test_size=<span class="number">0.1</span>, random_state=<span class="number">144</span>)</span><br><span class="line">    d_train = lgb.Dataset(train_X, label=train_y)</span><br><span class="line">    d_valid = lgb.Dataset(valid_X, label=valid_y)</span><br><span class="line">    watchlist = [d_train, d_valid]</span><br><span class="line"></span><br><span class="line">    model = lgb.train(params, train_set=d_train, num_boost_round=<span class="number">2200</span>, valid_sets=watchlist, \</span><br><span class="line">                      early_stopping_rounds=<span class="number">50</span>, verbose_eval=<span class="number">100</span>)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><ul><li><code>mlp</code> Model</li></ul><p>The <code>MLP</code> model consists of two fully connected layers and a dropout layer, which is essentially a network of multiple hidden layers.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mlp_model</span>(<span class="params">train_data,train_label,row_train</span>):</span><br><span class="line">    model = Sequential()</span><br><span class="line">    <span class="comment"># fully connected layer</span></span><br><span class="line">    model.add(Dense(<span class="number">64</span>, input_shape=(row_train,), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">    <span class="comment"># DropOut layer</span></span><br><span class="line">    model.add(Dropout(<span class="number">0.4</span>))</span><br><span class="line">    <span class="comment"># fully connected layer + classifier</span></span><br><span class="line">    model.add(Dense(<span class="number">1</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;mean_squared_logarithmic_error&#x27;</span>,</span><br><span class="line">                  optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">                  metrics=[<span class="string">&#x27;accuracy&#x27;</span>]</span><br><span class="line">                  )</span><br><span class="line"></span><br><span class="line">    model.fit(train_data, train_label,</span><br><span class="line">              batch_size=<span class="number">300</span>,</span><br><span class="line">              epochs=<span class="number">1</span>,</span><br><span class="line">              )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model.predict(X_test)</span><br></pre></td></tr></table></figure><h2 id="Optimization-when-forming-word-frequency-matrix"><a href="#Optimization-when-forming-word-frequency-matrix" class="headerlink" title="Optimization when forming word frequency matrix"></a>Optimization when forming word frequency matrix</h2><p>In the sample code we tried to replace all <code>CountVectorizer</code> with <code>TdidfVectorizer</code> and then use the Ridge model for prediction, but the result is not much optimized, only up to 2.9.<br>Later, when using the <code>MLP</code>, we completely discarded the <code>CountVectorizer</code> and used only the <code>TdidfVectorizer</code>.</p><h2 id="Optimize-the-data-pre-processing-process"><a href="#Optimize-the-data-pre-processing-process" class="headerlink" title="Optimize the data pre-processing process"></a>Optimize the data pre-processing process</h2><p>The way we optimize the <code>MLP</code>, which is basically perfected above, is to <strong>try different combinations of features</strong>.</p><h3 id="Analysis-of-the-attributes"><a href="#Analysis-of-the-attributes" class="headerlink" title="Analysis of the attributes"></a>Analysis of the attributes</h3><p>First I analyzed the attributes:<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">item_condition_id    300000 non-null int64</span><br><span class="line">shipping             300000 non-null int64</span><br><span class="line"></span><br><span class="line">name                 300000 non-null object</span><br><span class="line">category_name        298719 non-null object</span><br><span class="line">brand_name           171929 non-null object</span><br><span class="line">item_description     300000 non-null object</span><br><span class="line"></span><br></pre></td></tr></table></figure><br><code>item_condition_id</code> and <code>shipping</code> are considered directly as inputs, while <code>name</code>, <code>category_name</code>, <code>brand_name</code>, <code>item_description</code> are considered for different combinations to try.</p><p>Before that, we found an example tutorial on data visualization to analyze the attributes of the data.<br>The optimal combination of inputs is obtained by observing the data.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train.head()</span><br></pre></td></tr></table></figure><center><img src="/eysblog_en/imgsource/img-price-prediction/1-3.png" alt="" width="100%" /></center><ol><li><code>price</code><br>By looking at the data after visualization we know why we have to do <code>log1p</code> on <code>price</code> to make the distribution of <code>price</code> better.</li></ol><center><img src="/eysblog_en/imgsource/img-price-prediction/1-4.png" alt="" width="100%" /></center><ol><li><code>category_name</code><br>Try to split the property into various subclasses and view the corresponding data.</li></ol><center><img src="/eysblog_en/imgsource/img-price-prediction/1-5.png" alt="" width="100%" /></center><ol><li><code>item_description</code><center><img src="/eysblog_en/imgsource/img-price-prediction/1-6.png" alt="" width="100%" /></center></li></ol><h3 id="Try-different-attributes-combination"><a href="#Try-different-attributes-combination" class="headerlink" title="Try different attributes combination"></a>Try different attributes combination</h3><ol><li>simply combine the attributes together in the sample code for text analysis, i.e. <code>name</code> + <code>item_condition_id</code> + <code>category_name</code> + <code>brand_name</code> + <code>shipping</code> + <code>item_description</code>. (6 inputs)</li><li>try <code>name</code>, <code>item_condition_id</code>, <code>shipping</code>,<code>category_name</code> + <code>item_description</code>, <code>brand_name</code>. (5 inputs)</li><li>try <code>name</code>, <code>item_condition_id</code>, <code>shipping</code>, <code>category_name</code> + <code>brand_name</code> + <code>item_description</code>. (4 inputs)</li><li>try <code>name</code>, <code>item_condition_id</code>, <code>shipping</code>, <code>name</code> + <code>category_name</code> + <code>brand_name</code> + <code>item_description</code>. (4 inputs)</li></ol><p>The results for the four combinations as input are very similar, except that combination 1 <code>MSLE</code> is around 0.4, combinations 2 and 3 are around 0.21, and combination 4 eventually runs to around 0.17. Combination 4 actually increases the weight of <code>name</code> to make the final result better.</p><h1 id="Final-source-code-and-experimental-results"><a href="#Final-source-code-and-experimental-results" class="headerlink" title="Final source code and experimental results"></a>Final source code and experimental results</h1><ol><li><p>Data preprocessing</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># data preprocessing</span></span><br><span class="line"><span class="comment"># There are 8 attributes, remove price, train_id will have no influence on the result.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_preprocess</span>(<span class="params">df</span>):</span><br><span class="line">    df[<span class="string">&#x27;name&#x27;</span>] = df[<span class="string">&#x27;name&#x27;</span>].fillna(<span class="string">&#x27;&#x27;</span>) + <span class="string">&#x27; &#x27;</span> + df[<span class="string">&#x27;brand_name&#x27;</span>].fillna(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    df[<span class="string">&#x27;text&#x27;</span>] = (df[<span class="string">&#x27;item_description&#x27;</span>].fillna(<span class="string">&#x27;&#x27;</span>) + <span class="string">&#x27; &#x27;</span> + df[<span class="string">&#x27;name&#x27;</span>] + <span class="string">&#x27; &#x27;</span> + df[<span class="string">&#x27;category_name&#x27;</span>].fillna(<span class="string">&#x27;&#x27;</span>))</span><br><span class="line">    <span class="keyword">return</span> df[[<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;text&#x27;</span>, <span class="string">&#x27;shipping&#x27;</span>, <span class="string">&#x27;item_condition_id&#x27;</span>]]</span><br></pre></td></tr></table></figure></li><li><p>Model Constructio</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fit_predict</span>(<span class="params">xs, y_train</span>):</span><br><span class="line">    X_train, X_test = xs</span><br><span class="line">    <span class="comment"># Configure the operation method of tf.Session, such as gpu operation or cpu operation</span></span><br><span class="line">    config = tf.ConfigProto(</span><br><span class="line">        <span class="comment"># Set the number of threads for multiple operations in parallel</span></span><br><span class="line">        intra_op_parallelism_threads=<span class="number">1</span>, use_per_session_threads=<span class="number">1</span>, inter_op_parallelism_threads=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># Session provides the environment for Operation execution and Tensor evaluation.</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session(graph=tf.Graph(), config=config) <span class="keyword">as</span> sess, timer(<span class="string">&#x27;fit_predict&#x27;</span>):</span><br><span class="line">        ks.backend.set_session(sess)</span><br><span class="line">        model_in = ks.Input(shape=(X_train.shape[<span class="number">1</span>],), dtype=<span class="string">&#x27;float32&#x27;</span>, sparse=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># ks.layers.Dense means the dimension of output</span></span><br><span class="line">        <span class="comment"># Dense full connected layer, equals to add one layer directly.</span></span><br><span class="line">        <span class="comment"># activation is the activation function.</span></span><br><span class="line">        out = ks.layers.Dense(<span class="number">192</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(model_in)</span><br><span class="line">        out = ks.layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(out)</span><br><span class="line">        out = ks.layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(out)</span><br><span class="line">        out = ks.layers.Dense(<span class="number">1</span>)(out)</span><br><span class="line">        model = ks.Model(model_in, out)</span><br><span class="line">        model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;mean_squared_error&#x27;</span>, optimizer=ks.optimizers.Adam(lr=<span class="number">3e-3</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            <span class="keyword">with</span> timer(<span class="string">f&#x27;epoch <span class="subst">&#123;i + <span class="number">1</span>&#125;</span>&#x27;</span>):</span><br><span class="line">                model.fit(x=X_train, y=y_train, batch_size=<span class="number">2</span> ** (<span class="number">11</span> + i), epochs=<span class="number">1</span>, verbose=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> model.predict(X_test)[:, <span class="number">0</span>]</span><br></pre></td></tr></table></figure></li><li><p>Model training and prediction</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    vectorizer = make_union(<span class="comment"># Assemble all transformers into a FeatureUnion. n_jobs means it can be done simultaneously</span></span><br><span class="line">        <span class="comment"># FunctionTransformer implements a custom transformation with no input validation when validate=False</span></span><br><span class="line">        <span class="comment"># TfidfVectorizer function, consider only the words in the first max_feature bits by word frequency, token_pattern=&#x27;\w+&#x27; matches at least one word</span></span><br><span class="line">        make_pipeline(FunctionTransformer(itemgetter(<span class="string">&#x27;name&#x27;</span>), validate=<span class="literal">False</span>), TfidfVectorizer(max_features=<span class="number">100000</span>, token_pattern=<span class="string">&#x27;\w+&#x27;</span>)),</span><br><span class="line">        make_pipeline(FunctionTransformer(itemgetter(<span class="string">&#x27;text&#x27;</span>), validate=<span class="literal">False</span>), TfidfVectorizer(max_features=<span class="number">100000</span>, token_pattern=<span class="string">&#x27;\w+&#x27;</span>)),</span><br><span class="line">        make_pipeline(FunctionTransformer(itemgetter([<span class="string">&#x27;shipping&#x27;</span>, <span class="string">&#x27;item_condition_id&#x27;</span>]), validate=<span class="literal">False</span>),</span><br><span class="line">                      FunctionTransformer(to_records, validate=<span class="literal">False</span>), DictVectorizer()),</span><br><span class="line">        n_jobs=<span class="number">4</span>)</span><br><span class="line">    <span class="comment"># StandardScaler() performs data normalization. Save the parameters (mean, variance) from the training set directly using its object to transform the test set data.</span></span><br><span class="line">    y_scaler = StandardScaler()</span><br><span class="line">    <span class="comment"># The with statement is used when accessing resources to ensure that the necessary &quot;cleanup&quot; operations are performed to release resources regardless of exceptions during use, such as automatic closure of files after use, automatic acquisition and release of locks in threads, etc.</span></span><br><span class="line">    <span class="keyword">with</span> timer(<span class="string">&#x27;process train&#x27;</span>):</span><br><span class="line">        train = pd.read_csv(<span class="string">&#x27;train.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        test = pd.read_csv(<span class="string">&#x27;test.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        <span class="comment"># remove &#x27;price&#x27;</span></span><br><span class="line">        train = train[train[<span class="string">&#x27;price&#x27;</span>] &gt; <span class="number">0</span>].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># normalization of price</span></span><br><span class="line">        y_train = y_scaler.fit_transform(np.log1p(train[<span class="string">&#x27;price&#x27;</span>].values.reshape(-<span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line">        X_train = vectorizer.fit_transform(data_preprocess(train)).astype(np.float32)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;X_train: <span class="subst">&#123;X_train.shape&#125;</span> of <span class="subst">&#123;X_train.dtype&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> timer(<span class="string">&#x27;process valid&#x27;</span>):</span><br><span class="line">        X_test = vectorizer.transform(data_preprocess(test)).astype(np.float32)</span><br><span class="line">    <span class="keyword">with</span> ThreadPool(processes=<span class="number">4</span>) <span class="keyword">as</span> pool:</span><br><span class="line">        Xb_train, Xb_test = [x.astype(np.<span class="built_in">bool</span>).astype(np.float32) <span class="keyword">for</span> x <span class="keyword">in</span> [X_train, X_test]]</span><br><span class="line">        xs = [[Xb_train, Xb_test], [X_train, X_test]] * <span class="number">2</span></span><br><span class="line">        <span class="comment"># prediction</span></span><br><span class="line">        y_pred = np.mean(pool.<span class="built_in">map</span>(partial(fit_predict, y_train=y_train), xs), axis=<span class="number">0</span>)</span><br><span class="line">    y_pred = np.expm1(y_scaler.inverse_transform(y_pred.reshape(-<span class="number">1</span>, <span class="number">1</span>))[:, <span class="number">0</span>])</span><br><span class="line">    <span class="comment"># print(type(y_pred))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Export prediction results to csv</span></span><br><span class="line">    test_id = np.array(<span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(y_pred)))</span><br><span class="line">    dataframe = pd.DataFrame(&#123;<span class="string">&#x27;test_id&#x27;</span>: test_id, <span class="string">&#x27;price&#x27;</span>: y_pred&#125;)</span><br><span class="line">    dataframe.to_csv(<span class="string">&quot;res.csv&quot;</span>, index=<span class="literal">False</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(&#x27;Valid MSLE: &#123;:.4f&#125;&#x27;.format(mean_squared_log_error(valid[&#x27;price&#x27;], y_pred)))</span></span><br></pre></td></tr></table></figure></li></ol><p>The final experimental result reached 0.179.</p><h1 id="Other-optimization-directions-in-the-MLP-model"><a href="#Other-optimization-directions-in-the-MLP-model" class="headerlink" title="Other optimization directions in the MLP model"></a>Other optimization directions in the <code>MLP</code> model</h1><ol><li>It can be observed that in the word cloud of <code>item_desciption</code>, there are words such as <code>shipping</code> and <code>free</code>, which may stand for free shipping and other meanings, and there is some duplication with the <code>shipping</code> attribute, and using it as a feature word to train the model will cause interference.</li><li>The information contained in a single keyword may not be comprehensive, and there may be great correlation between keywords.</li><li>In the final model <code>MLP</code> uses a four-layer perceptron, and the number of layers of the perceptron and the input size of each layer can be further tuned.</li></ol><h1 id="Experience"><a href="#Experience" class="headerlink" title="Experience"></a>Experience</h1><p>This experiment was very difficult and I didnâ€™t know where to start.</p><p>After carefully studying the sample code given in the course and the content of data visualization and analysis, I got a preliminary understanding of both the dataset and the method of prediction.  </p><p>Since I was very unfamiliar with models such as <code>MLP</code> and <code>Lightgbm</code>, I started from the input point of view and experimented with the combination of different attributes to get the final and better results.  </p><p>In the following study, we should learn and understand the model more deeply, and try to create the model independently, instead of modifying other models that have been written.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1].<a href="https://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html">https://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html</a></p><p>[2].  <a href="https://github.com/pjankiewicz/mercari-solution">https://github.com/pjankiewicz/mercari-solution</a></p><p>[3].<a href="https://www.kaggle.com/thykhuely/mercari-interactive-eda-topic-modelling">https://www.kaggle.com/thykhuely/mercari-interactive-eda-topic-modelling</a></p><p>[4].<a href="https://wklchris.github.io/Py3-pandas.html#ç»Ÿè®¡ä¿¡æ¯dfdescribe-svalue_counts--unique">https://wklchris.github.io/Py3-pandas.html#%E7%BB%9F%E8%AE%A1%E4%BF%A1%E6%81%AFdfdescribe-svalue_countsâ€”unique</a></p><p>[5].<a href="https://zh.wikipedia.org/wiki/å¤šå±‚æ„ŸçŸ¥å™¨">https://zh.wikipedia.org/wiki/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8</a></p><p>[6].<a href="https://blog.csdn.net/weixin_39807102/article/details/81912566">https://blog.csdn.net/weixin_39807102/article/details/81912566</a></p><p>[7].<a href="https://github.com/maiwen/NLP">https://github.com/maiwen/NLP</a></p><p>[8]. <a href="https://zh.wikipedia.org/wiki/æ­£åˆ™è¡¨è¾¾å¼">https://zh.wikipedia.org/wiki/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F</a></p><p>[9].<a href="https://blog.csdn.net/u012609509/article/details/72911564">https://blog.csdn.net/u012609509/article/details/72911564</a></p><p>[10]. <a href="https://www.kaggle.com/tunguz/more-effective-ridge-lgbm-script-lb-0-44823">https://www.kaggle.com/tunguz/more-effective-ridge-lgbm-script-lb-0-44823</a></p><p>[11]. <a href="https://qiita.com/kazuhirokomoda/items/1e9b7ebcacf264b2d814">https://qiita.com/kazuhirokomoda/items/1e9b7ebcacf264b2d814</a></p><p>[12]. <a href="https://www.jianshu.com/p/c532424541ad">https://www.jianshu.com/p/c532424541ad</a></p><p>[13]. <a href="https://www.jiqizhixin.com/articles/2017-11-13-7">https://www.jiqizhixin.com/articles/2017-11-13-7</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Topic&quot;&gt;&lt;a href=&quot;#Topic&quot; class=&quot;headerlink&quot; title=&quot;Topic&quot;&gt;&lt;/a&gt;Topic&lt;/h1&gt;&lt;h2 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; </summary>
      
    
    
    
    <category term="NLP (Natural Language Processing)" scheme="https://enblog.crocodilezs.top/categories/NLP-Natural-Language-Processing/"/>
    
    
  </entry>
  
  <entry>
    <title>Linuxå¼€å‘ç¯å¢ƒåŠåº”ç”¨ä½œä¸š1</title>
    <link href="https://enblog.crocodilezs.top/201911/Linux%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%8F%8A%E5%BA%94%E7%94%A8%E4%BD%9C%E4%B8%9A%2020191031/"/>
    <id>https://enblog.crocodilezs.top/201911/Linux%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%8F%8A%E5%BA%94%E7%94%A8%E4%BD%9C%E4%B8%9A%2020191031/</id>
    <published>2019-10-31T14:10:14.000Z</published>
    <updated>2022-05-23T17:15:29.807Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ä½œä¸šè¦æ±‚"><a href="#ä½œä¸šè¦æ±‚" class="headerlink" title="ä½œä¸šè¦æ±‚"></a>ä½œä¸šè¦æ±‚</h2><p>ä»å› ç‰¹ç½‘ä¸Šæœç´¢ç›¸å…³Webç½‘é¡µï¼Œå¤„ç†ç½‘é¡µ<code>html</code>æ•°æ®ï¼Œä»ä¸­æå–å‡ºå½“å‰æ—¶é—´ç‚¹åŒ—äº¬å„ç›‘æµ‹ç«™çš„ PM2.5æµ“åº¦ï¼Œè¾“å‡ºæ ¼å¼å¦‚ä¸‹ã€‚è¦æ±‚ï¼šå†™å‡ºå„ä¸ªå¤„ ç†æ­¥éª¤ï¼Œå¹¶ç»™å‡ºè§£é‡Šã€‚<br>2018-03-15 13:00:00,æµ·æ·€åŒºä¸‡æŸ³,73<br>2018-03-15 13:00:00,æ˜Œå¹³é•‡,67<br>2018-03-15 13:00:00,å¥¥ä½“ä¸­å¿ƒ,66<br>2018-03-15 14:00:00,æµ·æ·€åŒºä¸‡æŸ³,73<br>2018-03-15 14:00:00,æ˜Œå¹³é•‡,73<br>2018-03-15 14:00:00,å¥¥ä½“ä¸­å¿ƒ,75</p><span id="more"></span><h2 id="å®éªŒè¿‡ç¨‹"><a href="#å®éªŒè¿‡ç¨‹" class="headerlink" title="å®éªŒè¿‡ç¨‹"></a>å®éªŒè¿‡ç¨‹</h2><h3 id="æ•°æ®æœé›†"><a href="#æ•°æ®æœé›†" class="headerlink" title="æ•°æ®æœé›†"></a>æ•°æ®æœé›†</h3><p>åŒ—äº¬å„ç›‘æµ‹ç«™çš„<code>PM2.5</code>æŒ‡æ•°çš„æ•°æ®æ¥æºç½‘ç«™ï¼š<a href="http://www.86pm25.com/city/beijing.html">http://www.86pm25.com/city/beijing.html</a><br><img src="https://s2.ax1x.com/2019/10/31/K5rmkV.jpg" alt=""></p><h3 id="æ•°æ®æ•´ç†åŠæ±‡æ€»"><a href="#æ•°æ®æ•´ç†åŠæ±‡æ€»" class="headerlink" title="æ•°æ®æ•´ç†åŠæ±‡æ€»"></a>æ•°æ®æ•´ç†åŠæ±‡æ€»</h3><p>å…ˆå±•ç¤ºå®ç°è¯¥æ“ä½œçš„æŒ‡ä»¤å’Œæœ€åçš„ç»“æœï¼š<br><img src="https://s2.ax1x.com/2019/10/31/K5rl6J.jpg" alt=""><br><img src="https://s2.ax1x.com/2019/10/31/K5rQl4.jpg" alt=""><br><img src="https://s2.ax1x.com/2019/10/31/K5rJTx.jpg" alt=""></p><p>ä¸‹é¢è¯¦ç»†è§£é‡ŠæŒ‡ä»¤ï¼š</p><ol><li>é¦–å…ˆåˆ©ç”¨<tr>æ ‡ç­¾æŠŠæ•°æ®åˆ†æˆå•ç‹¬çš„è¡Œï¼Œ<code>sed -e &#39;s/&lt;tr/\n&lt;tr/g&#39;</code></li><li>å…¶æ¬¡åˆ æ‰htmlæ–‡ä»¶ä¸­çš„æ‰€æœ‰æ ‡ç­¾<code>-e &#39;s/&lt;[^&lt;&gt;]*&gt;/ /g</code>ï¼ŒæŠŠæ‰€æœ‰æ ‡ç­¾éƒ½æ¢æˆäº†ç©ºæ ¼ã€‚</li><li>æˆ‘å…ˆåœ¨htmlæ–‡ä»¶ä¸­å¯»æ‰¾æ—¥æœŸå’Œæ—¶é—´ï¼Œå‘ç°æ—¶é—´çš„é‚£ä¸€è¡Œæœ‰â€œæ›´æ–°â€çš„å­—æ ·ï¼Œäºæ˜¯å»ºç«‹awkæ–‡ä»¶ï¼Œæ­¤æ—¶å‘ç°â€œæ›´æ–°â€åé¢ä¸­æ–‡çš„å†’å·ç´§è·Ÿç€æ—¥æœŸï¼Œæ²¡å‘æŠŠæ—¥æœŸåˆ†ç¦»å¼€ï¼Œäºæ˜¯å…ˆåœ¨ä¸­æ–‡å†’å·åé¢æ·»åŠ ç©ºæ ¼ã€‚é¡ºä¾¿æŠŠæ—¥æœŸå’Œæ—¶é—´çš„æ ¼å¼æ”¹æˆæ ‡å‡†çš„è¾“å‡ºçš„æ ¼å¼ã€‚<code>-e &#39;s/ï¼š/ï¼š /g&#39; -e &#39;s/[å¹´æœˆ]/-/g&#39; -e &#39;s/æ—¥//g -e &#39;s/æ—¶/:00:00/g&#39;</code><br><img src="https://s2.ax1x.com/2019/10/31/K5rrnA.jpg" alt=""></li><li>æ­¤æ—¶å¯ä»¥æŠŠæ—¶é—´å’Œæ—¥æœŸæŠ½ç¦»å‡ºæ¥äº†ã€‚åœ¨å»ºç«‹çš„awkæ–‡ä»¶ä¸­è¾“å…¥<code>/æ›´æ–°/ &#123;data = $2; time = $3&#125;</code></li><li>å¾—åˆ°æ—¥æœŸå’Œæ—¶é—´ ä¹‹åï¼Œæˆ‘ä»¬å»æ‰¾ç›‘æµ‹ç«™å’Œpm2.5æŒ‡æ•°ï¼Œå‘ç°åœ¨è¿™äº›æ•°æ®æœ€åéƒ½æœ‰$m^3$å•ä½åœ¨ï¼Œäºæ˜¯åœ¨awkæ–‡ä»¶ä¸­æ·»åŠ <code>/m3/&#123;printf(&quot;%s %s,%s,%s\n&quot;,date, time, $1, $3);&#125;</code><br><img src="https://s2.ax1x.com/2019/10/31/K5rs0I.jpg" alt=""></li><li>æœ€åæŠŠå•ä½åˆ æ‰ï¼Œå¹¶è¾“å‡ºåˆ°csvæ–‡ä»¶ä¸­å³å¯ã€‚<code>awk -f flow.awk | sed -e &#39;s/[ug/m3]//g&#39; &gt; flow.csv</code></li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;ä½œä¸šè¦æ±‚&quot;&gt;&lt;a href=&quot;#ä½œä¸šè¦æ±‚&quot; class=&quot;headerlink&quot; title=&quot;ä½œä¸šè¦æ±‚&quot;&gt;&lt;/a&gt;ä½œä¸šè¦æ±‚&lt;/h2&gt;&lt;p&gt;ä»å› ç‰¹ç½‘ä¸Šæœç´¢ç›¸å…³Webç½‘é¡µï¼Œå¤„ç†ç½‘é¡µ&lt;code&gt;html&lt;/code&gt;æ•°æ®ï¼Œä»ä¸­æå–å‡ºå½“å‰æ—¶é—´ç‚¹åŒ—äº¬å„ç›‘æµ‹ç«™çš„ PM2.5æµ“åº¦ï¼Œè¾“å‡ºæ ¼å¼å¦‚ä¸‹ã€‚è¦æ±‚ï¼šå†™å‡ºå„ä¸ªå¤„ ç†æ­¥éª¤ï¼Œå¹¶ç»™å‡ºè§£é‡Šã€‚&lt;br&gt;2018-03-15 13:00:00,æµ·æ·€åŒºä¸‡æŸ³,73&lt;br&gt;2018-03-15 13:00:00,æ˜Œå¹³é•‡,67&lt;br&gt;2018-03-15 13:00:00,å¥¥ä½“ä¸­å¿ƒ,66&lt;br&gt;2018-03-15 14:00:00,æµ·æ·€åŒºä¸‡æŸ³,73&lt;br&gt;2018-03-15 14:00:00,æ˜Œå¹³é•‡,73&lt;br&gt;2018-03-15 14:00:00,å¥¥ä½“ä¸­å¿ƒ,75&lt;/p&gt;</summary>
    
    
    
    <category term="Linux" scheme="https://enblog.crocodilezs.top/categories/Linux/"/>
    
    
    <category term="æ–‡æœ¬å¤„ç†" scheme="https://enblog.crocodilezs.top/tags/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>ç¥ç»ç½‘ç»œå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ç®—æ³•æ¨å¯¼</title>
    <link href="https://enblog.crocodilezs.top/201911/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BD%9C%E4%B8%9A%E6%8A%A5%E5%91%8A/"/>
    <id>https://enblog.crocodilezs.top/201911/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BD%9C%E4%B8%9A%E6%8A%A5%E5%91%8A/</id>
    <published>2019-10-28T17:32:10.000Z</published>
    <updated>2022-05-23T16:54:41.206Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ä¸€ã€ç›®æ ‡"><a href="#ä¸€ã€ç›®æ ‡" class="headerlink" title="ä¸€ã€ç›®æ ‡"></a>ä¸€ã€ç›®æ ‡</h2><ol><li>æ¨å¯¼å…·æœ‰å•éšå±‚çš„ç¥ç»ç½‘ç»œçš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ç®—æ³•ï¼Œå¹¶è¿›è¡Œç¼–ç¨‹ï¼ˆå¯ä»¥ä½¿ç”¨<code>sklearn</code>ä¸­çš„ç¥ç»ç½‘ç»œï¼‰ã€‚<ul><li>æ¢è®¨10ï¼Œ30ï¼Œ100ï¼Œ300ï¼Œ1000ï¼Œä¸åŒéšè—èŠ‚ç‚¹æ•°å¯¹ç½‘ç»œæ€§èƒ½çš„å½±å“ã€‚</li><li>æ¢è®¨ä¸åŒå­¦ä¹ ç‡å’Œè¿­ä»£æ¬¡æ•°å¯¹ç½‘ç»œæ€§èƒ½çš„å½±å“ã€‚</li><li>æ”¹å˜æ•°æ®çš„æ ‡å‡†åŒ–æ–¹æ³•ï¼Œæ¢è®¨å¯¹è®­ç»ƒçš„å½±å“ã€‚</li></ul></li><li>æŸ¥é˜…èµ„æ–™è¯´æ˜ä»€ä¹ˆæ˜¯<code>Hebb</code>å­¦ä¹ è§„åˆ™</li></ol><span id="more"></span><h2 id="äºŒã€æ¨å¯¼å•éšå±‚ç¥ç»ç½‘ç»œçš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ç®—æ³•"><a href="#äºŒã€æ¨å¯¼å•éšå±‚ç¥ç»ç½‘ç»œçš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ç®—æ³•" class="headerlink" title="äºŒã€æ¨å¯¼å•éšå±‚ç¥ç»ç½‘ç»œçš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ç®—æ³•"></a>äºŒã€æ¨å¯¼å•éšå±‚ç¥ç»ç½‘ç»œçš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ç®—æ³•</h2><p>å‚è€ƒèµ„æ–™ï¼š<a href="https://blog.csdn.net/Lucky_Go/article/details/89738286">https://blog.csdn.net/Lucky_Go/article/details/89738286</a><br><img src="image4\reduction1.jpg" alt=""><br><img src="image4\reduction2.jpg" alt=""><br><img src="image4\reduction3.jpg" alt=""><br><img src="image4\reduction4.jpg" alt=""></p><h2 id="ä¸‰ã€ç®—æ³•å®ç°"><a href="#ä¸‰ã€ç®—æ³•å®ç°" class="headerlink" title="ä¸‰ã€ç®—æ³•å®ç°"></a>ä¸‰ã€ç®—æ³•å®ç°</h2><p>å‚è€ƒèµ„æ–™ï¼š<a href="https://blog.csdn.net/zsx17/article/details/89342506">https://blog.csdn.net/zsx17/article/details/89342506</a></p><p>å› ä¸ºç½‘ä¸Šç¥ç»ç½‘ç»œçš„ä»£ç åŸºæœ¬éƒ½æ˜¯ç”¨<code>tensorflow</code>å®ç°çš„ï¼Œè¿™é‡Œæ˜¯ç›´æ¥è°ƒåº“ã€‚åœ¨å®Œæˆäº†ä½œä¸šçš„åŸºæœ¬è¦æ±‚ä¹‹åæˆ‘ä¹Ÿå°è¯•äº†è‡ªå·±å®ç°å•éšå±‚ç¥ç»ç½‘ç»œçš„ä»£ç ï¼ˆåœ¨å®éªŒæŠ¥å‘Šçš„åéƒ¨åˆ†ï¼‰ã€‚</p><h3 id="1-è½½å…¥æ•°æ®"><a href="#1-è½½å…¥æ•°æ®" class="headerlink" title="1. è½½å…¥æ•°æ®"></a>1. è½½å…¥æ•°æ®</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1ã€è½½å…¥æ•°æ®</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.examples.tutorials.mnist.input_data <span class="keyword">as</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># è¯»å–mnistæ•°æ®</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">&#x27;MNIST_data/&#x27;</span>, one_hot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="2-å»ºç«‹æ¨¡å‹"><a href="#2-å»ºç«‹æ¨¡å‹" class="headerlink" title="2. å»ºç«‹æ¨¡å‹"></a>2. å»ºç«‹æ¨¡å‹</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.å»ºç«‹æ¨¡å‹</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.1 æ„å»ºè¾“å…¥å±‚</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>], name=<span class="string">&#x27;X&#x27;</span>)</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>], name=<span class="string">&#x27;Y&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.2 æ„å»ºéšè—å±‚</span></span><br><span class="line"><span class="comment"># éšè—å±‚ç¥ç»å…ƒæ•°é‡(éšæ„è®¾ç½®ï¼‰</span></span><br><span class="line">H1_NN = <span class="number">256</span></span><br><span class="line"><span class="comment"># æƒé‡</span></span><br><span class="line">W1 = tf.Variable(tf.random_normal([<span class="number">784</span>, H1_NN]))</span><br><span class="line"><span class="comment"># åç½®é¡¹</span></span><br><span class="line">b1 = tf.Variable(tf.zeros([H1_NN]))</span><br><span class="line"></span><br><span class="line">Y1 = tf.nn.relu(tf.matmul(x, W1) + b1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.3 æ„å»ºè¾“å‡ºå±‚</span></span><br><span class="line">W2 = tf.Variable(tf.random_normal([H1_NN, <span class="number">10</span>]))</span><br><span class="line">b2 = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line">forward = tf.matmul(Y1, W2) + b2</span><br><span class="line">pred = tf.nn.softmax(forward)</span><br></pre></td></tr></table></figure><h3 id="3-è®­ç»ƒæ¨¡å‹"><a href="#3-è®­ç»ƒæ¨¡å‹" class="headerlink" title="3. è®­ç»ƒæ¨¡å‹"></a>3. è®­ç»ƒæ¨¡å‹</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.è®­ç»ƒæ¨¡å‹</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.1 å®šä¹‰æŸå¤±å‡½æ•°</span></span><br><span class="line"><span class="comment"># tensorflowæä¾›äº†ä¸‹é¢çš„å‡½æ•°ï¼Œç”¨äºé¿å…log(0)å€¼ä¸ºNané€ æˆæ•°æ®ä¸ç¨³å®š</span></span><br><span class="line">loss_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=forward, labels=y))</span><br><span class="line"><span class="comment"># # äº¤å‰ç†µæŸå¤±å‡½æ•°</span></span><br><span class="line"><span class="comment"># loss_function = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.2 è®¾ç½®è®­ç»ƒå‚æ•°</span></span><br><span class="line">train_epochs = <span class="number">40</span>  <span class="comment"># è®­ç»ƒè½®æ•°</span></span><br><span class="line">batch_size = <span class="number">50</span>  <span class="comment"># å•æ¬¡è®­ç»ƒæ ·æœ¬æ•°(æ‰¹æ¬¡å¤§å°)</span></span><br><span class="line"><span class="comment"># ä¸€è½®è®­ç»ƒçš„æ‰¹æ¬¡æ•°</span></span><br><span class="line">total_batch = <span class="built_in">int</span>(mnist.train.num_examples / batch_size)</span><br><span class="line">display_step = <span class="number">1</span>  <span class="comment"># æ˜¾ç¤ºç²’æ•°</span></span><br><span class="line">learning_rate = <span class="number">0.01</span>  <span class="comment"># å­¦ä¹ ç‡</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.2 é€‰æ‹©ä¼˜åŒ–å™¨</span></span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss_function)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.3å®šä¹‰å‡†ç¡®ç‡</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(pred, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.4 æ¨¡å‹çš„è®­ç»ƒ</span></span><br><span class="line"><span class="comment"># è®°å½•è®­ç»ƒå¼€å§‹çš„æ—¶é—´</span></span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">startTime = time()</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(train_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> <span class="built_in">range</span>(total_batch):</span><br><span class="line">        <span class="comment"># è¯»å–æ‰¹æ¬¡è®­ç»ƒæ•°æ®</span></span><br><span class="line">        xs, ys = mnist.train.next_batch(batch_size)</span><br><span class="line">        <span class="comment"># æ‰§è¡Œæ‰¹æ¬¡è®­ç»ƒ</span></span><br><span class="line">        sess.run(optimizer, feed_dict=&#123;x: xs, y: ys&#125;)</span><br><span class="line">    <span class="comment"># åœ¨total_batchæ‰¹æ¬¡æ•°æ®è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨éªŒè¯æ•°æ®è®¡ç®—è¯¯å·®å’Œå‡†ç¡®ç‡ï¼ŒéªŒè¯é›†ä¸åˆ†æ‰¹</span></span><br><span class="line">    loss, acc = sess.run([loss_function, accuracy], feed_dict=&#123;x: mnist.validation.images, y: mnist.validation.labels&#125;)</span><br><span class="line">    <span class="comment"># æ‰“å°è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¯¦ç»†ä¿¡æ¯</span></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;è®­ç»ƒè½®æ¬¡ï¼š&#x27;</span>, <span class="string">&#x27;%02d&#x27;</span> % (epoch + <span class="number">1</span>),</span><br><span class="line">              <span class="string">&#x27;æŸå¤±ï¼š&#x27;</span>, <span class="string">&#x27;&#123;:.9f&#125;&#x27;</span>.<span class="built_in">format</span>(loss),</span><br><span class="line">              <span class="string">&#x27;å‡†ç¡®ç‡ï¼š&#x27;</span>, <span class="string">&#x27;&#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(acc))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;è®­ç»ƒç»“æŸ&#x27;</span>)</span><br><span class="line"><span class="comment"># æ˜¾ç¤ºæ€»è¿è¡Œæ—¶é—´</span></span><br><span class="line">duration = time() - startTime</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;æ€»è¿è¡Œæ—¶é—´ä¸ºï¼š&quot;</span>, <span class="string">&quot;&#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(duration))</span><br></pre></td></tr></table></figure><h3 id="4-æ¨¡å‹è¯„ä¼°"><a href="#4-æ¨¡å‹è¯„ä¼°" class="headerlink" title="4. æ¨¡å‹è¯„ä¼°"></a>4. æ¨¡å‹è¯„ä¼°</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 4.è¯„ä¼°æ¨¡å‹</span></span><br><span class="line">accu_test = sess.run(accuracy,</span><br><span class="line">                     feed_dict=&#123;x: mnist.test.images, y: mnist.test.labels&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š&#x27;</span>, accu_test)</span><br></pre></td></tr></table></figure><h3 id="5-åº”ç”¨æ¨¡å‹"><a href="#5-åº”ç”¨æ¨¡å‹" class="headerlink" title="5. åº”ç”¨æ¨¡å‹"></a>5. åº”ç”¨æ¨¡å‹</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 5.åº”ç”¨æ¨¡å‹</span></span><br><span class="line">prediction_result = sess.run(tf.argmax(pred, <span class="number">1</span>), feed_dict=&#123;x: mnist.test.images&#125;)</span><br><span class="line"><span class="comment"># æŸ¥çœ‹é¢„æµ‹ç»“æœçš„å‰10é¡¹</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;å‰10é¡¹çš„ç»“æœï¼š&quot;</span>, prediction_result[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.1æ‰¾å‡ºé¢„æµ‹é”™è¯¯çš„æ ·æœ¬</span></span><br><span class="line">compare_lists = prediction_result == np.argmax(mnist.test.labels, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(compare_lists)</span><br><span class="line">err_lists = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(compare_lists)) <span class="keyword">if</span> compare_lists[i] == <span class="literal">False</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;é¢„æµ‹é”™è¯¯çš„å›¾ç‰‡ï¼š&#x27;</span>, err_lists)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;é¢„æµ‹é”™è¯¯å›¾ç‰‡çš„æ€»æ•°ï¼š&#x27;</span>, <span class="built_in">len</span>(err_lists))</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰ä¸€ä¸ªè¾“å‡ºé”™è¯¯åˆ†ç±»çš„å‡½æ•°</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">print_predict_errs</span>(<span class="params">labels,  <span class="comment"># æ ‡ç­¾åˆ—è¡¨</span></span></span><br><span class="line"><span class="params">                       prediction</span>):  <span class="comment"># é¢„æµ‹å€¼åˆ—è¡¨</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    compare_lists = (prediction == np.argmax(labels, <span class="number">1</span>))</span><br><span class="line">    err_lists = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(compare_lists)) <span class="keyword">if</span> compare_lists[i] == <span class="literal">False</span>]</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> err_lists:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;index=&#x27;</span> + <span class="built_in">str</span>(x) + <span class="string">&#x27;æ ‡ç­¾å€¼=&#x27;</span>, np.argmax(labels[x]), <span class="string">&#x27;é¢„æµ‹å€¼=&#x27;</span>, prediction[x])</span><br><span class="line">        count = count + <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;æ€»è®¡ï¼š&quot;</span> + <span class="built_in">str</span>(count))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print_predict_errs(labels=mnist.test.labels, prediction=prediction_result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å¯è§†åŒ–</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_images_labels_prediction</span>(<span class="params">images,  <span class="comment"># å›¾åƒåˆ—è¡¨</span></span></span><br><span class="line"><span class="params">                                  labels,  <span class="comment"># æ ‡ç­¾åˆ—è¡¨</span></span></span><br><span class="line"><span class="params">                                  predication,  <span class="comment"># é¢„æµ‹å€¼åˆ—è¡¨</span></span></span><br><span class="line"><span class="params">                                  index,  <span class="comment"># ä»ç¬¬indexä¸ªå¼€å§‹æ˜¾ç¤º</span></span></span><br><span class="line"><span class="params">                                  num=<span class="number">10</span></span>):  <span class="comment"># ç¼ºçœä¸€æ¬¡æ˜¾ç¤º10å¹…</span></span><br><span class="line">    fig = plt.gcf()  <span class="comment"># è·å–å½“å‰å›¾è¡¨ï¼Œget current figure</span></span><br><span class="line">    fig.set_size_inches(<span class="number">10</span>, <span class="number">12</span>)  <span class="comment"># è®¾ä¸ºè‹±å¯¸ï¼Œ1è‹±å¯¸=2.53å˜ç±³</span></span><br><span class="line">    <span class="keyword">if</span> num &gt; <span class="number">25</span>:</span><br><span class="line">        num = <span class="number">25</span>  <span class="comment"># æœ€å¤šæ˜¾ç¤º25ä¸ªå­å›¾</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num):</span><br><span class="line">        ax = plt.subplot(<span class="number">5</span>, <span class="number">5</span>, i + <span class="number">1</span>)  <span class="comment"># è·å–å½“å‰è¦å¤„ç†çš„å­å›¾</span></span><br><span class="line">        <span class="comment"># æ˜¾ç¤ºç¬¬indexå›¾åƒ</span></span><br><span class="line">        ax.imshow(np.reshape(images[index], (<span class="number">28</span>, <span class="number">28</span>)), cmap=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># æ„å»ºè¯¥å›¾ä¸Šæ˜¾ç¤ºçš„title</span></span><br><span class="line">        title = <span class="string">&#x27;label=&#x27;</span> + <span class="built_in">str</span>(np.argmax(labels[index]))</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(predication) &gt; <span class="number">0</span>:</span><br><span class="line">            title += <span class="string">&quot;,predict=&quot;</span> + <span class="built_in">str</span>(predication[index])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># æ˜¾ç¤ºå›¾ä¸Šçš„titleä¿¡æ¯</span></span><br><span class="line">        ax.set_title(title, fontsize=<span class="number">10</span>)</span><br><span class="line">        ax.set_xticks([])  <span class="comment"># ä¸æ˜¾ç¤ºåæ ‡è½´</span></span><br><span class="line">        ax.set_yticks([])</span><br><span class="line">        index += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plot_images_labels_prediction(mnist.test.images,</span><br><span class="line">                              mnist.test.labels,</span><br><span class="line">                              prediction_result, <span class="number">10</span>, <span class="number">25</span>)</span><br><span class="line">plot_images_labels_prediction(mnist.test.images,</span><br><span class="line">                              mnist.test.labels,</span><br><span class="line">                              prediction_result, <span class="number">610</span>, <span class="number">20</span>)</span><br></pre></td></tr></table></figure><h3 id="6-ç»“æœå±•ç¤º"><a href="#6-ç»“æœå±•ç¤º" class="headerlink" title="6. ç»“æœå±•ç¤º"></a>6. ç»“æœå±•ç¤º</h3><p>ä¸Šé¢çš„ä»£ç ä¸­éšå±‚èŠ‚ç‚¹ä¸ªæ•°ä¸º256ä¸ªï¼Œå­¦ä¹ ç‡ä¸º0.01ï¼Œè¿­ä»£æ¬¡æ•°ä¸º40æ¬¡ã€‚è®­ç»ƒç»“æœå¦‚ä¸‹ï¼š</p><p><img src="image4\256-0.01-40.jpg" alt=""></p><p>éƒ¨åˆ†åˆ†ç±»å›¾åƒå¦‚ä¸‹æ‰€ç¤ºï¼š</p><p><img src="image4\256-0.01-40-1.jpg" alt=""></p><p><img src="image4\256-0.01-40-2.jpg" alt=""></p><h2 id="å››ã€ç®—æ³•è°ƒä¼˜"><a href="#å››ã€ç®—æ³•è°ƒä¼˜" class="headerlink" title="å››ã€ç®—æ³•è°ƒä¼˜"></a>å››ã€ç®—æ³•è°ƒä¼˜</h2><p>åœ¨ä¸Šé¢çš„æ¨¡å‹ä¸­éšå±‚ç»“ç‚¹æ•°ä¸º256ï¼Œå­¦ä¹ ç‡ä¸º0.01ï¼Œè¿­ä»£æ¬¡æ•°ä¸º40æ¬¡ã€‚</p><p>ä¸‹é¢åˆ†åˆ«ä»éšå±‚èŠ‚ç‚¹æ•°ã€å­¦ä¹ ç‡å’Œè¿­ä»£æ¬¡æ•°ä¸‰ä¸ªè§’åº¦è¿›è¡Œè°ƒä¼˜ã€‚</p><h3 id="1-éšå±‚èŠ‚ç‚¹æ•°"><a href="#1-éšå±‚èŠ‚ç‚¹æ•°" class="headerlink" title="1. éšå±‚èŠ‚ç‚¹æ•°"></a>1. éšå±‚èŠ‚ç‚¹æ•°</h3><p>å°†éšå±‚èŠ‚ç‚¹æ•°è®¾ä¸º10ï¼Œå¾—åˆ°çš„ç»“æœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p><p><img src="image4\10-0.01-40.jpg" alt=""></p><p>å°†éšå±‚èŠ‚ç‚¹è®¾ä¸º30ï¼Œ100ï¼Œ300ï¼Œ1000çš„æ•ˆæœä¸å†å…·ä½“å±•ç¤ºï¼Œæ•ˆæœå¦‚ä¸‹æ‰€ç¤ºï¼š</p><div class="table-container"><table><thead><tr><th style="text-align:center">éšå±‚èŠ‚ç‚¹ä¸ªæ•°</th><th style="text-align:center">æ€»è¿è¡Œæ—¶é—´/s</th><th style="text-align:center">é¢„æµ‹é”™è¯¯çš„å›¾ç‰‡æ•°</th><th style="text-align:center">å‡†ç¡®ç‡</th></tr></thead><tbody><tr><td style="text-align:center">10</td><td style="text-align:center">46.29</td><td style="text-align:center">736</td><td style="text-align:center">0.9264</td></tr><tr><td style="text-align:center">30</td><td style="text-align:center">43.46</td><td style="text-align:center">528</td><td style="text-align:center">0.9472</td></tr><tr><td style="text-align:center">100</td><td style="text-align:center">59.06</td><td style="text-align:center">343</td><td style="text-align:center">0.9657</td></tr><tr><td style="text-align:center">256</td><td style="text-align:center">84.48</td><td style="text-align:center">249</td><td style="text-align:center">0.9751</td></tr><tr><td style="text-align:center">300</td><td style="text-align:center">76.64</td><td style="text-align:center">269</td><td style="text-align:center">0.9731</td></tr><tr><td style="text-align:center">1000</td><td style="text-align:center">302.27</td><td style="text-align:center">240</td><td style="text-align:center">0.976</td></tr></tbody></table></div><p>ç”±è¡¨å¯çŸ¥ï¼Œå‡†ç¡®ç‡éšç€éšå±‚èŠ‚ç‚¹ä¸ªæ•°çš„å¢åŠ è€Œå¢åŠ ï¼Œå¢åŠ é€Ÿç‡é€æ­¥å‡å°‘ã€‚</p><h3 id="2-å­¦ä¹ ç‡"><a href="#2-å­¦ä¹ ç‡" class="headerlink" title="2. å­¦ä¹ ç‡"></a>2. å­¦ä¹ ç‡</h3><p>å­¦ä¹ ç‡åˆ†åˆ«ä¸º0.005ï¼Œ0.01ï¼Œ 0.02ï¼Œ 0.1ï¼Œéšå±‚èŠ‚ç‚¹æ•°é€‰æ‹©256ï¼Œè¿­ä»£æ¬¡æ•°é€‰æ‹©40ã€‚åˆ†ç±»ç»“æœå¦‚ä¸‹ï¼š</p><div class="table-container"><table><thead><tr><th style="text-align:center">å­¦ä¹ ç‡</th><th style="text-align:center">æ€»è¿è¡Œæ—¶é—´/s</th><th style="text-align:center">é¢„æµ‹é”™è¯¯çš„å›¾ç‰‡æ•°</th><th style="text-align:center">å‡†ç¡®ç‡</th></tr></thead><tbody><tr><td style="text-align:center">0.005</td><td style="text-align:center">78.81</td><td style="text-align:center">231</td><td style="text-align:center">0.9769</td></tr><tr><td style="text-align:center">0.01</td><td style="text-align:center">84.48</td><td style="text-align:center">249</td><td style="text-align:center">0.9751</td></tr><tr><td style="text-align:center">0.02</td><td style="text-align:center">69.72</td><td style="text-align:center">446</td><td style="text-align:center">0.9554</td></tr><tr><td style="text-align:center">0.1</td><td style="text-align:center">73.87</td><td style="text-align:center">2561</td><td style="text-align:center">0.7439</td></tr></tbody></table></div><p>ç”±è¡¨å¯çŸ¥ï¼Œå‡†ç¡®ç‡éšç€å­¦ä¹ ç‡çš„å¢åŠ è€Œé™ä½ã€‚åœ¨å­¦ä¹ ç‡ä½äº0.01æ—¶ï¼Œå›¾ç‰‡åˆ†ç±»çš„å‡†ç¡®ç‡æå‡çš„é€Ÿç‡è¾ƒå°ã€‚</p><h3 id="3-è¿­ä»£æ¬¡æ•°"><a href="#3-è¿­ä»£æ¬¡æ•°" class="headerlink" title="3. è¿­ä»£æ¬¡æ•°"></a>3. è¿­ä»£æ¬¡æ•°</h3><p>è¿­ä»£æ¬¡æ•°åˆ†åˆ«ä¸º20ï¼Œ40ï¼Œ100ï¼Œéšå±‚èŠ‚ç‚¹æ•°é€‰æ‹©256ï¼Œå­¦ä¹ ç‡é€‰æ‹©0.01ã€‚åˆ†ç±»ç»“æœå¦‚ä¸‹ï¼š</p><div class="table-container"><table><thead><tr><th style="text-align:center">è¿­ä»£æ¬¡æ•°</th><th style="text-align:center">æ€»è¿è¡Œæ—¶é—´/s</th><th style="text-align:center">é¢„æµ‹é”™è¯¯çš„å›¾ç‰‡æ•°</th><th style="text-align:center">å‡†ç¡®ç‡</th></tr></thead><tbody><tr><td style="text-align:center">20</td><td style="text-align:center">37.12</td><td style="text-align:center">307</td><td style="text-align:center">0.9693</td></tr><tr><td style="text-align:center">40</td><td style="text-align:center">84.48</td><td style="text-align:center">249</td><td style="text-align:center">0.9751</td></tr><tr><td style="text-align:center">100</td><td style="text-align:center">184.39</td><td style="text-align:center">239</td><td style="text-align:center">0.9761</td></tr></tbody></table></div><p>ç”±è¡¨å¯çŸ¥ï¼Œè¿­ä»£æ¬¡æ•°å¯¹æ€»è¿è¡Œæ—¶é—´çš„å½±å“ç‡å¾ˆå¤§ï¼Œå‡†ç¡®ç‡éšç€è¿­ä»£æ¬¡æ•°çš„å¢åŠ è€Œå¢åŠ ï¼Œä½†å¯¹å‡†ç¡®ç‡èµ·å†³å®šå› ç´ çš„è¿˜æ˜¯éšå±‚çš„èŠ‚ç‚¹ä¸ªæ•°ä»¥åŠå­¦ä¹ ç‡ã€‚</p><h3 id="4-æ”¹å˜æ•°æ®æ ‡å‡†åŒ–æ–¹æ³•"><a href="#4-æ”¹å˜æ•°æ®æ ‡å‡†åŒ–æ–¹æ³•" class="headerlink" title="4. æ”¹å˜æ•°æ®æ ‡å‡†åŒ–æ–¹æ³•"></a>4. æ”¹å˜æ•°æ®æ ‡å‡†åŒ–æ–¹æ³•</h3><h4 id="æœ€å¤§-æœ€å°è§„èŒƒåŒ–"><a href="#æœ€å¤§-æœ€å°è§„èŒƒåŒ–" class="headerlink" title="æœ€å¤§-æœ€å°è§„èŒƒåŒ–"></a>æœ€å¤§-æœ€å°è§„èŒƒåŒ–</h4><h4 id="Z-scoreè§„èŒƒåŒ–"><a href="#Z-scoreè§„èŒƒåŒ–" class="headerlink" title="Z-scoreè§„èŒƒåŒ–"></a><code>Z-score</code>è§„èŒƒåŒ–</h4><h2 id="äº”ã€Hebbå­¦ä¹ è§„åˆ™"><a href="#äº”ã€Hebbå­¦ä¹ è§„åˆ™" class="headerlink" title="äº”ã€Hebbå­¦ä¹ è§„åˆ™"></a>äº”ã€<code>Hebb</code>å­¦ä¹ è§„åˆ™</h2><p>å‚è€ƒèµ„æ–™ï¼š<a href="https://baike.baidu.com/item/Hebbå­¦ä¹ è§„åˆ™/3061563?fr=aladdin">https://baike.baidu.com/item/Hebb%E5%AD%A6%E4%B9%A0%E8%A7%84%E5%88%99/3061563?fr=aladdin</a></p><p><code>Hebb</code>å­¦ä¹ è§„åˆ™æ˜¯ä¸€ä¸ªæ— ç›‘ç£å­¦ä¹ è§„åˆ™ï¼Œè¿™ç§å­¦ä¹ çš„ç»“æœæ˜¯ä½¿ç½‘ç»œèƒ½å¤Ÿæå–è®­ç»ƒé›†çš„ç»Ÿè®¡ç‰¹æ€§ï¼Œä»è€ŒæŠŠè¾“å…¥ä¿¡æ¯æŒ‰ç…§å®ƒä»¬çš„ç›¸ä¼¼æ€§ç¨‹åº¦åˆ’åˆ†ä¸ºè‹¥å¹²ç±»ã€‚è¿™ä¸€ç‚¹ä¸äººç±»è§‚å¯Ÿå’Œè®¤è¯†ä¸–ç•Œçš„è¿‡ç¨‹éå¸¸å»åˆï¼Œäººç±»è§‚å¯Ÿå’Œè®¤è¯†ä¸–ç•Œåœ¨ç›¸å½“ç¨‹åº¦ä¸Šå°±æ˜¯åœ¨æ ¹æ®äº‹ç‰©çš„ç»Ÿè®¡ç‰¹å¾è¿›è¡Œåˆ†ç±»ã€‚<code>Hebb</code>å­¦ä¹ è§„åˆ™åªæ ¹æ®ç¥ç»å…ƒè¿æ¥é—´çš„æ¿€æ´»æ°´å¹³æ”¹å˜æƒå€¼ï¼Œå› æ­¤è¿™ç§æ–¹æ³•åˆç§°ä¸ºç›¸å…³å­¦ä¹ æˆ–å¹¶è”å­¦ä¹ ã€‚</p><p>æ— ç›‘ç£å­¦ä¹ è§„åˆ™<br>â€ƒå”çº³å¾·Â·èµ«å¸ƒï¼ˆ1904-1985ï¼‰æ˜¯åŠ æ‹¿å¤§è‘—åç”Ÿç†å¿ƒç†å­¦å®¶ã€‚<code>Hebb</code>å­¦ä¹ è§„åˆ™ä¸â€œæ¡ä»¶åå°„â€æœºç†ä¸€è‡´ï¼Œå¹¶ä¸”å·²ç»å¾—åˆ°äº†ç¥ç»ç»†èƒå­¦è¯´çš„è¯å®ã€‚<br>â€ƒå·´ç”«æ´›å¤«çš„æ¡ä»¶åå°„å®éªŒï¼šæ¯æ¬¡ç»™ç‹—å–‚é£Ÿå‰éƒ½å…ˆå“é“ƒï¼Œæ—¶é—´ä¸€é•¿ï¼Œç‹—å°±ä¼šå°†é“ƒå£°å’Œé£Ÿç‰©è”ç³»èµ·æ¥ã€‚ä»¥åå¦‚æœå“é“ƒä½†æ˜¯ä¸ç»™é£Ÿç‰©ï¼Œç‹—ä¹Ÿä¼šæµå£æ°´ã€‚<br>â€ƒå—è¯¥å®éªŒçš„å¯å‘ï¼ŒHebbçš„ç†è®ºè®¤ä¸ºåœ¨åŒä¸€æ—¶é—´è¢«æ¿€å‘çš„ç¥ç»å…ƒé—´çš„è”ç³»ä¼šè¢«å¼ºåŒ–ã€‚æ¯”å¦‚ï¼Œé“ƒå£°å“æ—¶ä¸€ä¸ªç¥ç»å…ƒè¢«æ¿€å‘ï¼Œåœ¨åŒä¸€æ—¶é—´é£Ÿç‰©çš„å‡ºç°ä¼šæ¿€å‘é™„è¿‘çš„å¦ä¸€ä¸ªç¥ç»å…ƒï¼Œé‚£ä¹ˆè¿™ä¸¤ä¸ªç¥ç»å…ƒé—´çš„è”ç³»å°±ä¼šå¼ºåŒ–ï¼Œä»è€Œè®°ä½è¿™ä¸¤ä¸ªäº‹ç‰©ä¹‹é—´å­˜åœ¨ç€è”ç³»ã€‚ç›¸åï¼Œå¦‚æœä¸¤ä¸ªç¥ç»å…ƒæ€»æ˜¯ä¸èƒ½åŒæ­¥æ¿€å‘ï¼Œé‚£ä¹ˆå®ƒä»¬é—´çš„è”ç³»å°†ä¼šè¶Šæ¥è¶Šå¼±ã€‚<br>â€ƒ<code>Hebb</code>å­¦ä¹ å¾‹å¯è¡¨ç¤ºä¸ºï¼š<br>$W<em>{ij}(t+1)=W</em>{ij}(t)+aâ‹…y<em>iâ‹…y_j$<br>$W</em>{ij}(t+1)=W_{ij}(t)+aâ‹…y_iâ‹…y_j$</p><p>â€ƒå…¶ä¸­$W<em>{ij}$è¡¨ç¤ºç¥ç»å…ƒ$j$åˆ°ç¥ç»å…ƒ$i$çš„è¿æ¥æƒï¼Œ$y_i$ä¸$y_j$è¡¨ç¤ºä¸¤ä¸ªç¥ç»å…ƒçš„è¾“å‡ºï¼Œ$a$æ˜¯è¡¨ç¤ºå­¦ä¹ é€Ÿç‡çš„å¸¸æ•°ï¼Œå¦‚æœ$y_i$ä¸$y_j$åŒæ—¶è¢«æ¿€æ´»ï¼Œå³$y_i$ä¸$y_j$åŒæ—¶ä¸ºæ­£ï¼Œé‚£ä¹ˆ$W</em>{ij}$å°†å¢å¤§ã€‚å¦‚æœ$y<em>i$è¢«æ¿€æ´»ï¼Œè€Œ$y_j$å¤„äºæŠ‘åˆ¶çŠ¶æ€ï¼Œå³$y_i$ä¸ºæ­£$y_j$ä¸ºè´Ÿï¼Œé‚£ä¹ˆ$W</em>{ij}$å°†å˜å°ã€‚</p><p><img src="https://images2015.cnblogs.com/blog/520787/201510/520787-20151021081107630-1544768706.png" alt=""></p><h2 id="å…­ã€-è‡ªå·±å®ç°å•éšå±‚ç¥ç»ç½‘ç»œ"><a href="#å…­ã€-è‡ªå·±å®ç°å•éšå±‚ç¥ç»ç½‘ç»œ" class="headerlink" title="å…­ã€ è‡ªå·±å®ç°å•éšå±‚ç¥ç»ç½‘ç»œ"></a>å…­ã€ è‡ªå·±å®ç°å•éšå±‚ç¥ç»ç½‘ç»œ</h2><p>å‚è€ƒèµ„æ–™ï¼š<a href="https://blog.csdn.net/hellozhxy/article/details/81055391">https://blog.csdn.net/hellozhxy/article/details/81055391</a></p><p>ç½‘ç»œç»“æ„çš„å‡½æ•°å®šä¹‰ï¼š</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">layer_sizes</span>(<span class="params">X, Y</span>):</span><br><span class="line">    n_x = X.shape[<span class="number">0</span>] <span class="comment"># size of input layer</span></span><br><span class="line">    n_h = <span class="number">4</span> <span class="comment"># size of hidden layer</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>] <span class="comment"># size of output layer</span></span><br><span class="line">    <span class="keyword">return</span> (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><p>å‚æ•°åˆå§‹åŒ–å‡½æ•°ï¼š</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_parameters</span>(<span class="params">n_x, n_h, n_y</span>):</span><br><span class="line">    W1 = np.random.randn(n_h, n_x)*<span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)*<span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>)) </span><br><span class="line">   </span><br><span class="line">    <span class="keyword">assert</span> (W1.shape == (n_h, n_x))    </span><br><span class="line">    <span class="keyword">assert</span> (b1.shape == (n_h, <span class="number">1</span>))    </span><br><span class="line">    <span class="keyword">assert</span> (W2.shape == (n_y, n_h))    </span><br><span class="line">    <span class="keyword">assert</span> (b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line">    parameters = &#123;<span class="string">&quot;W1&quot;</span>: W1, </span><br><span class="line">                  <span class="string">&quot;b1&quot;</span>: b1,                 </span><br><span class="line">                  <span class="string">&quot;W2&quot;</span>: W2,                  </span><br><span class="line">                  <span class="string">&quot;b2&quot;</span>: b2&#125;   </span><br><span class="line">                   </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>å‰å‘ä¼ æ’­è®¡ç®—å‡½æ•°ï¼š</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_propagation</span>(<span class="params">X, parameters</span>):</span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary &quot;parameters&quot;</span></span><br><span class="line">    W1 = parameters[<span class="string">&#x27;W1&#x27;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&#x27;b1&#x27;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&#x27;W2&#x27;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&#x27;b2&#x27;</span>]    </span><br><span class="line">    <span class="comment"># Implement Forward Propagation to calculate A2 (probabilities)</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = np.tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2, Z1) + b2</span><br><span class="line">    A2 = sigmoid(Z2)    </span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = &#123;<span class="string">&quot;Z1&quot;</span>: Z1,                   </span><br><span class="line">             <span class="string">&quot;A1&quot;</span>: A1,                   </span><br><span class="line">             <span class="string">&quot;Z2&quot;</span>: Z2,                  </span><br><span class="line">             <span class="string">&quot;A2&quot;</span>: A2&#125;    </span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure><p>è®¡ç®—æŸå¤±å‡½æ•°ï¼š</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost</span>(<span class="params">A2, Y, parameters</span>):</span><br><span class="line">    m = Y.shape[<span class="number">1</span>] <span class="comment"># number of example</span></span><br><span class="line">    <span class="comment"># Compute the cross-entropy cost</span></span><br><span class="line">    logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(<span class="number">1</span>-A2), <span class="number">1</span>-Y)</span><br><span class="line">    cost = -<span class="number">1</span>/m * np.<span class="built_in">sum</span>(logprobs)</span><br><span class="line">    cost = np.squeeze(cost)     <span class="comment"># makes sure cost is the dimension we expect.</span></span><br><span class="line">    <span class="keyword">assert</span>(<span class="built_in">isinstance</span>(cost, <span class="built_in">float</span>))    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><p>åå‘ä¼ æ’­å‡½æ•°ï¼š</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backward_propagation</span>(<span class="params">parameters, cache, X, Y</span>):</span><br><span class="line">    m = X.shape[<span class="number">1</span>]    </span><br><span class="line">    <span class="comment"># First, retrieve W1 and W2 from the dictionary &quot;parameters&quot;.</span></span><br><span class="line">    W1 = parameters[<span class="string">&#x27;W1&#x27;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&#x27;W2&#x27;</span>]    </span><br><span class="line">    <span class="comment"># Retrieve also A1 and A2 from dictionary &quot;cache&quot;.</span></span><br><span class="line">    A1 = cache[<span class="string">&#x27;A1&#x27;</span>]</span><br><span class="line">    A2 = cache[<span class="string">&#x27;A2&#x27;</span>]    </span><br><span class="line">    <span class="comment"># Backward propagation: calculate dW1, db1, dW2, db2. </span></span><br><span class="line">    dZ2 = A2-Y</span><br><span class="line">    dW2 = <span class="number">1</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1</span>/m * np.<span class="built_in">sum</span>(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dZ1 = np.dot(W2.T, dZ2)*(<span class="number">1</span>-np.power(A1, <span class="number">2</span>))</span><br><span class="line">    dW1 = <span class="number">1</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1</span>/m * np.<span class="built_in">sum</span>(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    grads = &#123;<span class="string">&quot;dW1&quot;</span>: dW1,</span><br><span class="line">             <span class="string">&quot;db1&quot;</span>: db1,                      </span><br><span class="line">             <span class="string">&quot;dW2&quot;</span>: dW2,             </span><br><span class="line">             <span class="string">&quot;db2&quot;</span>: db2&#125;   </span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><p>æƒå€¼æ›´æ–°å‡½æ•°ï¼š</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">update_parameters</span>(<span class="params">parameters, grads, learning_rate = <span class="number">1.2</span></span>):</span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary &quot;parameters&quot;</span></span><br><span class="line">    W1 = parameters[<span class="string">&#x27;W1&#x27;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&#x27;b1&#x27;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&#x27;W2&#x27;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&#x27;b2&#x27;</span>]    </span><br><span class="line">    <span class="comment"># Retrieve each gradient from the dictionary &quot;grads&quot;</span></span><br><span class="line">    dW1 = grads[<span class="string">&#x27;dW1&#x27;</span>]</span><br><span class="line">    db1 = grads[<span class="string">&#x27;db1&#x27;</span>]</span><br><span class="line">    dW2 = grads[<span class="string">&#x27;dW2&#x27;</span>]</span><br><span class="line">    db2 = grads[<span class="string">&#x27;db2&#x27;</span>]    </span><br><span class="line">    <span class="comment"># Update rule for each parameter</span></span><br><span class="line">    W1 -= dW1 * learning_rate</span><br><span class="line">    b1 -= db1 * learning_rate</span><br><span class="line">    W2 -= dW2 * learning_rate</span><br><span class="line">    b2 -= db2 * learning_rate</span><br><span class="line">    parameters = &#123;<span class="string">&quot;W1&quot;</span>: W1, </span><br><span class="line">                  <span class="string">&quot;b1&quot;</span>: b1,            </span><br><span class="line">                  <span class="string">&quot;W2&quot;</span>: W2,   </span><br><span class="line">                  <span class="string">&quot;b2&quot;</span>: b2&#125;    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>æœ€ç»ˆçš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼š</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">nn_model</span>(<span class="params">X, Y, n_h, num_iterations = <span class="number">10000</span>, print_cost=<span class="literal">False</span></span>):</span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>]</span><br><span class="line">    n_y = layer_sizes(X, Y)[<span class="number">2</span>]    </span><br><span class="line">    <span class="comment"># Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: &quot;n_x, n_h, n_y&quot;. Outputs = &quot;W1, b1, W2, b2, parameters&quot;.</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">    W1 = parameters[<span class="string">&#x27;W1&#x27;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&#x27;b1&#x27;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&#x27;W2&#x27;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&#x27;b2&#x27;</span>]    </span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_iterations):        </span><br><span class="line">    <span class="comment"># Forward propagation. Inputs: &quot;X, parameters&quot;. Outputs: &quot;A2, cache&quot;.</span></span><br><span class="line">        A2, cache = forward_propagation(X, parameters)        </span><br><span class="line">        <span class="comment"># Cost function. Inputs: &quot;A2, Y, parameters&quot;. Outputs: &quot;cost&quot;.</span></span><br><span class="line">        cost = compute_cost(A2, Y, parameters)        </span><br><span class="line">        <span class="comment"># Backpropagation. Inputs: &quot;parameters, cache, X, Y&quot;. Outputs: &quot;grads&quot;.</span></span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)        </span><br><span class="line">        <span class="comment"># Gradient descent parameter update. Inputs: &quot;parameters, grads&quot;. Outputs: &quot;parameters&quot;.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate=<span class="number">1.2</span>)        </span><br><span class="line">        <span class="comment"># Print the cost every 1000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:            </span><br><span class="line">            <span class="built_in">print</span> (<span class="string">&quot;Cost after iteration %i: %f&quot;</span> %(i, cost))    </span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;ä¸€ã€ç›®æ ‡&quot;&gt;&lt;a href=&quot;#ä¸€ã€ç›®æ ‡&quot; class=&quot;headerlink&quot; title=&quot;ä¸€ã€ç›®æ ‡&quot;&gt;&lt;/a&gt;ä¸€ã€ç›®æ ‡&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;æ¨å¯¼å…·æœ‰å•éšå±‚çš„ç¥ç»ç½‘ç»œçš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ç®—æ³•ï¼Œå¹¶è¿›è¡Œç¼–ç¨‹ï¼ˆå¯ä»¥ä½¿ç”¨&lt;code&gt;sklearn&lt;/code&gt;ä¸­çš„ç¥ç»ç½‘ç»œï¼‰ã€‚&lt;ul&gt;
&lt;li&gt;æ¢è®¨10ï¼Œ30ï¼Œ100ï¼Œ300ï¼Œ1000ï¼Œä¸åŒéšè—èŠ‚ç‚¹æ•°å¯¹ç½‘ç»œæ€§èƒ½çš„å½±å“ã€‚&lt;/li&gt;
&lt;li&gt;æ¢è®¨ä¸åŒå­¦ä¹ ç‡å’Œè¿­ä»£æ¬¡æ•°å¯¹ç½‘ç»œæ€§èƒ½çš„å½±å“ã€‚&lt;/li&gt;
&lt;li&gt;æ”¹å˜æ•°æ®çš„æ ‡å‡†åŒ–æ–¹æ³•ï¼Œæ¢è®¨å¯¹è®­ç»ƒçš„å½±å“ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;æŸ¥é˜…èµ„æ–™è¯´æ˜ä»€ä¹ˆæ˜¯&lt;code&gt;Hebb&lt;/code&gt;å­¦ä¹ è§„åˆ™&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="æœºå™¨å­¦ä¹ " scheme="https://enblog.crocodilezs.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="ç¥ç»ç½‘ç»œ" scheme="https://enblog.crocodilezs.top/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Findsç®—æ³•å’ŒID3ç®—æ³•</title>
    <link href="https://enblog.crocodilezs.top/201910/FINDS%E7%AE%97%E6%B3%95%E5%92%8CID3%E7%AE%97%E6%B3%95/"/>
    <id>https://enblog.crocodilezs.top/201910/FINDS%E7%AE%97%E6%B3%95%E5%92%8CID3%E7%AE%97%E6%B3%95/</id>
    <published>2019-10-28T04:21:10.000Z</published>
    <updated>2022-05-23T17:10:07.174Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ä½œä¸šè¦æ±‚"><a href="#ä½œä¸šè¦æ±‚" class="headerlink" title="ä½œä¸šè¦æ±‚"></a>ä½œä¸šè¦æ±‚</h2><ol><li>å®ç°<code>FINDS</code>ç®—æ³•</li><li>å®ç°<code>ID3</code>ç®—æ³•</li></ol><ul><li>ä¸è¦è°ƒåº“è‡ªå·±å†™ã€‚å¦‚æœæœ‰èƒ½åŠ›å¯ä»¥ç»§ç»­ç”¨è¯¾ä»¶é‡Œçš„æ•°æ®é›†æµ‹è¯•ä¸¤ä¸ªç®—æ³•ï¼ˆç”¨å¤©æ°”çš„4æ¡è®°å½•æµ‹è¯•<code>FINDS</code>ï¼Œç”¨è´·æ¬¾çš„15æ¡è®°å½•æµ‹è¯•<code>ID3</code>ï¼‰ç»™å‡ºè®­ç»ƒè¯¯å·®æµ‹è¯•è¯¯å·®ç­‰ï¼›  </li><li>å†æœ‰èƒ½åŠ›å¯ä»¥ä½¿ç”¨æ›´å¤§çš„æ•°æ®é›†æµ‹è¯•ç®—æ³•ã€‚</li></ul><span id="more"></span><h2 id="ç®—æ³•å®ç°"><a href="#ç®—æ³•å®ç°" class="headerlink" title="ç®—æ³•å®ç°"></a>ç®—æ³•å®ç°</h2><h3 id="FINDSç®—æ³•"><a href="#FINDSç®—æ³•" class="headerlink" title="FINDSç®—æ³•"></a><code>FINDS</code>ç®—æ³•</h3><ol><li>ç›®æ ‡ï¼šå¯»æ‰¾æå¤§ç‰¹æ®Šå‡è®¾ã€‚</li><li>ä»å‡è®¾é›†åˆHä¸­æœ€ç‰¹æ®Šçš„å‡è®¾å¼€å§‹ã€‚åœ¨è¯¥å‡è®¾ä¸èƒ½æ­£ç¡®åœ°åˆ’åˆ†ä¸€ä¸ªæ­£ä¾‹çš„æ—¶å€™å°†å…¶è¿›è¡Œä¸€èˆ¬åŒ–ã€‚ç®—æ³•å¦‚ä¸‹ï¼š<br><img src="https://s2.ax1x.com/2019/10/22/K3ymOH.jpg" alt="ç®—æ³•æµç¨‹"></li><li><code>FINDS</code>ç®—æ³•æ˜¯ä¸€ç§åˆ©ç”¨<code>more-general-than</code>çš„ååºç»“æ„æ¥æœç´¢å‡è®¾ç©ºé—´çš„æ–¹æ³•ï¼Œè¿™ä¸€æœç´¢æ²¿ç€ååºé“¾ï¼Œä»è¾ƒç‰¹æ®Šçš„å‡è®¾é€æ¸æ¼”å˜ä¸ºè¾ƒä¸€èˆ¬çš„å‡è®¾ã€‚</li><li>ç®—æ³•<code>Python</code>å®ç°ï¼š</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string"> Created on 2019/10/21 21:02</span></span><br><span class="line"><span class="string"> FINDS</span></span><br><span class="line"><span class="string"> @Author  : Zhouy</span></span><br><span class="line"><span class="string"> @Blog    : www.crocodilezs.top</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create dataset</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">CreateDataset</span>():</span><br><span class="line">    dataset = [[<span class="string">&#x27;Sunny&#x27;</span>, <span class="string">&#x27;Warm&#x27;</span>, <span class="string">&#x27;Normal&#x27;</span>, <span class="string">&#x27;Strong&#x27;</span>, <span class="string">&#x27;Warm&#x27;</span>, <span class="string">&#x27;Same&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>],</span><br><span class="line">               [<span class="string">&#x27;Sunny&#x27;</span>, <span class="string">&#x27;Warm&#x27;</span>, <span class="string">&#x27;High&#x27;</span>, <span class="string">&#x27;Strong&#x27;</span>, <span class="string">&#x27;Warm&#x27;</span>, <span class="string">&#x27;Same&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>],</span><br><span class="line">               [<span class="string">&#x27;Rainy&#x27;</span>, <span class="string">&#x27;Cold&#x27;</span>, <span class="string">&#x27;High&#x27;</span>, <span class="string">&#x27;Strong&#x27;</span>, <span class="string">&#x27;Warm&#x27;</span>, <span class="string">&#x27;Change&#x27;</span>, <span class="string">&#x27;No&#x27;</span>],</span><br><span class="line">               [<span class="string">&#x27;Sunny&#x27;</span>, <span class="string">&#x27;Warm&#x27;</span>, <span class="string">&#x27;High&#x27;</span>, <span class="string">&#x27;Strong&#x27;</span>, <span class="string">&#x27;Cold&#x27;</span>, <span class="string">&#x27;Change&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>]]</span><br><span class="line">    labels = [<span class="string">&#x27;Sky&#x27;</span>, <span class="string">&#x27;Temp&#x27;</span>, <span class="string">&#x27;Humidity&#x27;</span>, <span class="string">&#x27;Wind&#x27;</span>, <span class="string">&#x27;Water&#x27;</span>, <span class="string">&#x27;Forest&#x27;</span>, <span class="string">&#x27;OutdoorSport&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> dataset, labels</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find one version space by using FINDS</span></span><br><span class="line"><span class="comment"># &#x27;/&#x27; means null, and &#x27;*&#x27; means generalization</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">FINDS</span>(<span class="params">dataset</span>):</span><br><span class="line">    constraint = [<span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;/&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> dataset:</span><br><span class="line">        <span class="keyword">if</span> item[-<span class="number">1</span>] == <span class="string">&#x27;Yes&#x27;</span>:</span><br><span class="line">            <span class="comment"># only go through positive instances</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(item)-<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span>(item[i] != constraint[i] <span class="keyword">and</span> constraint[i] != <span class="string">&#x27;*&#x27;</span>):</span><br><span class="line">                    <span class="keyword">if</span>(constraint[i] == <span class="string">&#x27;/&#x27;</span>):</span><br><span class="line">                        constraint[i] = item[i]</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        constraint[i] = <span class="string">&#x27;*&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> constraint</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    dataset, labels = CreateDataset()</span><br><span class="line">    constraint = FINDS(dataset)</span><br><span class="line">    <span class="built_in">print</span>(constraint)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h3 id="ID3ç®—æ³•"><a href="#ID3ç®—æ³•" class="headerlink" title="ID3ç®—æ³•"></a><code>ID3</code>ç®—æ³•</h3><ol><li>å†³ç­–æ ‘ï¼šå†³ç­–æ ‘æ˜¯ä¸€ç§å¸¸ç”¨çš„åˆ†ç±»ä¸å›å½’æ–¹æ³•ã€‚å†³ç­–æ ‘çš„æ¨¡å‹ä¸ºæ ‘å½¢ç»“æ„ï¼Œåœ¨é’ˆå¯¹åˆ†ç±»é—®é¢˜æ—¶ï¼Œå®é™…ä¸Šå°±æ˜¯é’ˆå¯¹è¾“å…¥æ•°æ®çš„å„ä¸ªç‰¹å¾å¯¹å®ä¾‹è¿›è¡Œåˆ†ç±»çš„è¿‡ç¨‹ï¼Œå³é€šè¿‡æ ‘å½¢ç»“æ„çš„æ¨¡å‹ï¼Œåœ¨æ¯ä¸€å±‚çº§ä¸Šå¯¹ç‰¹å¾å€¼è¿›è¡Œåˆ¤æ–­ï¼Œè¿›è€Œåˆ°è¾¾å†³ç­–æ ‘å¶å­èŠ‚ç‚¹ï¼Œå³å®Œæˆåˆ†ç±»è¿‡ç¨‹ã€‚<br><strong>å†³ç­–æ ‘çš„æœ¬è´¨æ˜¯æ¦‚å¿µå­¦ä¹ ã€‚</strong></li><li><p>ä¿¡æ¯ç†µï¼ˆé¦™æµ“ç†µï¼‰ã€æ¡ä»¶ç†µå’Œä¿¡æ¯å¢ç›Šçš„æ¦‚å¿µ</p><ul><li>ä¿¡æ¯é‡ï¼šä¸€ä»¶äº‹å‘ç”Ÿçš„æ¦‚ç‡è¶Šå°ï¼Œæˆ‘ä»¬è¯´å®ƒæ‰€è•´å«çš„ä¿¡æ¯é‡è¶Šå¤§ã€‚<br><img src="https://s2.ax1x.com/2019/10/22/K36VNq.jpg" alt="ä¿¡æ¯é‡"></li><li>ä¿¡æ¯ç†µï¼šä¿¡æ¯ç†µå°±æ˜¯æ‰€æœ‰å¯èƒ½å‘ç”Ÿçš„äº‹ä»¶çš„ä¿¡æ¯é‡çš„æœŸæœ›<br><img src="https://s2.ax1x.com/2019/10/22/K36EEn.jpg" alt="ä¿¡æ¯ç†µ"></li><li>æ¡ä»¶ç†µï¼šè¡¨ç¤ºåœ¨Xç»™å®šæ¡ä»¶ä¸‹ï¼ŒYçš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒçš„ç†µå¯¹Xçš„æ•°å­¦æœŸæœ›ã€‚<br>![æ¡ä»¶ç†µ(<a href="https://s2.ax1x.com/2019/10/22/K36FBj.jpg">https://s2.ax1x.com/2019/10/22/K36FBj.jpg</a>)</li><li>ä¿¡æ¯å¢ç›Šï¼šå½“æˆ‘ä»¬ç”¨å¦ä¸€ä¸ªå˜é‡Xå¯¹åŸå˜é‡Yåˆ†ç±»åï¼ŒåŸå˜é‡Yçš„ä¸ç¡®å®šæ€§å°±ä¼šå‡å°äº†(å³ç†µå€¼å‡å°)ã€‚è€Œç†µå°±æ˜¯ä¸ç¡®å®šæ€§ï¼Œä¸ç¡®å®šç¨‹åº¦å‡å°‘äº†å¤šå°‘å…¶å®å°±æ˜¯ä¿¡æ¯å¢ç›Šã€‚è¿™å°±æ˜¯ä¿¡æ¯å¢ç›Šçš„ç”±æ¥ï¼Œæ‰€ä»¥ä¿¡æ¯å¢ç›Šå®šä¹‰å¦‚ä¸‹ï¼š<br><img src="https://s2.ax1x.com/2019/10/22/K36kHs.jpg" alt="ä¿¡æ¯å¢ç›Š"></li></ul></li><li><p>ç®—æ³•â€™pythonâ€™å®ç°:<br>(ç”¨è¯¾ä»¶ä¸Šçš„è´·æ¬¾æ•°æ®é›†ä¸€ç›´æ²¡æ³•æˆåŠŸåˆ†ç±»ï¼Œäºæ˜¯å‚è€ƒäº†csdnåšå®¢çš„å¦ä¸€ä¸ªæ•°æ®é›†åˆä»£ç )</p></li></ol><p><code>myTrees.py</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string"> Created on 2019/10/22 11:59</span></span><br><span class="line"><span class="string"> myTrees</span></span><br><span class="line"><span class="string"> @Author  : Zhouy</span></span><br><span class="line"><span class="string"> @Blog    : www.crocodilezs.top</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># åŸå§‹æ•°æ®</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createDataSet</span>():</span><br><span class="line">    dataSet = [[<span class="number">1</span>, <span class="number">1</span>, <span class="string">&#x27;yes&#x27;</span>],</span><br><span class="line">               [<span class="number">1</span>, <span class="number">1</span>, <span class="string">&#x27;yes&#x27;</span>],</span><br><span class="line">               [<span class="number">1</span>, <span class="number">0</span>, <span class="string">&#x27;no&#x27;</span>],</span><br><span class="line">               [<span class="number">0</span>, <span class="number">1</span>, <span class="string">&#x27;no&#x27;</span>],</span><br><span class="line">               [<span class="number">0</span>, <span class="number">1</span>, <span class="string">&#x27;no&#x27;</span>]]</span><br><span class="line">    labels = [<span class="string">&#x27;no surfacing&#x27;</span>,<span class="string">&#x27;flippers&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> dataSet, labels</span><br><span class="line"></span><br><span class="line"><span class="comment"># å¤šæ•°è¡¨å†³å™¨</span></span><br><span class="line"><span class="comment"># åˆ—ä¸­ç›¸åŒå€¼æ•°é‡æœ€å¤šä¸ºç»“æœ</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">majorityCnt</span>(<span class="params">classList</span>):</span><br><span class="line">    classCounts = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> classList:</span><br><span class="line">        <span class="keyword">if</span> (value <span class="keyword">not</span> <span class="keyword">in</span> classCounts.keys()):</span><br><span class="line">            classCounts[value] = <span class="number">0</span></span><br><span class="line">        classCounts[value] += <span class="number">1</span></span><br><span class="line">    sortedClassCount = <span class="built_in">sorted</span>(classCounts.iteritems(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># åˆ’åˆ†æ•°æ®é›†</span></span><br><span class="line"><span class="comment"># dataSet:åŸå§‹æ•°æ®é›†</span></span><br><span class="line"><span class="comment"># axis:è¿›è¡Œåˆ†å‰²çš„æŒ‡å®šåˆ—ç´¢å¼•</span></span><br><span class="line"><span class="comment"># value:æŒ‡å®šåˆ—ä¸­çš„å€¼</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">splitDataSet</span>(<span class="params">dataSet, axis, value</span>):</span><br><span class="line">    retDataSet = []</span><br><span class="line">    <span class="keyword">for</span> featDataVal <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">if</span> featDataVal[axis] == value:</span><br><span class="line">            <span class="comment"># ä¸‹é¢ä¸¤è¡Œå»é™¤æŸä¸€é¡¹æŒ‡å®šåˆ—çš„å€¼ï¼Œå¾ˆå·§å¦™æœ‰æ²¡æœ‰</span></span><br><span class="line">            reducedFeatVal = featDataVal[:axis]</span><br><span class="line">            reducedFeatVal.extend(featDataVal[axis + <span class="number">1</span>:])</span><br><span class="line">            retDataSet.append(reducedFeatVal)</span><br><span class="line">    <span class="keyword">return</span> retDataSet</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># è®¡ç®—é¦™å†œç†µ</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcShannonEnt</span>(<span class="params">dataSet</span>):</span><br><span class="line">    <span class="comment"># æ•°æ®é›†æ€»é¡¹æ•°</span></span><br><span class="line">    numEntries = <span class="built_in">len</span>(dataSet)</span><br><span class="line">    <span class="comment"># æ ‡ç­¾è®¡æ•°å¯¹è±¡åˆå§‹åŒ–</span></span><br><span class="line">    labelCounts = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> featDataVal <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="comment"># è·å–æ•°æ®é›†æ¯ä¸€é¡¹çš„æœ€åä¸€åˆ—çš„æ ‡ç­¾å€¼</span></span><br><span class="line">        currentLabel = featDataVal[-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># å¦‚æœå½“å‰æ ‡ç­¾ä¸åœ¨æ ‡ç­¾å­˜å‚¨å¯¹è±¡é‡Œï¼Œåˆ™åˆå§‹åŒ–ï¼Œç„¶åè®¡æ•°</span></span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():</span><br><span class="line">            labelCounts[currentLabel] = <span class="number">0</span></span><br><span class="line">        labelCounts[currentLabel] += <span class="number">1</span></span><br><span class="line">    <span class="comment"># ç†µåˆå§‹åŒ–</span></span><br><span class="line">    shannonEnt = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># éå†æ ‡ç­¾å¯¹è±¡ï¼Œæ±‚æ¦‚ç‡ï¼Œè®¡ç®—ç†µ</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts.keys():</span><br><span class="line">        prop = labelCounts[key] / <span class="built_in">float</span>(numEntries)</span><br><span class="line">        shannonEnt -= prop * log(prop, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> shannonEnt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># é€‰å‡ºæœ€ä¼˜ç‰¹å¾åˆ—ç´¢å¼•</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chooseBestFeatureToSplit</span>(<span class="params">dataSet</span>):</span><br><span class="line">    <span class="comment"># è®¡ç®—ç‰¹å¾ä¸ªæ•°ï¼ŒdataSetæœ€åä¸€åˆ—æ˜¯æ ‡ç­¾å±æ€§ï¼Œä¸æ˜¯ç‰¹å¾é‡</span></span><br><span class="line">    numFeatures = <span class="built_in">len</span>(dataSet[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">    <span class="comment"># è®¡ç®—åˆå§‹æ•°æ®é¦™å†œç†µ</span></span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)</span><br><span class="line">    <span class="comment"># åˆå§‹åŒ–ä¿¡æ¯å¢ç›Šï¼Œæœ€ä¼˜åˆ’åˆ†ç‰¹å¾åˆ—ç´¢å¼•</span></span><br><span class="line">    bestInfoGain = <span class="number">0.0</span></span><br><span class="line">    bestFeatureIndex = -<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numFeatures):</span><br><span class="line">        <span class="comment"># è·å–æ¯ä¸€åˆ—æ•°æ®</span></span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">        <span class="comment"># å°†æ¯ä¸€åˆ—æ•°æ®å»é‡</span></span><br><span class="line">        uniqueVals = <span class="built_in">set</span>(featList)</span><br><span class="line">        newEntropy = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">            subDataSet = splitDataSet(dataSet, i, value)</span><br><span class="line">            <span class="comment"># è®¡ç®—æ¡ä»¶æ¦‚ç‡</span></span><br><span class="line">            prob = <span class="built_in">len</span>(subDataSet) / <span class="built_in">float</span>(<span class="built_in">len</span>(dataSet))</span><br><span class="line">            <span class="comment"># è®¡ç®—æ¡ä»¶ç†µ</span></span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataSet)</span><br><span class="line">        <span class="comment"># è®¡ç®—ä¿¡æ¯å¢ç›Š</span></span><br><span class="line">        infoGain = baseEntropy - newEntropy</span><br><span class="line">        <span class="keyword">if</span> (infoGain &gt; bestInfoGain):</span><br><span class="line">            bestInfoGain = infoGain</span><br><span class="line">            bestFeatureIndex = i</span><br><span class="line">    <span class="keyword">return</span> bestFeatureIndex</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># å†³ç­–æ ‘åˆ›å»º</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createTree</span>(<span class="params">dataSet, labels</span>):</span><br><span class="line">    <span class="comment"># è·å–æ ‡ç­¾å±æ€§ï¼ŒdataSetæœ€åä¸€åˆ—ï¼ŒåŒºåˆ«äºlabelsæ ‡ç­¾åç§°</span></span><br><span class="line">    classList = [example[-<span class="number">1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    <span class="comment"># æ ‘æç«¯ç»ˆæ­¢æ¡ä»¶åˆ¤æ–­</span></span><br><span class="line">    <span class="comment"># æ ‡ç­¾å±æ€§å€¼å…¨éƒ¨ç›¸åŒï¼Œè¿”å›æ ‡ç­¾å±æ€§ç¬¬ä¸€é¡¹å€¼</span></span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == <span class="built_in">len</span>(classList):</span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># åªæœ‰ä¸€ä¸ªç‰¹å¾ï¼ˆ1åˆ—ï¼‰</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(dataSet[<span class="number">0</span>]) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)</span><br><span class="line">    <span class="comment"># è·å–æœ€ä¼˜ç‰¹å¾åˆ—ç´¢å¼•</span></span><br><span class="line">    bestFeatureIndex = chooseBestFeatureToSplit(dataSet)</span><br><span class="line">    <span class="comment"># è·å–æœ€ä¼˜ç´¢å¼•å¯¹åº”çš„æ ‡ç­¾åç§°</span></span><br><span class="line">    bestFeatureLabel = labels[bestFeatureIndex]</span><br><span class="line">    <span class="comment"># åˆ›å»ºæ ¹èŠ‚ç‚¹</span></span><br><span class="line">    myTree = &#123;bestFeatureLabel: &#123;&#125;&#125;</span><br><span class="line">    <span class="comment"># å»é™¤æœ€ä¼˜ç´¢å¼•å¯¹åº”çš„æ ‡ç­¾åï¼Œä½¿labelsæ ‡ç­¾èƒ½æ­£ç¡®éå†</span></span><br><span class="line">    <span class="keyword">del</span> (labels[bestFeatureIndex])</span><br><span class="line">    <span class="comment"># è·å–æœ€ä¼˜åˆ—</span></span><br><span class="line">    bestFeature = [example[bestFeatureIndex] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    uniquesVals = <span class="built_in">set</span>(bestFeature)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniquesVals:</span><br><span class="line">        <span class="comment"># å­æ ‡ç­¾åç§°é›†åˆ</span></span><br><span class="line">        subLabels = labels[:]</span><br><span class="line">        <span class="comment"># é€’å½’</span></span><br><span class="line">        myTree[bestFeatureLabel][value] = createTree(splitDataSet(dataSet, bestFeatureIndex, value), subLabels)</span><br><span class="line">    <span class="keyword">return</span> myTree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># è·å–åˆ†ç±»ç»“æœ</span></span><br><span class="line"><span class="comment"># inputTree:å†³ç­–æ ‘å­—å…¸</span></span><br><span class="line"><span class="comment"># featLabels:æ ‡ç­¾åˆ—è¡¨</span></span><br><span class="line"><span class="comment"># testVec:æµ‹è¯•å‘é‡  ä¾‹å¦‚ï¼šç®€å•å®ä¾‹ä¸‹æŸä¸€è·¯å¾„ [1,1]  =&gt; yesï¼ˆæ ‘å¹²å€¼ç»„åˆï¼Œä»æ ¹ç»“ç‚¹åˆ°å¶å­èŠ‚ç‚¹ï¼‰</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">classify</span>(<span class="params">inputTree, featLabels, testVec</span>):</span><br><span class="line">    <span class="comment"># è·å–æ ¹ç»“ç‚¹åç§°ï¼Œå°†dictè½¬åŒ–ä¸ºlist</span></span><br><span class="line">    firstSide = <span class="built_in">list</span>(inputTree.keys())</span><br><span class="line">    <span class="comment"># æ ¹ç»“ç‚¹åç§°Stringç±»å‹</span></span><br><span class="line">    firstStr = firstSide[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># è·å–æ ¹ç»“ç‚¹å¯¹åº”çš„å­èŠ‚ç‚¹</span></span><br><span class="line">    secondDict = inputTree[firstStr]</span><br><span class="line">    <span class="comment"># è·å–æ ¹ç»“ç‚¹åç§°åœ¨æ ‡ç­¾åˆ—è¡¨ä¸­å¯¹åº”çš„ç´¢å¼•</span></span><br><span class="line">    featIndex = featLabels.index(firstStr)</span><br><span class="line">    <span class="comment"># ç”±ç´¢å¼•è·å–å‘é‡è¡¨ä¸­çš„å¯¹åº”å€¼</span></span><br><span class="line">    key = testVec[featIndex]</span><br><span class="line">    <span class="comment"># è·å–æ ‘å¹²å‘é‡åçš„å¯¹è±¡</span></span><br><span class="line">    valueOfFeat = secondDict[key]</span><br><span class="line">    <span class="comment"># åˆ¤æ–­æ˜¯å­ç»“ç‚¹è¿˜æ˜¯å¶å­èŠ‚ç‚¹ï¼šå­ç»“ç‚¹å°±å›è°ƒåˆ†ç±»å‡½æ•°ï¼Œå¶å­ç»“ç‚¹å°±æ˜¯åˆ†ç±»ç»“æœ</span></span><br><span class="line">    <span class="comment"># if type(valueOfFeat).__name__==&#x27;dict&#x27;: ç­‰ä»· if isinstance(valueOfFeat, dict):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(valueOfFeat, <span class="built_in">dict</span>):</span><br><span class="line">        classLabel = classify(valueOfFeat, featLabels, testVec)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        classLabel = valueOfFeat</span><br><span class="line">    <span class="keyword">return</span> classLabel</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># å°†å†³ç­–æ ‘åˆ†ç±»å™¨å­˜å‚¨åœ¨ç£ç›˜ä¸­ï¼Œfilenameä¸€èˆ¬ä¿å­˜ä¸ºtxtæ ¼å¼</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">storeTree</span>(<span class="params">inputTree, filename</span>):</span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fw = <span class="built_in">open</span>(filename, <span class="string">&#x27;wb+&#x27;</span>)</span><br><span class="line">    pickle.dump(inputTree, fw)</span><br><span class="line">    fw.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># å°†ç“·ç›˜ä¸­çš„å¯¹è±¡åŠ è½½å‡ºæ¥ï¼Œè¿™é‡Œçš„filenameå°±æ˜¯ä¸Šé¢å‡½æ•°ä¸­çš„txtæ–‡ä»¶</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">grabTree</span>(<span class="params">filename</span>):</span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fr = <span class="built_in">open</span>(filename, <span class="string">&#x27;rb&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> pickle.load(fr)</span><br></pre></td></tr></table></figure><p><code>treePlotter.py</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string"> Created on 2019/10/22 12:00</span></span><br><span class="line"><span class="string"> treePlotter</span></span><br><span class="line"><span class="string"> @Author  : Zhouy</span></span><br><span class="line"><span class="string"> @Blog    : www.crocodilezs.top</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">decisionNode = <span class="built_in">dict</span>(boxstyle=<span class="string">&quot;sawtooth&quot;</span>, fc=<span class="string">&quot;0.8&quot;</span>)</span><br><span class="line">leafNode = <span class="built_in">dict</span>(boxstyle=<span class="string">&quot;round4&quot;</span>, fc=<span class="string">&quot;0.8&quot;</span>)</span><br><span class="line">arrow_args = <span class="built_in">dict</span>(arrowstyle=<span class="string">&quot;&lt;-&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># è·å–æ ‘çš„å¶å­èŠ‚ç‚¹</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getNumLeafs</span>(<span class="params">myTree</span>):</span><br><span class="line">    numLeafs = <span class="number">0</span></span><br><span class="line">    <span class="comment"># dictè½¬åŒ–ä¸ºlist</span></span><br><span class="line">    firstSides = <span class="built_in">list</span>(myTree.keys())</span><br><span class="line">    firstStr = firstSides[<span class="number">0</span>]</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="comment"># åˆ¤æ–­æ˜¯å¦æ˜¯å¶å­èŠ‚ç‚¹ï¼ˆé€šè¿‡ç±»å‹åˆ¤æ–­ï¼Œå­ç±»ä¸å­˜åœ¨ï¼Œåˆ™ç±»å‹ä¸ºstrï¼›å­ç±»å­˜åœ¨ï¼Œåˆ™ä¸ºdictï¼‰</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(secondDict[</span><br><span class="line">                    key]).__name__ == <span class="string">&#x27;dict&#x27;</span>:  <span class="comment"># test to see if the nodes are dictonaires, if not they are leaf nodes</span></span><br><span class="line">            numLeafs += getNumLeafs(secondDict[key])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            numLeafs += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> numLeafs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># è·å–æ ‘çš„å±‚æ•°</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getTreeDepth</span>(<span class="params">myTree</span>):</span><br><span class="line">    maxDepth = <span class="number">0</span></span><br><span class="line">    <span class="comment"># dictè½¬åŒ–ä¸ºlist</span></span><br><span class="line">    firstSides = <span class="built_in">list</span>(myTree.keys())</span><br><span class="line">    firstStr = firstSides[<span class="number">0</span>]</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(secondDict[</span><br><span class="line">                    key]).__name__ == <span class="string">&#x27;dict&#x27;</span>:  <span class="comment"># test to see if the nodes are dictonaires, if not they are leaf nodes</span></span><br><span class="line">            thisDepth = <span class="number">1</span> + getTreeDepth(secondDict[key])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            thisDepth = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> thisDepth &gt; maxDepth: maxDepth = thisDepth</span><br><span class="line">    <span class="keyword">return</span> maxDepth</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotNode</span>(<span class="params">nodeTxt, centerPt, parentPt, nodeType</span>):</span><br><span class="line">    createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords=<span class="string">&#x27;axes fraction&#x27;</span>,</span><br><span class="line">                            xytext=centerPt, textcoords=<span class="string">&#x27;axes fraction&#x27;</span>,</span><br><span class="line">                            va=<span class="string">&quot;center&quot;</span>, ha=<span class="string">&quot;center&quot;</span>, bbox=nodeType, arrowprops=arrow_args)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotMidText</span>(<span class="params">cntrPt, parentPt, txtString</span>):</span><br><span class="line">    xMid = (parentPt[<span class="number">0</span>] - cntrPt[<span class="number">0</span>]) / <span class="number">2.0</span> + cntrPt[<span class="number">0</span>]</span><br><span class="line">    yMid = (parentPt[<span class="number">1</span>] - cntrPt[<span class="number">1</span>]) / <span class="number">2.0</span> + cntrPt[<span class="number">1</span>]</span><br><span class="line">    createPlot.ax1.text(xMid, yMid, txtString, va=<span class="string">&quot;center&quot;</span>, ha=<span class="string">&quot;center&quot;</span>, rotation=<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotTree</span>(<span class="params">myTree, parentPt, nodeTxt</span>):  <span class="comment"># if the first key tells you what feat was split on</span></span><br><span class="line">    numLeafs = getNumLeafs(myTree)  <span class="comment"># this determines the x width of this tree</span></span><br><span class="line">    depth = getTreeDepth(myTree)</span><br><span class="line">    firstSides = <span class="built_in">list</span>(myTree.keys())</span><br><span class="line">    firstStr = firstSides[<span class="number">0</span>]  <span class="comment"># the text label for this node should be this</span></span><br><span class="line">    cntrPt = (plotTree.xOff + (<span class="number">1.0</span> + <span class="built_in">float</span>(numLeafs)) / <span class="number">2.0</span> / plotTree.totalW, plotTree.yOff)</span><br><span class="line">    plotMidText(cntrPt, parentPt, nodeTxt)</span><br><span class="line">    plotNode(firstStr, cntrPt, parentPt, decisionNode)</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    plotTree.yOff = plotTree.yOff - <span class="number">1.0</span> / plotTree.totalD</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(secondDict[</span><br><span class="line">                    key]).__name__ == <span class="string">&#x27;dict&#x27;</span>:  <span class="comment"># test to see if the nodes are dictonaires, if not they are leaf nodes</span></span><br><span class="line">            plotTree(secondDict[key], cntrPt, <span class="built_in">str</span>(key))  <span class="comment"># recursion</span></span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># it&#x27;s a leaf node print the leaf node</span></span><br><span class="line">            plotTree.xOff = plotTree.xOff + <span class="number">1.0</span> / plotTree.totalW</span><br><span class="line">            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)</span><br><span class="line">            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, <span class="built_in">str</span>(key))</span><br><span class="line">    plotTree.yOff = plotTree.yOff + <span class="number">1.0</span> / plotTree.totalD</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># if you do get a dictonary you know it&#x27;s a tree, and the first element will be another dict</span></span><br><span class="line"><span class="comment"># ç»˜åˆ¶å†³ç­–æ ‘</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createPlot</span>(<span class="params">inTree</span>):</span><br><span class="line">    fig = plt.figure(<span class="number">1</span>, facecolor=<span class="string">&#x27;white&#x27;</span>)</span><br><span class="line">    fig.clf()</span><br><span class="line">    axprops = <span class="built_in">dict</span>(xticks=[], yticks=[])</span><br><span class="line">    createPlot.ax1 = plt.subplot(<span class="number">111</span>, frameon=<span class="literal">False</span>, **axprops)  <span class="comment"># no ticks</span></span><br><span class="line">    <span class="comment"># createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo puropses</span></span><br><span class="line">    plotTree.totalW = <span class="built_in">float</span>(getNumLeafs(inTree))</span><br><span class="line">    plotTree.totalD = <span class="built_in">float</span>(getTreeDepth(inTree))</span><br><span class="line">    plotTree.xOff = -<span class="number">0.5</span> / plotTree.totalW</span><br><span class="line">    plotTree.yOff = <span class="number">1.0</span></span><br><span class="line">    plotTree(inTree, (<span class="number">0.5</span>, <span class="number">1.0</span>), <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ç»˜åˆ¶æ ‘çš„æ ¹èŠ‚ç‚¹å’Œå¶å­èŠ‚ç‚¹ï¼ˆæ ¹èŠ‚ç‚¹å½¢çŠ¶ï¼šé•¿æ–¹å½¢ï¼Œå¶å­èŠ‚ç‚¹ï¼šæ¤­åœ†å½¢ï¼‰</span></span><br><span class="line"><span class="comment"># def createPlot():</span></span><br><span class="line"><span class="comment">#    fig = plt.figure(1, facecolor=&#x27;white&#x27;)</span></span><br><span class="line"><span class="comment">#    fig.clf()</span></span><br><span class="line"><span class="comment">#    createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo puropses</span></span><br><span class="line"><span class="comment">#    plotNode(&#x27;a decision node&#x27;, (0.5, 0.1), (0.1, 0.5), decisionNode)</span></span><br><span class="line"><span class="comment">#    plotNode(&#x27;a leaf node&#x27;, (0.8, 0.1), (0.3, 0.8), leafNode)</span></span><br><span class="line"><span class="comment">#    plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">retrieveTree</span>(<span class="params">i</span>):</span><br><span class="line">    listOfTrees = [&#123;<span class="string">&#x27;no surfacing&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;no&#x27;</span>, <span class="number">1</span>: &#123;<span class="string">&#x27;flippers&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;no&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;yes&#x27;</span>&#125;&#125;&#125;&#125;,</span><br><span class="line">                   &#123;<span class="string">&#x27;no surfacing&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;no&#x27;</span>, <span class="number">1</span>: &#123;<span class="string">&#x27;flippers&#x27;</span>: &#123;<span class="number">0</span>: &#123;<span class="string">&#x27;head&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;no&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;yes&#x27;</span>&#125;&#125;, <span class="number">1</span>: <span class="string">&#x27;no&#x27;</span>&#125;&#125;&#125;&#125;</span><br><span class="line">                   ]</span><br><span class="line">    <span class="keyword">return</span> listOfTrees[i]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># thisTree = retrieveTree(0)</span></span><br><span class="line"><span class="comment"># createPlot(thisTree)</span></span><br><span class="line"><span class="comment"># createPlot()</span></span><br><span class="line"><span class="comment"># myTree = retrieveTree(0)</span></span><br><span class="line"><span class="comment"># numLeafs =getNumLeafs(myTree)</span></span><br><span class="line"><span class="comment"># treeDepth =getTreeDepth(myTree)</span></span><br><span class="line"><span class="comment"># print(u&quot;å¶å­èŠ‚ç‚¹æ•°ç›®ï¼š%d&quot;% numLeafs)</span></span><br><span class="line"><span class="comment"># print(u&quot;æ ‘æ·±åº¦ï¼š%d&quot;%treeDepth)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>testTrees_3.py</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string"> Created on 2019/10/22 12:00</span></span><br><span class="line"><span class="string"> testTrees_3</span></span><br><span class="line"><span class="string"> @Author  : Zhouy</span></span><br><span class="line"><span class="string"> @Blog    : www.crocodilezs.top</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> myTrees <span class="keyword">as</span> mt</span><br><span class="line"><span class="keyword">import</span> treePlotter <span class="keyword">as</span> tp</span><br><span class="line"><span class="comment">#æµ‹è¯•</span></span><br><span class="line">dataSet, labels = mt.createDataSet()</span><br><span class="line"><span class="comment">#copyå‡½æ•°ï¼šæ–°å¼€è¾Ÿä¸€å—å†…å­˜ï¼Œç„¶åå°†listçš„æ‰€æœ‰å€¼å¤åˆ¶åˆ°æ–°å¼€è¾Ÿçš„å†…å­˜ä¸­</span></span><br><span class="line">labels1 = labels.copy()</span><br><span class="line"><span class="comment">#createTreeå‡½æ•°ä¸­å°†labels1çš„å€¼æ”¹å˜äº†ï¼Œæ‰€ä»¥åœ¨åˆ†ç±»æµ‹è¯•æ—¶ä¸èƒ½ç”¨labels1</span></span><br><span class="line">myTree = mt.createTree(dataSet,labels1)</span><br><span class="line"><span class="comment">#ä¿å­˜æ ‘åˆ°æœ¬åœ°</span></span><br><span class="line">mt.storeTree(myTree,<span class="string">&#x27;myTree.txt&#x27;</span>)</span><br><span class="line"><span class="comment">#åœ¨æœ¬åœ°ç£ç›˜è·å–æ ‘</span></span><br><span class="line">myTree = mt.grabTree(<span class="string">&#x27;myTree.txt&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">u&quot;å†³ç­–æ ‘ç»“æ„ï¼š%s&quot;</span>%myTree)</span><br><span class="line"><span class="comment">#ç»˜åˆ¶å†³ç­–æ ‘</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">u&quot;ç»˜åˆ¶å†³ç­–æ ‘ï¼š&quot;</span>)</span><br><span class="line">tp.createPlot(myTree)</span><br><span class="line">numLeafs =tp.getNumLeafs(myTree)</span><br><span class="line">treeDepth =tp.getTreeDepth(myTree)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">u&quot;å¶å­èŠ‚ç‚¹æ•°ç›®ï¼š%d&quot;</span>% numLeafs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">u&quot;æ ‘æ·±åº¦ï¼š%d&quot;</span>%treeDepth)</span><br><span class="line"><span class="comment">#æµ‹è¯•åˆ†ç±» ç®€å•æ ·æœ¬æ•°æ®3åˆ—</span></span><br><span class="line">labelResult =mt.classify(myTree,labels,[<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">u&quot;[1,1] æµ‹è¯•ç»“æœä¸ºï¼š%s&quot;</span>%labelResult)</span><br><span class="line">labelResult =mt.classify(myTree,labels,[<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">u&quot;[1,0] æµ‹è¯•ç»“æœä¸ºï¼š%s&quot;</span>%labelResult)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;ä½œä¸šè¦æ±‚&quot;&gt;&lt;a href=&quot;#ä½œä¸šè¦æ±‚&quot; class=&quot;headerlink&quot; title=&quot;ä½œä¸šè¦æ±‚&quot;&gt;&lt;/a&gt;ä½œä¸šè¦æ±‚&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;å®ç°&lt;code&gt;FINDS&lt;/code&gt;ç®—æ³•&lt;/li&gt;
&lt;li&gt;å®ç°&lt;code&gt;ID3&lt;/code&gt;ç®—æ³•&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;ä¸è¦è°ƒåº“è‡ªå·±å†™ã€‚å¦‚æœæœ‰èƒ½åŠ›å¯ä»¥ç»§ç»­ç”¨è¯¾ä»¶é‡Œçš„æ•°æ®é›†æµ‹è¯•ä¸¤ä¸ªç®—æ³•ï¼ˆç”¨å¤©æ°”çš„4æ¡è®°å½•æµ‹è¯•&lt;code&gt;FINDS&lt;/code&gt;ï¼Œç”¨è´·æ¬¾çš„15æ¡è®°å½•æµ‹è¯•&lt;code&gt;ID3&lt;/code&gt;ï¼‰ç»™å‡ºè®­ç»ƒè¯¯å·®æµ‹è¯•è¯¯å·®ç­‰ï¼›  &lt;/li&gt;
&lt;li&gt;å†æœ‰èƒ½åŠ›å¯ä»¥ä½¿ç”¨æ›´å¤§çš„æ•°æ®é›†æµ‹è¯•ç®—æ³•ã€‚&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="æœºå™¨å­¦ä¹ " scheme="https://enblog.crocodilezs.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="FindS" scheme="https://enblog.crocodilezs.top/tags/FindS/"/>
    
    <category term="ID3" scheme="https://enblog.crocodilezs.top/tags/ID3/"/>
    
  </entry>
  
  <entry>
    <title>è®¡ç®—æœºç³»ç»ŸåŸºç¡€å®éªŒä¸€ã€Linuxç¯å¢ƒå’ŒGCCå·¥å…·é“¾</title>
    <link href="https://enblog.crocodilezs.top/201910/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80%E5%AE%9E%E9%AA%8C%E4%B8%80/"/>
    <id>https://enblog.crocodilezs.top/201910/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80%E5%AE%9E%E9%AA%8C%E4%B8%80/</id>
    <published>2019-10-25T07:00:12.000Z</published>
    <updated>2022-05-23T17:10:06.177Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Linuxæ“ä½œç³»ç»Ÿæ¦‚è¿°å’Œå®éªŒç¯å¢ƒä»‹ç»"><a href="#Linuxæ“ä½œç³»ç»Ÿæ¦‚è¿°å’Œå®éªŒç¯å¢ƒä»‹ç»" class="headerlink" title="Linuxæ“ä½œç³»ç»Ÿæ¦‚è¿°å’Œå®éªŒç¯å¢ƒä»‹ç»"></a><code>Linux</code>æ“ä½œç³»ç»Ÿæ¦‚è¿°å’Œå®éªŒç¯å¢ƒä»‹ç»</h1><h2 id="æ“ä½œç³»ç»Ÿæ¦‚å¿µ"><a href="#æ“ä½œç³»ç»Ÿæ¦‚å¿µ" class="headerlink" title="æ“ä½œç³»ç»Ÿæ¦‚å¿µ"></a>æ“ä½œç³»ç»Ÿæ¦‚å¿µ</h2><p><code>OS</code>æ˜¯ç®¡ç†å’Œæ§åˆ¶è®¡ç®—æœºç¡¬ä»¶ä¸è½¯ä»¶èµ„æºçš„è®¡ç®—æœºç¨‹åºï¼Œæ˜¯ç›´æ¥åœ¨â€œè£¸æœºâ€ä¸Šçš„æœ€åŸºæœ¬çš„ç³»ç»Ÿè½¯ä»¶ã€‚</p><h2 id="Linuxçš„åº”ç”¨"><a href="#Linuxçš„åº”ç”¨" class="headerlink" title="Linuxçš„åº”ç”¨"></a><code>Linux</code>çš„åº”ç”¨</h2><ol><li>æœåŠ¡å™¨ç«¯ï¼šLinuxéå¸¸ç¨³å®šï¼Œç‰¹åˆ«é€‚åˆå¤§å‹ä¼ä¸šç”Ÿäº§ç¯å¢ƒã€‚</li><li>ä½œä¸ºç½‘ç»œå¹³å°çš„åç«¯æœåŠ¡å™¨è¢«ä½¿ç”¨ã€‚</li><li>ä½œä¸ºåº”ç”¨æœåŠ¡å™¨ã€æ•°æ®åº“æœåŠ¡å™¨è¢«ä½¿ç”¨ï¼šè§£å†³æµ·é‡æ•°æ®ã€é«˜å¹¶å‘çš„é—®é¢˜ï¼›</li><li>ä½œä¸ºåµŒå…¥å¼æ“ä½œç³»ç»Ÿè¢«ä½¿ç”¨ï¼šæ™ºèƒ½æ§åˆ¶ã€è‡ªåŠ¨åŒ–ã€ç‰©è”ç½‘ç­‰é¢†åŸŸã€‚</li></ol><span id="more"></span><h2 id="Linuxå†å²"><a href="#Linuxå†å²" class="headerlink" title="Linuxå†å²"></a><code>Linux</code>å†å²</h2><p>è¿½æº¯åˆ°<code>UNIX</code><br>ç®€å•åœ°è¯´ï¼Œ<code>Linux</code>æ˜¯å¯¹<code>UNIX</code>çš„é‡æ–°å®ç°ã€‚ä¸–ç•Œå„åœ°çš„<code>Linux</code>å¼€å‘äººå‘˜å€Ÿé‰´äº†<code>UNIX</code>çš„æŠ€æœ¯å’Œç”¨æˆ·ç•Œé¢ï¼Œå¹¶ä¸”èå…¥äº†å¾ˆå¤šç‹¬åˆ›çš„æŠ€æœ¯ã€‚<code>Linux</code>ä¸å±äº<code>BSD</code>å’Œ<code>AT&amp;T</code>é£æ ¼çš„<code>UNIX</code>ä¸­çš„ä»»ä½•ä¸€ç§ã€‚å› æ­¤ï¼Œä¸¥æ ¼æ¥è¯´ï¼Œ<code>Linux</code>æ˜¯æœ‰åˆ«äº<code>UNIX</code>çš„å¦ä¸€ç§æ“ä½œç³»ç»Ÿã€‚</p><h2 id="Linuxç®€ä»‹"><a href="#Linuxç®€ä»‹" class="headerlink" title="Linuxç®€ä»‹"></a><code>Linux</code>ç®€ä»‹</h2><p><code>Linux</code>å‘ç°è¡Œç‰ˆæœ¬ä¸¾ä¾‹ï¼š<code>Ubuntu</code>ã€<code>redhat</code></p><h2 id="æ“ä½œç³»ç»Ÿçš„ä¸‰ä¸ªéƒ¨åˆ†"><a href="#æ“ä½œç³»ç»Ÿçš„ä¸‰ä¸ªéƒ¨åˆ†" class="headerlink" title="æ“ä½œç³»ç»Ÿçš„ä¸‰ä¸ªéƒ¨åˆ†"></a>æ“ä½œç³»ç»Ÿçš„ä¸‰ä¸ªéƒ¨åˆ†</h2><h3 id="å†…æ ¸"><a href="#å†…æ ¸" class="headerlink" title="å†…æ ¸"></a>å†…æ ¸</h3><p>æ“ä½œç³»ç»Ÿäº”å¤§ç®¡ç†åŠŸèƒ½ä¸€èˆ¬éƒ½ç”±æ“ä½œç³»ç»Ÿå†…æ ¸è´Ÿè´£ã€‚</p><h3 id="å¤–å£³"><a href="#å¤–å£³" class="headerlink" title="å¤–å£³"></a>å¤–å£³</h3><ul><li>å¤–å£³ç¨‹åºè´Ÿè´£æ¥æ”¶ç”¨æˆ·æ“ä½œï¼Œæä¾›ä¸ ç”¨æˆ·çš„äº¤äº’ç•Œé¢ã€‚</li><li>ä¸€èˆ¬æ“ä½œç³»ç»Ÿæä¾›ç»™ç”¨æˆ·çš„ç•Œé¢ä¸»è¦æœ‰ä¸¤ç§ï¼šæ–‡æœ¬ç•Œé¢ã€<code>GUI</code>å›¾å½¢ç•Œé¢ã€‚<h3 id="ç®¡ç†å·¥å…·å’Œé™„å±è½¯ä»¶"><a href="#ç®¡ç†å·¥å…·å’Œé™„å±è½¯ä»¶" class="headerlink" title="ç®¡ç†å·¥å…·å’Œé™„å±è½¯ä»¶"></a>ç®¡ç†å·¥å…·å’Œé™„å±è½¯ä»¶</h3></li></ul><h2 id="æ“ä½œç³»ç»Ÿçš„åŠŸèƒ½"><a href="#æ“ä½œç³»ç»Ÿçš„åŠŸèƒ½" class="headerlink" title="æ“ä½œç³»ç»Ÿçš„åŠŸèƒ½"></a>æ“ä½œç³»ç»Ÿçš„åŠŸèƒ½</h2><ol><li><code>CPU</code>çš„æ§åˆ¶ä¸ç®¡ç†ï¼šå¤„ç†å™¨ç®¡ç†</li><li>å†…å­˜çš„åˆ†é…ä¸ç®¡ç†ï¼šå­˜å‚¨å™¨ç®¡ç†</li><li>å¤–éƒ¨è®¾å¤‡çš„æ§åˆ¶ä¸ç®¡ç†ï¼šè®¾å¤‡ç®¡ç†</li><li>æ–‡ä»¶ç®¡ç†</li><li>ä½œä¸šç®¡ç†å’Œæ§åˆ¶ï¼šç”¨æˆ·æ¥å£</li></ol><h2 id="Shell"><a href="#Shell" class="headerlink" title="Shell"></a><code>Shell</code></h2><ol><li>å¤–å£³ç¨‹åºå¯¹ç”¨æˆ·çš„è¾“å…¥å‘½ä»¤è¿›è¡Œè§£é‡Šï¼Œä¸ºç”¨æˆ·æä¾›ä¸€ç§é€šè¿‡æ“ä½œç³»ç»Ÿä½¿ç”¨è®¡ç®—æœºçš„æ“ä½œç¯å¢ƒã€‚</li><li><code>Windows</code>çš„å›¾å½¢ç•Œé¢ï¼Œç”±ä¸€ä¸ªæˆä¸º<code>Explorer</code>çš„æ¨¡å—è§£é‡Šç”¨æˆ·çš„è¾“å…¥ã€‚</li><li>å¦‚<code>DOS</code>çš„å‘½ä»¤è¡Œç•Œé¢ï¼Œ<code>Command.com</code>æ˜¯å¯¹å‘½ä»¤è¾“å…¥è¿›è¡Œè§£é‡Šçš„å¤–å£³ç¨‹åº(<code>Linux</code>çš„<code>Shell</code>)</li><li>Shellå‘½ä»¤ï¼šä»å‘½ä»¤è¡Œè¾“å…¥è¯­å¥ï¼Œæ¯è¾“å…¥ä¸€æ¬¡å°±èƒ½å¾—åˆ°ä¸€æ¬¡å“åº”ï¼Œè¿™äº›è¯­å¥å°±æ˜¯<code>shell</code>å‘½ä»¤ã€‚</li><li><code>Shell</code>ç¨‹åºï¼šåˆç§°<code>Shell</code>è„šæœ¬ã€‚ï¼ˆæŠŠä¸€ç³»åˆ—çš„<code>shell</code>å‘½ä»¤ï¼ŒæŒ‰ç…§ä¸€å®šçš„è¯­æ³•è§„åˆ™å’Œæ§åˆ¶ç»“æ„ï¼Œç»„ç»‡åœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­ï¼Œç„¶åç”±å†…æ ¸æ¥ä¸€æ¡æ¥ä¸€æ¡åœ°è§£é‡Šå’Œæ‰§è¡Œè¿™äº›å‘½ä»¤ï¼Œè¿™ä¸ªæ–‡ä»¶å°±æ˜¯shellç¨‹åºï¼Œç±»ä¼¼<code>DOS</code>/<code>Winsows</code>ä¸­çš„ã€‚batæ‰¹å¤„ç†æ–‡ä»¶ã€‚ï¼‰</li><li>[username@computername ~]$<br>user nameä¸ºå½“å‰ç”¨æˆ·åï¼Œcomputername ä¸ºå½“å‰è®¡ç®—æœºå ï¼Œ$è¡¨ç¤ºå½“å‰ç”¨æˆ·æ˜¯ä¸€èˆ¬ç”¨æˆ·ã€‚ <h2 id="ssh-secure-shell"><a href="#ssh-secure-shell" class="headerlink" title="ssh secure shell"></a><code>ssh</code> secure shell</h2>æŠŠ<code>Linux</code>ç»ˆç«¯æ¬åˆ°<code>Windows</code>ä¸‹ï¼Œè¿æ¥åˆ°BUPT1.<h2 id="Shellå¸¸ç”¨å‘½ä»¤"><a href="#Shellå¸¸ç”¨å‘½ä»¤" class="headerlink" title="Shellå¸¸ç”¨å‘½ä»¤"></a><code>Shell</code>å¸¸ç”¨å‘½ä»¤</h2><h3 id="ç›®å½•æ“ä½œå‘½ä»¤"><a href="#ç›®å½•æ“ä½œå‘½ä»¤" class="headerlink" title="ç›®å½•æ“ä½œå‘½ä»¤"></a>ç›®å½•æ“ä½œå‘½ä»¤</h3>ç›®å½•æ“ä½œå‘½ä»¤èƒ½å¤Ÿå¯¹å½“å‰çš„ç›®å½•è¿›è¡ŒæŸ¥çœ‹ã€åˆ›å»ºã€åˆ é™¤ï¼Œä»¥åŠæ˜¾ç¤ºå½“å‰å·¥ä½œç›®å½•å’Œæ”¹å˜å½“å‰ç›®å½•ç­‰æ“ä½œã€‚</li></ol><div note="class info">    1. /etc - ç³»ç»Ÿæ‰€éœ€çš„é‡è¦é…ç½®å’Œç®¡ç†æ–‡ä»¶<br />    2. /dev - å­˜æ”¾device fileï¼ˆè£…ç½®æ–‡ä»¶ï¼‰<br />    3. /boot - å­˜æ”¾ç³»ç»Ÿæ¿€æ´»çš„ç›¸å…³æ–‡ä»¶ï¼Œä¸å¯ä»»æ„åˆ é™¤ã€‚<br />    4. /home - ç™»é™†ç”¨æˆ·çš„ä¸»ç›®å½•<br />    5. /lib - å­˜æ”¾ç³»ç»Ÿæ¿€æ´»æ—¶éœ€è¦çš„ç³»ç»Ÿå‡½æ•°åº“<br />    6. /usr/lib - å­˜æ”¾ä¸€äº›åº”ç”¨ç¨‹åºçš„å…±äº«å‡½æ•°åº“<br />    7. /mnt - ç³»ç»Ÿé»˜è®¤çš„æŒ‚è½½ç‚¹(mount point)    8. /proc - è™šæ‹Ÿæ–‡ä»¶ç³»ç»Ÿï¼Œä¸å ç”¨ç¡¬ç›˜ç©ºé—´ï¼Œç›®å½•ä¸‹çš„æ–‡ä»¶å‡æ”¾ç½®äºå†…å­˜ä¸­<br />    9. /root - ç³»ç»Ÿç®¡ç†ç”¨æˆ·rootçš„ä¸»ç›®å½•<br />    10. /bin - å­˜æ”¾ä¸€äº›ç³»ç»Ÿå¯åŠ¨æ—¶æ‰€éœ€çš„æ™®é€šç¨‹åºå’Œç³»ç»Ÿç¨‹åº<br />    11. /tmp - å­˜æ”¾ä¸´æ—¶æ–‡ä»¶    12. /var - å­˜æ”¾è¢«ç³»ç»Ÿä¿®æ”¹è¿‡çš„æ•°æ®ã€‚</div><p>å¸¸ç”¨çš„ç›®å½•æ“ä½œå‘½ä»¤åŒ…æ‹¬ï¼š</p><ol><li>pwd æ‰“å°å½“å‰å·¥ä½œç›®å½•</li><li>cd æ”¹å˜å½“å‰æ‰€åœ¨ç›®å½•</li><li>ls æŸ¥çœ‹å½“å‰ç›®å½•ä¸‹çš„å†…å®¹</li><li>dir ç±»ä¼¼lså‘½ä»¤</li><li>mkdir åˆ›å»ºç›®å½•</li><li>rmdir åˆ é™¤ç©ºç›®å½•</li></ol><h3 id="æ–‡ä»¶æ“ä½œå‘½ä»¤"><a href="#æ–‡ä»¶æ“ä½œå‘½ä»¤" class="headerlink" title="æ–‡ä»¶æ“ä½œå‘½ä»¤"></a>æ–‡ä»¶æ“ä½œå‘½ä»¤</h3><ul><li>åœ¨å‘½ä»¤è¡Œç¯å¢ƒä¸‹å¯¹æ–‡ä»¶è¿›è¡Œæ“ä½œå°†æ¯”åœ¨å›¾å½¢ç¯å¢ƒä¸‹æ“ä½œæ–‡ä»¶æ›´åŠ å¿«æ·å’Œé«˜æ•ˆ</li><li>æ–‡ä»¶æ“ä½œä¸»è¦åŒ…æ‹¬ï¼šæœç´¢æ–‡ä»¶ã€å¤åˆ¶å’Œç§»åŠ¨æ–‡ä»¶ã€åˆ é™¤æ–‡ä»¶ä»¥åŠåˆå¹¶æ–‡ä»¶å†…å®¹</li></ul><p>å¸¸ç”¨æ–‡ä»¶æ“ä½œå‘½ä»¤ï¼š  </p><ul><li><code>cat</code>  </li><li><code>more</code>  </li><li><code>less</code>  </li><li><code>head</code></li><li><code>tail</code></li><li><code>cp</code></li><li><code>mv</code></li><li><code>rm</code></li><li><code>find</code></li><li><code>touch</code></li><li><code>ln</code></li></ul><h3 id="ä½¿ç”¨å¸®åŠ©å‘½ä»¤"><a href="#ä½¿ç”¨å¸®åŠ©å‘½ä»¤" class="headerlink" title="ä½¿ç”¨å¸®åŠ©å‘½ä»¤"></a>ä½¿ç”¨å¸®åŠ©å‘½ä»¤</h3><ol><li><code>man å‘½ä»¤å</code> </li><li><code>whatis å‘½ä»¤å</code>  </li><li><code>help å‘½ä»¤å</code>ï¼šé€‚ç”¨äºéƒ¨åˆ†å‘½ä»¤</li></ol><h2 id="Viç¼–è¾‘å™¨"><a href="#Viç¼–è¾‘å™¨" class="headerlink" title="Viç¼–è¾‘å™¨"></a>Viç¼–è¾‘å™¨</h2><h3 id="Viç®€ä»‹"><a href="#Viç®€ä»‹" class="headerlink" title="Viç®€ä»‹"></a><code>Vi</code>ç®€ä»‹</h3><ul><li><code>Vi</code>ç¼–è¾‘å™¨æ˜¯<code>Visual interface</code>çš„ç®€ç§°ï¼Œå®ƒå¯ä»¥æ‰§è¡Œè¾“å‡ºã€åˆ é™¤ã€æŸ¥æ‰¾ã€æ›¿æ¢ã€å—æ“ä½œç­‰ä¼—å¤šæ–‡æœ¬æ“ä½œ</li><li><code>Vi</code>ä¸æ˜¯ä¸€ä¸ªæ’ç‰ˆç¨‹åºï¼Œåªæ˜¯ä¸€ä¸ªæ–‡æœ¬ç¼–è¾‘ç¨‹åºã€‚</li><li>æ˜¯å…¨å±å¹•æ–‡æœ¬ç¼–è¾‘å™¨ï¼Œæ²¡æœ‰èœå•ï¼Œåªæœ‰å‘½ä»¤ã€‚</li></ul><h3 id="Viçš„åŸºæœ¬æ¦‚å¿µ"><a href="#Viçš„åŸºæœ¬æ¦‚å¿µ" class="headerlink" title="Viçš„åŸºæœ¬æ¦‚å¿µ"></a><code>Vi</code>çš„åŸºæœ¬æ¦‚å¿µ</h3><ol><li>å‘½ä»¤è¡Œæ¨¡å¼ï¼ˆcommand modeï¼‰<br>æ§åˆ¶å±å¹•å…‰æ ‡çš„ç§»åŠ¨ã€å­—ç¬¦ã€å­—æˆ–è¡Œçš„åˆ é™¤ã€ç§»åŠ¨å¤åˆ¶æŸåŒºæ®µåŠè¿›å…¥<code>Insert mode</code>ä¸‹ï¼Œæˆ–è€…åˆ°<code>last line mode</code>ã€‚</li><li>æ’å…¥æ¨¡å¼ï¼ˆInsert modeï¼‰<br>åªæœ‰åœ¨<code>Insert mode</code>ä¸‹ï¼Œæ‰å¯ä»¥åšæ–‡å­—è¾“å…¥ï¼ŒæŒ‰ESCé”®å¯å›åˆ°å‘½ä»¤è¡Œæ¨¡å¼ã€‚</li><li>åº•è¡Œæ¨¡å¼(last line mode)<br>å°†æ–‡ä»¶ä¿å­˜æˆ–é€€å‡ºviï¼Œä¹Ÿå¯ä»¥è®¾ç½®ç¼–è¾‘ç¯å¢ƒï¼Œå¦‚å¯»æ‰¾å­—ç¬¦ä¸²ã€åˆ—å‡ºè¡Œå·ã€‚</li></ol><div class="note info">    $ vi test.txt<br />    å³å¯è¿›å…¥viï¼ˆæ‰“å¼€æˆ–æ–°å»ºæ–‡ä»¶ï¼‰</div><p>æ“ä½œï¼š</p><ol><li>å‘½ä»¤è¡Œæ¨¡å¼ â€”-&gt;(i)  æ’å…¥æ¨¡å¼</li><li>æ’å…¥æ¨¡å¼  â€”-&gt;ï¼ˆESCï¼‰  å‘½ä»¤è¡Œæ¨¡å¼</li><li>å¦‚æœå¤„äºã€Œæ’å…¥æ¨¡å¼ã€ï¼Œå°±åªèƒ½ä¸€ç›´è¾“å…¥æ–‡å­—ï¼Œå¦‚æœå‘ç°è¾“é”™äº†å­—æƒ³ç”¨å…‰æ ‡å¾€å›ç§»åŠ¨å°†è¯¥å­—åˆ é™¤ï¼Œå°±å¾—å…ˆå›åˆ°ã€Œå‘½ä»¤è¡Œæ¨¡å¼ã€</li><li>åœ¨ã€Œå‘½ä»¤è¡Œæ¨¡å¼ã€ä¸‹ï¼ŒæŒ‰ä¸‹ï¼šè¿›å…¥åº•è¡Œæ¨¡å¼<br><code>: w filename</code><br><code>: wq</code><br><code>: q!</code></li></ol><h2 id="GCCå·¥å…·é“¾"><a href="#GCCå·¥å…·é“¾" class="headerlink" title="GCCå·¥å…·é“¾"></a><code>GCC</code>å·¥å…·é“¾</h2><h3 id="ç®€ä»‹"><a href="#ç®€ä»‹" class="headerlink" title="ç®€ä»‹"></a>ç®€ä»‹</h3><ol><li><code>GCC</code>ç¼–è¯‘å™¨èƒ½å°†<code>C</code>å’Œ<code>C++</code>è¯­è¨€æºç¨‹åºã€æ±‡ç¼–ç¨‹åºç¼–è¯‘ã€é“¾æ¥æˆå¯æ‰§è¡Œæ–‡ä»¶ã€‚</li><li>ä½¿ç”¨<code>GCC</code>ç¼–è¯‘å™¨æ—¶ï¼Œç¼–è¯‘è¿‡ç¨‹å¯ä»¥è¢«ç»†åˆ†ä¸ºå››ä¸ªé˜¶æ®µï¼š<ul><li>é¢„å¤„ç†(Pre-Processing)</li><li>ç¼–è¯‘(Compiling)</li><li>æ±‡ç¼–(Assembling)</li><li>é“¾æ¥(Linking)</li></ul></li></ol><h3 id="GDBçš„æ¦‚è¿°"><a href="#GDBçš„æ¦‚è¿°" class="headerlink" title="GDBçš„æ¦‚è¿°"></a><code>GDB</code>çš„æ¦‚è¿°</h3><p><code>GDB</code>æ˜¯ä¸€æ¬¾GNUå¼€å‘ç»„ç»‡å¹¶å‘å¸ƒçš„UNIX/Linuxä¸‹çš„ç¨‹åºè°ƒè¯•å·¥å…·ã€‚å®ƒä½¿ä½ èƒ½å¤Ÿåœ¨ç¨‹åºè¿è¡Œæ—¶è§‚å¯Ÿç¨‹åºçš„å†…éƒ¨ç»“æ„å’Œå†…å­˜çš„ä½¿ç”¨æƒ…å†µã€‚ä»¥ä¸‹æ˜¯<code>GDB</code>æä¾›çš„ä¸€äº›åŠŸèƒ½ï¼š</p><ol><li>ç›‘è§†ç¨‹åºä¸­å˜é‡çš„å€¼</li><li>è®¾ç½®æ–­ç‚¹ä»¥ä½¿ç¨‹åºåœ¨æŒ‡å®šçš„ä»£ç è¡Œä¸Šåœæ­¢è¿è¡Œ</li><li>èƒ½é€è¡Œæ‰§è¡Œä»£ç </li></ol><h2 id="Objdumpç®€ä»‹"><a href="#Objdumpç®€ä»‹" class="headerlink" title="Objdumpç®€ä»‹"></a><code>Objdump</code>ç®€ä»‹</h2><p><code>Objdump</code>æ˜¯ä»¥ä¸€ç§å¯é˜…è¯»çš„æ ¼å¼è®©ä½ æ›´å¤šåœ°äº†è§£äºŒè¿›åˆ¶æ–‡ä»¶å¯èƒ½å¸¦æœ‰åœ°é™„åŠ ä¿¡æ¯ã€‚<br>å¯¹äºæƒ³è¿›ä¸€æ­¥äº†è§£ç³»ç»Ÿåœ°ç¨‹åºå‘˜ï¼Œè¿™ä¸ªå‘½ä»¤æ²¡æœ‰æ²¡æœ‰æ›´å¤šæ„ä¹‰ï¼Œå¯¹äºæƒ³è¿›ä¸€æ­¥äº†è§£ç³»ç»Ÿçš„ç¨‹åºå‘˜ï¼Œåº”è¯¥æŒæ¡è¿™ç§å·¥å…·ï¼Œè‡³å°‘ä½ å¯ä»¥è‡ªå·±å†™å†™<code>shellcode</code>äº†ï¼Œæˆ–è€…çœ‹çœ‹äººå®¶ç»™çš„<code>exploit</code>ä¸­çš„<code>shellcode</code>æ˜¯ä»€ä¹ˆä¸œè¥¿ã€‚<br><strong>æŠŠCè¯­è¨€æºä»£ç ç¼–è¯‘é“¾æ¥ç”Ÿæˆçš„å¯æ‰§è¡Œç¨‹åºåæ±‡ç¼–åå¾—åˆ°å¯¹åº”çš„æ±‡ç¼–ä»£ç ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬ç†è§£Cè¯­è¨€å’Œæ±‡ç¼–è¯­è¨€ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚éå¸¸æœ‰åŠ©äºæ·±å…¥ç†è§£Cè¯­è¨€</strong></p><div class="note warning">    è‡³æ­¤ï¼Œå·²ç»å®Œæˆäº†è®¡ç®—æœºç³»ç»ŸåŸºç¡€ç¬¬ä¸€æ¬¡å®éªŒçš„ç†è®ºéƒ¨åˆ†ï¼Œå…¶ä¸­æœ‰å¤ªå¤šçš„ä¸œè¥¿è¿˜éœ€è¦è‡ªå·±å»å®è·µã€æ¥ä¸‹æ¥å¼€å§‹å®éªŒï¼</div>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Linuxæ“ä½œç³»ç»Ÿæ¦‚è¿°å’Œå®éªŒç¯å¢ƒä»‹ç»&quot;&gt;&lt;a href=&quot;#Linuxæ“ä½œç³»ç»Ÿæ¦‚è¿°å’Œå®éªŒç¯å¢ƒä»‹ç»&quot; class=&quot;headerlink&quot; title=&quot;Linuxæ“ä½œç³»ç»Ÿæ¦‚è¿°å’Œå®éªŒç¯å¢ƒä»‹ç»&quot;&gt;&lt;/a&gt;&lt;code&gt;Linux&lt;/code&gt;æ“ä½œç³»ç»Ÿæ¦‚è¿°å’Œå®éªŒç¯å¢ƒä»‹ç»&lt;/h1&gt;&lt;h2 id=&quot;æ“ä½œç³»ç»Ÿæ¦‚å¿µ&quot;&gt;&lt;a href=&quot;#æ“ä½œç³»ç»Ÿæ¦‚å¿µ&quot; class=&quot;headerlink&quot; title=&quot;æ“ä½œç³»ç»Ÿæ¦‚å¿µ&quot;&gt;&lt;/a&gt;æ“ä½œç³»ç»Ÿæ¦‚å¿µ&lt;/h2&gt;&lt;p&gt;&lt;code&gt;OS&lt;/code&gt;æ˜¯ç®¡ç†å’Œæ§åˆ¶è®¡ç®—æœºç¡¬ä»¶ä¸è½¯ä»¶èµ„æºçš„è®¡ç®—æœºç¨‹åºï¼Œæ˜¯ç›´æ¥åœ¨â€œè£¸æœºâ€ä¸Šçš„æœ€åŸºæœ¬çš„ç³»ç»Ÿè½¯ä»¶ã€‚&lt;/p&gt;
&lt;h2 id=&quot;Linuxçš„åº”ç”¨&quot;&gt;&lt;a href=&quot;#Linuxçš„åº”ç”¨&quot; class=&quot;headerlink&quot; title=&quot;Linuxçš„åº”ç”¨&quot;&gt;&lt;/a&gt;&lt;code&gt;Linux&lt;/code&gt;çš„åº”ç”¨&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;æœåŠ¡å™¨ç«¯ï¼šLinuxéå¸¸ç¨³å®šï¼Œç‰¹åˆ«é€‚åˆå¤§å‹ä¼ä¸šç”Ÿäº§ç¯å¢ƒã€‚&lt;/li&gt;
&lt;li&gt;ä½œä¸ºç½‘ç»œå¹³å°çš„åç«¯æœåŠ¡å™¨è¢«ä½¿ç”¨ã€‚&lt;/li&gt;
&lt;li&gt;ä½œä¸ºåº”ç”¨æœåŠ¡å™¨ã€æ•°æ®åº“æœåŠ¡å™¨è¢«ä½¿ç”¨ï¼šè§£å†³æµ·é‡æ•°æ®ã€é«˜å¹¶å‘çš„é—®é¢˜ï¼›&lt;/li&gt;
&lt;li&gt;ä½œä¸ºåµŒå…¥å¼æ“ä½œç³»ç»Ÿè¢«ä½¿ç”¨ï¼šæ™ºèƒ½æ§åˆ¶ã€è‡ªåŠ¨åŒ–ã€ç‰©è”ç½‘ç­‰é¢†åŸŸã€‚&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="è®¡ç®—æœºç³»ç»ŸåŸºç¡€" scheme="https://enblog.crocodilezs.top/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"/>
    
    
  </entry>
  
</feed>
