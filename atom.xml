<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yuyang&#39;s Blog</title>
  
  
  <link href="https://enblog.crocodilezs.top/atom.xml" rel="self"/>
  
  <link href="https://enblog.crocodilezs.top/"/>
  <updated>2022-05-23T17:36:55.902Z</updated>
  <id>https://enblog.crocodilezs.top/</id>
  
  <author>
    <name>CrocodileZS</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>博客内容归档和英文博客</title>
    <link href="https://enblog.crocodilezs.top/202111/%E5%8D%9A%E5%AE%A2%E5%86%85%E5%AE%B9%E5%BD%92%E6%A1%A3%E5%92%8C%E8%8B%B1%E6%96%87%E5%8D%9A%E5%AE%A2/"/>
    <id>https://enblog.crocodilezs.top/202111/%E5%8D%9A%E5%AE%A2%E5%86%85%E5%AE%B9%E5%BD%92%E6%A1%A3%E5%92%8C%E8%8B%B1%E6%96%87%E5%8D%9A%E5%AE%A2/</id>
    <published>2021-11-20T10:08:21.000Z</published>
    <updated>2022-05-23T17:36:55.902Z</updated>
    
    <content type="html"><![CDATA[<div class="tag-plugin note" ><div class="body"><p>从2019年开始尝试搭建自己的博客，到现在已经本科毕业。这三年的时间虽然没有坚持更新博客，但是一直在坚持记录。无论是理论知识的学习、Debug的记录，还是读书健身笔记和感慨万千时的随笔，他们都散落在各种在线笔记软件和手机的备忘录中。</p><p>随着时间的推移，我对博客的认识也发生了很多变化。“记录”对我来说仍然具有非常大的意义，但是“博客”最重要的价值不应该是“记录”，而是“分享”——传递有价值的信息。我把平时的日常生活和学习经历记录在各种笔记中，过一段时间再去回顾和整理，选取其中有价值的东西去分享。</p><p>过去的一年再次参加了美赛，加入了几个和区块链有关的项目，也完成了不少专利和论文。更多的时间忙于运用知识，以需求为导向迫使自己进行广泛而不深刻的学习。对我来说真正有价值的输入变少了，输出和表达的欲望也降低了很多。</p><p>   我在原先的博客域名之下又建立了一个英文博客<a href="https://blog.crocodilezs.top/eysblog_en/">BLOG-EN</a>，旨在整理自己过去一年经历的各种项目。同时也将原博客中的内容进行整理归档，便于自己和访客的查阅。</p></div></div><span id="more"></span><h2 id="课程设计与实验"><a href="#课程设计与实验" class="headerlink" title="课程设计与实验"></a>课程设计与实验</h2><p><a href="/201911/Linux开发环境及应用作业%2020191031/" itemprop="url"> Linux文本处理作业 </a> &emsp;&emsp;<br><a href="/categories/操作系统/" itemprop="url"> 《30天自制操作系统》实验合辑 </a> &emsp;&emsp;<br><a href="/201909/周宇洋_「学生宿舍管理系统」实验报告/" itemprop="url"> 学生宿舍管理系统Python开发 </a> &emsp;&emsp;<br><a href="/201911/KNN与Naive_Bayes代码实现/" itemprop="url"> KNN和朴素贝叶斯的代码实现 </a> &emsp;&emsp;<br><a href="/201911/Price_Suggestion_Chanllenge/" itemprop="url"> 商品价格预测挑战 </a> &emsp;&emsp;<br><a href="/201911/Fisher算法&SVM&K-Means及其优化/" itemprop="url"> Fisher算法 &amp; SVM &amp; K-Means的实现和优化 </a> &emsp;&emsp;<br><a href="/201910/FINDS算法和ID3算法/" itemprop="url"> FINDS算法和ID3算法 </a> &emsp;&emsp;<br><a href="/201904/插入排序归并排序和快速排序/" itemprop="url"> 算法设计之排序 </a> &emsp;&emsp;<br><a href="/201904/循环赛赛程安排/" itemprop="url"> 算法设计之循环赛赛程安排 </a> &emsp;&emsp;</p><h2 id="展示和汇报"><a href="#展示和汇报" class="headerlink" title="展示和汇报"></a>展示和汇报</h2><p><a href="/201904/基于链接内容的社区发现（一）/" itemprop="url"> 基于链接内容的社区发现（一） </a> &emsp;&emsp;<br><a href="/201904/基于链接内容的社区发现（二）/" itemprop="url"> 基于链接内容的社区发现（二） </a> &emsp;&emsp;</p><h2 id="学习笔记"><a href="#学习笔记" class="headerlink" title="学习笔记"></a>学习笔记</h2><p><a href="/201908/「迁移学习简明手册」学习笔记（1）/" itemprop="url"> 《迁移学习简明手册》学习笔记 </a> &emsp;&emsp;<br><a href="/201909/实验室苦逼搬砖暑假生活纪实/" itemprop="url"> 用户对齐（实验室搬砖纪实） </a> &emsp;&emsp;</p><h2 id="读书笔记"><a href="#读书笔记" class="headerlink" title="读书笔记"></a>读书笔记</h2><p><a href="/202005/《苏东坡传》摘录/" itemprop="url"> 《苏东坡传》 </a> &emsp;&emsp;<br><a href="/202002/祭亡妻程氏文/" itemprop="url"> 《祭亡妻程氏文》 </a> &emsp;&emsp; </p>]]></content>
    
    
    <summary type="html">&lt;div class=&quot;tag-plugin note&quot; &gt;&lt;div class=&quot;body&quot;&gt;&lt;p&gt;从2019年开始尝试搭建自己的博客，到现在已经本科毕业。这三年的时间虽然没有坚持更新博客，但是一直在坚持记录。无论是理论知识的学习、Debug的记录，还是读书健身笔记和感慨万千时的随笔，他们都散落在各种在线笔记软件和手机的备忘录中。&lt;/p&gt;&lt;p&gt;随着时间的推移，我对博客的认识也发生了很多变化。“记录”对我来说仍然具有非常大的意义，但是“博客”最重要的价值不应该是“记录”，而是“分享”——传递有价值的信息。我把平时的日常生活和学习经历记录在各种笔记中，过一段时间再去回顾和整理，选取其中有价值的东西去分享。&lt;/p&gt;&lt;p&gt;过去的一年再次参加了美赛，加入了几个和区块链有关的项目，也完成了不少专利和论文。更多的时间忙于运用知识，以需求为导向迫使自己进行广泛而不深刻的学习。对我来说真正有价值的输入变少了，输出和表达的欲望也降低了很多。&lt;/p&gt;&lt;p&gt;   我在原先的博客域名之下又建立了一个英文博客&lt;a href=&quot;https://blog.crocodilezs.top/eysblog_en/&quot;&gt;BLOG-EN&lt;/a&gt;，旨在整理自己过去一年经历的各种项目。同时也将原博客中的内容进行整理归档，便于自己和访客的查阅。&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>[ICM 2021] Music Never Stops: Cracking the Secret of Musical Influence</title>
    <link href="https://enblog.crocodilezs.top/202102/2021-02-10-2021ICM-Music/"/>
    <id>https://enblog.crocodilezs.top/202102/2021-02-10-2021ICM-Music/</id>
    <published>2021-02-10T10:24:24.000Z</published>
    <updated>2022-01-14T14:09:07.804Z</updated>
    
    <content type="html"><![CDATA[<p>We participated in ICM 2021 during Feb 5th - Feb 9th and finally honored <strong>Meritorious Winner(Top 7%)</strong>😆. Because we 3 people are all music enthusiasts, we choosed the problem D about data mining in music similarity and influence. We thought we could do better in this field. It turns out that we were right. Here is our thinking and solution to this problem.</p><h1 id="Topic-and-Datasets"><a href="#Topic-and-Datasets" class="headerlink" title="Topic and Datasets"></a>Topic and Datasets</h1><p>Here are <a href="https://www.mathmodels.org/Problems/2021/ICM-D/2021_ICM_Problem_D.pdf">the topic and the datasets</a>. You can also see them on <a href="https://www.mathmodels.org/Problems/2021/ICM-D/index.html">COMAP</a>.</p><p>In short, our team has been identified by an organization to develop a model that measures musical influence. This problem asks us to examine evolutionary and revolutionary trends of artists and genres. We has been given 4 data sets:</p><ol><li><code>influence_data.csv</code> represents musical influencers and followers, as reported by the artists themselves, as well as the opinions of industry experts. These data contains influencers and followers for 5,854 artists in the last 90 years.</li><li><code>full_music_data.csv</code> provides 16 variable entries, including musical features such as <code>danceability</code>, <code>tempo</code>, <code>loudness</code>, and <code>key</code>, along with <code>artist_name</code> and <code>artist_id</code> for each of 98,340 songs. These data are used to create two summary data sets, including: mean values by artist - <code>data_by_artist.csv</code>, means across years <code>data_by_year.csv</code>.</li></ol><h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><blockquote><p>Music has been a necessary part of human life and history since we human have consciousness. At the same time of human evolution, music is constantly changing new forms and contents. Music itself doesn’t evolve, and there’s no doubt that it’s done by a group of people called “artists”. It’s a common sense that a new form of music occurs under the action of many factors, such as artists’ innate creativity, current social or political events, access to new instruments or tools, or other personal experiences.</p></blockquote><center><img src="/eysblog_en/imgsource/icm2021-figure-1.png" alt="The architecture of our model" width="80%"/><p><font size=3 color="black">Figure 1: The architecture of our model</font></p></center><p>Based on some basic musical attributes, our aim in this report is to build a model to quantify musical evolution. We are expected to provide a measurement mechanism to observe how the previous music affects the later music and musicians. So two major models are established to finish that job.</p><p>For the model 1, we analyzed <strong>the relationships</strong> between genre, artist and music from the perspective of influence and similarity. We use the <strong>BA model</strong> to explained the influence network, which connects the influencers and followers. Based on the influence relationships between artists, we use <strong>the directed edge of the model</strong> to construct an influence-index. And then based on this index, we use <strong>the improved Louvain algorithm</strong> to in <strong>community partition</strong> to get The partition based on influence.</p><p>On the other hand, from the perspective of the similarity between artists, we use <strong>Pearson correlation coefficient</strong> to remove the redundant of music attributes, and construct the measure index of similarity. Based on this index, we use <strong>the improved K-Means algorithm</strong> in community partition, and finally obtain The partition based on similarity.</p><p>We creatively proposed to <strong>combine these two networks</strong>, and <strong>the NMI index</strong> is used to analyze the relationship between similarity and influence. Finally, we prove that the influencers actually affect the music created by the followers.</p><p>For the model 2, we took <strong>time</strong> into consideration. In order to analyze the musical evolution process, We use <strong>ARIMA model</strong> to create an ideal evolution curve. We compare the real curve with the ideal evolution curve to find how the social, culture and technology affect the music. We give an example of electronic music. Symbols and Definitions show in the Table 1.</p><center><img src="/eysblog_en/imgsource/icm2021-table-1.png" alt="" width="100%" /></center><h1 id="🌟Model-1🌟-BA-Model-and-Optimized-Louvain-KMeans-Algorithm"><a href="#🌟Model-1🌟-BA-Model-and-Optimized-Louvain-KMeans-Algorithm" class="headerlink" title="🌟Model 1🌟: BA Model and Optimized Louvain-KMeans Algorithm"></a><strong>🌟Model 1🌟</strong>: BA Model and Optimized Louvain-KMeans Algorithm</h1><p>Model 1 we analyze social networks from the perspective of influence and similarity, and then analyze the influence and similarity relationship among genres, artists and music.</p><p>In this model, we use directed graph to construct influence network and BA model to explain the influence propagation relationship in the network. First, we analyzed the influence relationship between artists. We propose the influence index by using the indegree and outdegree parameters inthe directed graph, and use the improved Louvain algorithm in community partition to get the “The partition based on influence”.</p><p>Then we analyze the similarity among the music, we use <strong>Pearson correlation coefficient</strong> to delete redundant attributes, and propose the measure of “similarity”. Through similarity measurement, the improved KMeans algorithm is used in community partition to obtain “The partition based on “similarity”.</p><p>After that, we innovatively combine the above two networks, and perfectly explain the relationship between music influence and similarity by using NMI parameters. We call this innovative method Optimized Louvain KMeans Algorithm.</p><h2 id="Analysis-of-Music-Influence"><a href="#Analysis-of-Music-Influence" class="headerlink" title="Analysis of Music Influence"></a>Analysis of Music Influence</h2><h3 id="The-influence-of-network-BA-model"><a href="#The-influence-of-network-BA-model" class="headerlink" title="The influence of network (BA model)"></a>The influence of network (BA model)</h3><p>In Model 1 we analyze social networks from the perspective of influence and similarity, and then analyze the influence and similarity relationship between genres, artists and music.</p><p><code>Influence_data.csv</code> includes 5854 artists in the past 90 years according to the artists themselves and industry experts. <strong>We construct 42770 influence relationships among the 5854 artists through the data set, and the directed edges in the network point from the follower to the influencer</strong>. In the Figure3.1.1, artists of different genres are represented by different colors. There are <strong>19 genres of music (except unknown) in the dataset</strong>.</p><p>Through the Figure2, we can roughly know the influence relationship between different genres. For example, we can easily see that Pop/Rock music has influence on all other genres, which is also determined by the characteristics of Pop/Rock music itself. Based on this network, the later content of this paper will further analyze the relationship between the influence of music characteristics and social network, age, policy and so on.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-2.png" alt="" width="80%"/><p><font size=3 color="black">Figure 2: Directed network of musical influence</font></p></center><p>In network theory, scale-free network is a kind of complex network. Its typical feature is that most nodes in the network are only connected with a few nodes, and a few nodes are connected with a lot of nodes. In reality, many networks have scale-free characteristics, such as Internet, financial system network, social network and so on.</p><p>According to the influence direct graph, we guess that the influence network is a scale-free network, and then we will verify it by BA model.This model is based on two assumptions.</p><ul><li>Growth model: many real networks are constantly expanding and growing, such as the birth of new web pages in the Internet, the publication of new papers and so on. Obviously, the musical influence network is also expanding through the increasing influence relationship.</li><li>Priority connection mode: when new nodes join, they tend to connect with nodes with more connections. For example, new web pages usually have connections to well-known Web sites, and new papers tend to cite well-known literatures that have been widely cited, etc. In reality, the same is true of influence. New musicians are more likely to be influenced by<br>influential influencers.</li></ul><p>Based on the above two hypotheses, we randomly selected 500 followers from the data set, removed them from the established social network, and then connected them back to the influence network by building a scale-free network with BA model.</p><p>There are \(N<em>a\) nodes in the original network, we will add a new artist \(a_i\) to the network. When \(a_i\) is a new node, \(m\) edges are connected from the new node to the original node, and the connection mode is the node with priority given to the high heights. For the original artist \(a_j\) in the network,the number of degree in the original network is recorded as \(idg</em>{a<em>j}\). Then the probability \(p</em>{a_i,a_j}\) of the new node can be canculate as follows:</p><script type="math/tex; mode=display">P_{a_i,a_j}=\frac{idg_{a_j}}{\sum_{K \in artist_{id}}idg_{a_k}}\tag{1}</script><p>By choosing the appropriate probability threshold, we add these 500 nodes back to the influence network, and create related directed edges. According to the real influence network, we calculate the connection accuracy of each new node(\(a_i\)) and the original node. The accuracy of the connection betweennew node(\(a_i\)) and the original node is 81.74%. This shows that the influence network basically conforms to the BA model.</p><p>According to the properties of BA model, we can know that the distribution of the number of the followers of artists can be approximately described by a power function with a power exponent of 3. For artist \(a_i\), the distribution function of the number of people affected is:</p><script type="math/tex; mode=display">p(idg_{a_i}) \propto 2idg^3_{a_i}\tag{2}</script><p>The distribution function can help us better explain the spread of influence, and can also be used to predict which artists are more attractive to the new artists.</p><h3 id="Analysis-of-music-influence-from-the-perspective-of-genre"><a href="#Analysis-of-music-influence-from-the-perspective-of-genre" class="headerlink" title="Analysis of music influence from the perspective of genre"></a>Analysis of music influence from the perspective of genre</h3><p>To quantify the influence of music genres, based on the previous social network, we counted the number of directed edges between genres as the influence parameter, and built the heat map between genres. In order to show the heat map better, we take logarithm of the number of directed edges.</p><p>As can be seen from the below Figure 3, the influence of all artists is mainly reflected in the genre itself. Jazz has an impact on all other genres of music. Pop / Rock, R&amp;B and Vocal also have a great impact on other fields.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-3.png" alt="" width="60%"/><p><font size=3 color="black">Figure 3: Heat map between genres</font></p></center><h3 id="Influence-index"><a href="#Influence-index" class="headerlink" title="Influence-index"></a>Influence-index</h3><p>In order to evaluate the music influence of artists better, we introduce the following evaluation measures. On the basis of the number of followers, we construct the comprehensive measure of influence index, which combines the influence of artists in and out of their genres to give an objective evaluation of the influence of artists.</p><script type="math/tex; mode=display">Influence-index_i=F_{in_i}+logG_i * F_{out_i}\tag{3}</script><p>We choose pop / rock artists as a sub network to investigate the influence of all artists. According to the influence index we set, we find out the top ten artists of this genre.The music genres and influence-index influenced by the ten artists are shown in the following table.</p><center><img src="/eysblog_en/imgsource/icm2021-table-2.png" alt="" width="100%"/></center><p>It is easy to check the influencer and followers of each artist in the network we built. Taking The Beatles as an example, the influence network chart of its music is shown in the Figure 4.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-4.png" alt="" width="90%"/><p><font size=3 color="black">Figure 4: The influence network chart of The Beatles</font></p></center><h2 id="Analysis-of-Music-Similarity"><a href="#Analysis-of-Music-Similarity" class="headerlink" title="Analysis of Music Similarity"></a>Analysis of Music Similarity</h2><h3 id="The-similarity-index"><a href="#The-similarity-index" class="headerlink" title="The similarity index"></a>The similarity index</h3><p>In order to establish the music similarity measurement model, we can compare the similarity degree of each parameter between different music. In this question, there are 7 characteristics of the music and 5 types of vocals, a total of 12 parameters to reflect the characteristics of music (<code>danceability</code>, <code>energy</code>, <code>valence</code>, <code>tempo</code>, <code>loudness</code>, <code>mode</code>, <code>key</code>, <code>acousticenss</code>, <code>instrumentalness</code>, <code>livenss</code>, <code>speechiness</code>, <code>explicit</code>).</p><p>By calculating Pearson correlation coefficients between 12 music parameters, the correlation co-efficient matrix is obtained, and the redundant parameters with high correlation can be eliminated.</p><p>Energy and valence data in <code>full-music-data.csv</code> were taken as an example to calculate the correlation coefficient, and the data could be expressed as: \( e:\{e<em>{m_1}, e</em>{m<em>2}, e</em>{m<em>3}, …, e</em>{m<em>{N_n}}\} \), \( v:\{v</em>{m<em>1}, v</em>{m<em>2}, v</em>{m<em>3},…, v</em>{m_{N_n}}\} \).</p><script type="math/tex; mode=display">\begin{align}&Mean Value: E(e) = \frac{\sum_{i=1}^ne_{m_i}}{n}, E(v)=\frac{\sum_{i=1}^nv_{m_i}}{n}, \\&Covariance: Cov(e, v) = \frac{\sum_{i=1}^n(e_{m_i}-E(e))(v_{m_i}-E(v))}{n}, \\&Standard Deviation: \delta_e=\sqrt{\frac{\sum_{i=1}^n(e_{m_i}-E(e))^2}{n}}, \delta_v=\sqrt{\frac{\sum_{i=1}^n(v_{m_i}-E(v))^2}{n}}, \\&Pearson = \rho_{ev} = \frac{Cov(e, v)}{\delta_e\delta_v}=\frac{\frac{\sum_{i=1}^n(e_{m_i}-E(e))(V_{m_i}-E(v))}{\delta_e\delta_v}}{n}\end{align}\tag{4}</script><p>Using the data in full-music-data.csv, the correlation coefficient between each index can be calculated. Its correlation coefficient matrix is shown in Figure 5.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-5.png" alt="" width="100%"/><p><font size=3 color="black">Figure 5: The correlation coefficient between each index</font></p></center><p>In order to reduce the influence of the correlation between indicators, indicators in each group with the absolute value of correlation coefficient greater than 0.3 were selected. Then, one of the two indicators of each group was selected, and the index used to evaluate music similarity was as follows:<code>danceability</code>, <code>energy</code>, <code>mode</code>, <code>key</code>, <code>liveness</code>, <code>speechiness</code> and <code>explicit</code>.</p><p>The values of <code>mode</code> and <code>explict</code> are Boolean values, so they are excluded from the music similarity model. The similarity of songs was evaluated by comparing the degree of differentiation of the remaining five indicators between different songs. We defined a numerical quantity called similarity to indicate the degree of difference between songs, and the higher the similarity value is, the more similar the songs are.</p><p>For example, calculate the similarity of song j and song k in full-musci-data file, and its data can be expressed as: \(M<em>j{d</em>{m<em>j}, e</em>{m<em>j}, l</em>{m<em>j}, s</em>{m<em>j}, k</em>{m<em>j}}, M_k{d</em>{m<em>k}, e</em>{m<em>k}, l</em>{m<em>k}, s</em>{m<em>k}, k</em>{m_k}}\).<br>The similarity of the two songs is defined as:</p><script type="math/tex; mode=display">def: Similarity = \frac{1}{\sqrt{(d_{m_j}-d_{m_k})^2+(e_{m_j}-e_{m_K})^2+...+(k_{m_j}-k_{m_k})^2}}\tag{5}</script><h3 id="Similarity-analysis-between-music-genres"><a href="#Similarity-analysis-between-music-genres" class="headerlink" title="Similarity analysis between music genres"></a>Similarity analysis between music genres</h3><p>We selected 12 music genres in the data set to analyze their music similarity. For each genre, we selected 50 artists with the highest influence index, and calculated the music style similarity between these 500 artists. Then the music similarity between different genres is calculated according to the same genre and different genres. The results are shown in the Figure 6 below.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-6.png" alt="" width="60%"/><p><font size=3 color="black">Figure 6: Music similarity between different genres</font></p></center><p>The darker the color in the figure 6 is, the higher the similarity between genres. For example, the similarity between Blues and Pop/Rock,jazz and Country is very high, while the similarity between R&amp;B and Vocal is very low. Similar conclusions can be drawn directly from the figure.</p><p>In order to better distinguish the characteristics of music genres, we use radar chart to intuitively show the prominent characteristics of each genre.The musical characteristics of these 12 genres are shown in the Figure 7.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-7.png" alt="" width="80%"/><p><font size=3 color="black">Figure 7: Music characteristics</font></p></center><h3 id="The-analysis-of-the-factors-that-influence-the-spread-of-music"><a href="#The-analysis-of-the-factors-that-influence-the-spread-of-music" class="headerlink" title="The analysis of the factors that influence the spread of music"></a>The analysis of the factors that influence the spread of music</h3><p>Using the data, the correlation coefficients between 12 indexes and the popularity of music are calculated.</p><p>In this question, we assume that if the absolute value of the correlation coefficient is greater than 0.3, we can think that there is a strong correlation between the data. When the absolute value of the correlation coefficient is between 0.1 and 0.3, we can think that there is a correlation between the data.</p><p>The characteristics of music communication can be reflected from the popularity of music, so from the correlation coefficient, it can be concluded that the communicability of music is positively correlated with the dancebility, energy, loudness and explicit, and negatively correlated with the instrumental, acoustic and other indicators, and is most affected by the dancebility, loudness and acoustic.</p><h2 id="Optimized-Louvain-KMeans-Algorithm-and-NMI"><a href="#Optimized-Louvain-KMeans-Algorithm-and-NMI" class="headerlink" title="Optimized Louvain-KMeans Algorithm and NMI"></a>Optimized Louvain-KMeans Algorithm and NMI</h2><p>Through the heat map of influence and similarity, we can qualitatively observe the relationship between music influence and similarity. </p><p>Next, we innovatively propose the optimized louvain-kmeans algorithm to quantitatively analyze the relationship between music influence and similarity with the help of NMI.The architecture of this model is shown in the figure 8.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-8.png" alt="" width="80%"/><p><font size=3 color="black">Figure 8: Flow chart of the model</font></p></center><h3 id="The-community-partition-based-on-influence"><a href="#The-community-partition-based-on-influence" class="headerlink" title="The community partition based on influence"></a>The community partition based on influence</h3><p>The Louvain method for community detection is a method to extract communities from large networks created by Blondel et al. from the University of Louvain (the source of this method’s name). The method is a greedy optimization method that appears to run in time \(O(n · log_2n)\) if \(n\) is the number of nodes in the network.We will use the Louvain algorithm to divide the influence network.</p><p>Before using Louvain, we need to introduce the concept of modularity. We use the Louvain algorithm to make as many edges as possible in the community and as few as possible between the communities. The measure of this index is modularity. </p><p>The number of nodes in the network is \(N<em>a\), the number of edges is \(N</em>{infl}\), and the indegree of node \(a<em>i\) is \(idg</em>{a<em>i}\). The adjacency matrix of the network is expressed as A, \(A</em>{a_i,a_j}=0\) means there is no edge between \(a_i\) and \(a_j\). \(AVW = 1\) means there are edges between the two nodes. </p><p>Define variable s, and \(s<em>{a_ia_j}\) means that \(a_i\) and \(a_j\) belong to the same partition. \(s</em>{a<em>ia_j}=-1\) means that the two nodes belong to different partition. Then we can use \(\delta</em>{a<em>ia_j} = 1/2(s</em>{a_ia_j} + 1) \) to verify if \(a_i\) and \(a_j\) belong to the same partition in a quantitive way. If the result equals one, we can say the two nodes belong to the same partition. If not, the result equals zaro. Then the probability expectation of modularity can be expressed as:</p><script type="math/tex; mode=display">\overline{Q} = 1/2\frac{\sum_{a_ia_j}A_{a_ia_j}}{N_{infl}} \delta_{a_ia_j}\tag{6}</script><p>Modularity can be finally expressed as:</p><script type="math/tex; mode=display">Q=\frac{1}{2m}\sum_{a_ia_j}(A_{a_ia_j}-\frac{idg_{a_i}idg_{a_j}}{2N_{infl}})\delta_{a_ia_j}\tag{7}</script><p>Through greedy algorithm, the modularity is continuously optimized to achieve “community partition based on influence”,The schematic diagram of the improved Louvain algorithm is shown in the Figure 9.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-9.png" alt="" width="80%"/><p><font size=3 color="black">Figure 9: Optimized louvain algorithm</font></p></center><p>In the process of optimization, we select the artist with the biggest influence factor among 19 music genres, and prevent them from being divided into a module, so as to improve the efficiency of community partition. The community partition results are shown in the Figure 10.(different colors represent different communities)</p><center><img src="/eysblog_en/imgsource/icm2021-figure-10.png" alt="" width="50%"/><p><font size=3 color="black">Figure 10: The partition based on influence</font></p></center><h3 id="The-communiy-partition-based-on-similarity"><a href="#The-communiy-partition-based-on-similarity" class="headerlink" title="The communiy partition based on similarity"></a>The communiy partition based on similarity</h3><p>We use KMeans to divide all the artist nodes into communities, so that the music similarity within the community is higher, and the music similarity between the communities is lower. Similar to the Louvain algorithm, we use the artist with the largest influence factor in the same 19 music genres, and prevent them from being divided into a module, so as to improve the efficiency of community partition.The community partition results are shown in the Figure 11(different colors represent different communities)</p><center><img src="/eysblog_en/imgsource/icm2021-figure-11.png" alt="" width="50%"/><p><font size=3 color="black">Figure 11: The partition based on similarity</font></p></center><h3 id="The-relationship-between-influence-and-similarity-was-analyzed-through-NMI"><a href="#The-relationship-between-influence-and-similarity-was-analyzed-through-NMI" class="headerlink" title="The relationship between influence and similarity was analyzed through NMI"></a>The relationship between influence and similarity was analyzed through NMI</h3><p>NMI is often used to detect the difference between the results of the partition and the true partition of the network and calculate the correct rate. Here we use the NMI index to evaluate the similarity between the influence-based and similarity-based social partitions.The higher the NMI index is, the more similar the two partitions are, which indicates that the influencers actually affect the music created by the followers.</p><p>In our model,\(P<em>{SC}\) stands for “similarity-based community partition” and \(P</em>{IC}\) for “influence-based community partition”, and the NMI index of these two can be expressed by the following formula:</p><script type="math/tex; mode=display">\mathrm{NMI}=\frac{-2 \sum_{i=1}^{C_{P_{I C}}} \sum_{j=1}^{C_{P_{S C}}} C_{i j} \cdot \log \left(\frac{C_{i j} \cdot N}{C_{i} \cdot C_{j}}\right)}{\sum_{i=1}^{C_{P_{I C}}} C_{i} \cdot \log \left(\frac{C_{i}}{N}\right)+\sum_{j=1}^{C_{P S C}} C_{i j} \cdot \log \left(\frac{C_{j}}{N}\right)}\tag{8}</script><p>where N is the number of nodes, C is a confusion matrix, the element \(C<em>{ij}\) in the matrix indicates the number that the nodes belonging to the community i in the SC partition also belong to communities j in the IC partition. \(P</em>{IC}(P_{SC})\) is the number of communities in IC(SC) partition, \(C_i(C_j)\) is the sum of elements in matrix C. The grater the value of NMI,the more similarity between SC and IC partition, when the NMI value is 1, it indicates that SC and IC are the same partition of the network.</p><p>Finally, the NMI value we calculated is 0.6237, indicating that the influencers actually affect the music created by the followers.</p><h1 id="🌟Model-2🌟-Time-Series-Analysis"><a href="#🌟Model-2🌟-Time-Series-Analysis" class="headerlink" title="🌟Model 2🌟: Time-Series Analysis"></a><strong>🌟Model 2🌟</strong>: Time-Series Analysis</h1><p>It is a normal process that music genres emerge, evolve, and disappear. Our team member managed to observe big turns over time, and identify the key revolutionary artist of each genre. Whenever a music genre is about to leap, there will always be clues to change. As for a music genre, it is obvious that the explosive growth in the number of new artists and new songs indicates the prevalence and significant leap of the genre. So our team counted the number of artists and songs in the history of ten major music genres, and found the time of change in the visual image. According to the influence network, the most influential artists in these years were identified as the pioneers of the music revolution, that is, the so-called music revolutionaries. The details are shown below.</p><h2 id="The-evolution-of-genres-over-time"><a href="#The-evolution-of-genres-over-time" class="headerlink" title="The evolution of genres over time"></a>The evolution of genres over time</h2><p>We have studied the evolution of ten genres over time. In the text, we choose Jazz, R&amp;B and Country as three genres to illustrate their evolution over time.</p><h3 id="New-Artists"><a href="#New-Artists" class="headerlink" title="New Artists"></a>New Artists</h3><p>First of all, look at the number of artists added from the genre.According to the data given, the statistical changes of the number of people in the three schools from 1930 to 2010 are shown in Figure 12.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-12.png" alt="" width="60%"/><p><font size=3 color="black">Figure 12: The number of artists in different musical genres</font></p></center><p>It can be seen from the figure that jazz, R&amp;B and country all rose in the United States in the 1930s. Jazz flourished from the 1930s to the 1950s, and the number of new artists reached 102 in the 1950s. After the 1950s, jazz began to decline rapidly, and the number continued to decline. By the 2010s, there was no jazz New artists are included in the statistics.</p><p>R&amp;B music developed slowly from 1930’s to 1940’s, and the number of artists increased slowly. However, as time went into 1940’s, R&amp;B music genres developed rapidly, and the number of new artists increased from 17 in 1940 to 104 in 1950 in just 10 years. When R&amp;B developed into 1950s, the number of artists still maintained a steady growth momentum, and reached the peak in 1960 From the 1960s to the 21st century, R&amp;B declined as a whole. The number of new artists has been decreasing except for the growth in the 1980s. In 2010, the number of new artists fell back to the level of the early 1940s.</p><p>The number of artists in the country music genre increased slowly, showing a fluctuating upward trend from 1930 to 1990. The number of new artists reached the peak of 74 in 1990. However, compared with the peak of jazz and R&amp; B genre, the number of artists was still small. After 90 years, the country music genre began to decline, and the number of new artists fell back to the<br>beginning of the genre The number of musicians.</p><h3 id="The-release-of-songs"><a href="#The-release-of-songs" class="headerlink" title="The release of songs"></a>The release of songs</h3><p>In terms of the number of songs released, in the data used, we exclude the songs jointly released by multiple artists, and all the songs included in the calculation are published by artists alone, which can avoid that a song may be released by artists of multiple genres.</p><p>According to the given data, the changes of the number of songs released by the three genres from 1920 to 2020 are counted, as shown in Figure 13.</p><p>Compared with the broken line trend in Figure 1 and Figure 2, we can conclude that the increase of new generation artists will significantly affect the number of songs released, and this effect is often ahead of the increase in the number of songs released, and there is a cumulative effect. Taking jazz music genre as an example, the number of new generation artists in the genre was at a high growth level from 1930 to 1960. In 1950, the number of new generation artists reached its peak. During this period, the genre accumulated a large number of excellent artists. These artists matured in the 1950s and 1960s, and a large number of music works emerged. During this period, the music circulation of jazz music was much higher than that of any other period, reaching the peak of 486 songs in 1957. The same is true of R&amp;B music. In 1960, the number of new generation artists<br>reached its peak, and then in 1972, the number of R&amp;B music released reached its peak of 339.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-13.png" alt="" width="60%"/><p><font size=3 color="black">Figure 13: Total number of songs released each year by different genre</font></p></center><h3 id="Genre-popularity"><a href="#Genre-popularity" class="headerlink" title="Genre popularity"></a>Genre popularity</h3><p>From the perspective of popularity, we first give the definition of genre popularity. Gene popularity: based on the active-Start, the arithmetic mean value of the popularity of all artists of the same genre is defined as genre popularity.</p><p>For example: jazz music genre, active- There are 45 artists who started in 1930, according to the data table-by-artist.csv We can know the popularity value of artists who meet the conditions, and we can get the popularity of jazz music in 1930 by taking the arithmetic average of these values.</p><p>According to the above definition, we can get the change curve of the three genres from 1930 to<br>2010, as shown in Figure 14.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-14.png" alt="" width="60%"/><p><font size=3 color="black">Figure 14: Grnre popularity</font></p></center><p>From the trend of the line chart, the value of Genre popularity is increasing over time. The reason for the decline of jazz curve from 2000 to 2010 is the lack of 2010 active-Start’s Jazz artist data, so its value was zero in 2010. Because the popularity of songs is calculated by algorithm, and the result largely depends on the total number of tracks played and the time of the most recent track played. Generally speaking, as time goes on, the frequency of playing the track will increase significantly, and it has a higher popularity. Therefore, the popularity of genres will increase with<br>the passage of time.</p><h2 id="The-influence-of-external-factors-on-the-development-trend-of-genres"><a href="#The-influence-of-external-factors-on-the-development-trend-of-genres" class="headerlink" title="The influence of external factors on the development trend of genres"></a>The influence of external factors on the development trend of genres</h2><p>We divide the ideal evolution of music genres into four stages: initial stage, development stage, booming stage and recession stage. We use the number of songs of a genre in a certain period to express the prosperity of the genre in that period. Based on the number curve of 10 genres, we use ARIMA model to fit an ideal evolution curve of genres.</p><p>Without the influence of social environment, scientific and technological development and other factors, genres will evolve according to the ideal curve.</p><p>ARIMA model(Auto regressive Integrated Moving Average model ) is one of the time series prediction methods. In ARIMA (P, D, q), AR is “autoregressive”, P is the number of autoregressive terms; Ma is “moving average”, q is the number of moving average terms, and D is the difference order of making it a stationary sequence.</p><p>The ideal evolution curve of genres fitted by this model is shown in the Figure 15 (the light blue area is the error range). Compared with the ideal evolution curve and the actual evolution curve of music genre, when there is a significant difference between the two in a certain period, it shows that there are social and cultural factors that have a great impact on the genre at this time.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-15.png" alt="" width="60%"/><p><font size=3 color="black">Figure 15: The ideal evolution curve of genres</font></p></center><p>Taking electronic music as an example, the evolution curve of electronic music and the evolution curve of the genre in the ideal state are shown in the Figure 16.</p><center><img src="/eysblog_en/imgsource/icm2021-figure-16.png" alt="" width="60%"/><p><font size=3 color="black">Figure 16: The evolution curve of electronic</font></p></center><p>In fact,following the emergence of raving, pirate radios, and an upsurge of interest in club culture. Electronic music achieved widespread mainstream popularity in Europe. Meanwhile, MIDI devices, which has been the musical instrument industry standard interface since the 1980s through to the present day, became commercially available in 1980s.These cultural and technological factors have promoted the rapid development of electronic music.</p><h1 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h1><p>To understand and measure the influence of previously poducted music on new music and musical artists, we proposed a series of novel models to address the sub-issues from creating the influence network based on the similarity between artists. The proposed model achieves high accuracy and robustness.</p><ul><li>We create the directed network based on music influence and use BAmodel to explain how the influence network expend. Thenwe propose Influence-Index to analyze the influence between genres and artists. We select the most 10 influential artists and show their influence-index.</li><li>In order to analyze the similarity between genres, artists and music, we use correlation analysis to remove redundant attributes. We finally select 7 attributes which actually infect the similarity. They are danceability, energy, key, liveness, speechiness, mode and explicit. Based on these attributes, we propes similirity index.</li><li>To find the relationship between the music similarity and the influence, we propose a Optimized LOUVAIN-KMeans Algorithm and use it to community partition. By participating artists into different community through Louvain algorithm and KMeans algorithm, we can obtain 2 different partition. Then we use NMI to estimate the similarity between these 2 partions. Finally we get the conclusion that influencers actually affect the music created by followers.</li><li>We analyze the influence processes of musical evolution that occoured over time. We use ARIMA model to create an ideal evolution curve. By comparing with the ideal evolution curve, we can find how the social, culture and technology affect the music. We give an example about electronic music.</li></ul><h1 id="Strength-and-weaknesses"><a href="#Strength-and-weaknesses" class="headerlink" title="Strength and weaknesses"></a>Strength and weaknesses</h1><h2 id="Strengths"><a href="#Strengths" class="headerlink" title="Strengths"></a>Strengths</h2><p>-We creatively proposed Optimized LOUVAIN-KMeans Algorithm and use the partition to explain the relationship between the influence and music similarity. The community network is as a bridge between them, and the model can reflect the relationship effectively.</p><ul><li>We proposse Influence-Index to estimate the influence of the artists objectively. It’s not accuracy to use only the number of followers or the number of music.</li><li>When analyzing the similarity between music and artists, our model is simple and convenient. Among the 12 music attributes given in the original data set, we remove the redundant attributes through correlation analysis, so that our model can calculate the similarity more efficiently and maintain a higher accuracy.</li><li>We creatively use ARIMA model to generate an ideal evolution curce of the genre. It make our model more robust, and can be used in many other situation. The ideal evolution curve reveal the process of the evolution in these genres.</li></ul><h2 id="Weakness"><a href="#Weakness" class="headerlink" title="Weakness"></a>Weakness</h2><ul><li>We don’t consider the influence between genres when we construct the ideal evolution curve. And new genres will appear at any time, so the curve may not that accuracy.</li><li>The interpretability of the influence and similarity model is not strong. We only find the relationship between similarity and influence. But we don’t know how it operates indetail.</li><li>The amount of data of some minority genres is too small, the prediction result of the model for minority genres is not good.</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;We participated in ICM 2021 during Feb 5th - Feb 9th and finally honored &lt;strong&gt;Meritorious Winner(Top 7%)&lt;/strong&gt;😆. Because we 3 peop</summary>
      
    
    
    
    <category term="Mathematical Modeling" scheme="https://enblog.crocodilezs.top/categories/Mathematical-Modeling/"/>
    
    
  </entry>
  
  <entry>
    <title>A Renewable Energy Certificate Trading System Based on Blockchain</title>
    <link href="https://enblog.crocodilezs.top/202110/2021-10-02-REC-trading/"/>
    <id>https://enblog.crocodilezs.top/202110/2021-10-02-REC-trading/</id>
    <published>2020-10-02T09:32:11.000Z</published>
    <updated>2022-08-25T13:24:55.975Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>At present, the cumbersome issuing process of renewable energy certificate (REC) and inflexible pricing mechanism consume a lot of manpower and material resources. In order to solve this problem, this paper proposes a hybrid REC trading system based on Consortium Blockchain. The paper introduces the operation mode of the system in detail and changes the view replacement protocol in the Practical Byzantine Fault Tolerance (PBFT) Algorithm to improve the stability of the system. It also introduces the bidding rules of Continuous Double Auction (CDA) used in the system, and designs the bidding strategies to maximize the user’s profit and the success rate of transaction. Finally, ARIMA model is also used to forecast the price of RECs to provide guidance for both buyers and sellers. </p><ul><li><strong>Index Terms</strong><br>REC transaction, Consortium Blockchain, Continuous Double Auction, ARIMA</li></ul><h1 id="PDF"><a href="#PDF" class="headerlink" title="PDF"></a>PDF</h1><p><iframe style="height: 1000px; width: 100%; display: block" frameborder="0"  scrolling="no" src="/eysblog_en/objectsource/BDRA-13.pdf"> </iframe><br><br /></p><p>Accepted by TrustCom 2021: International Conference on Trust, Security and Privacy in Computing and Communications, Jul.2021.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h1&gt;&lt;p&gt;At present, the cumbersome issuing process of </summary>
      
    
    
    
    <category term="Blockchain" scheme="https://enblog.crocodilezs.top/categories/Blockchain/"/>
    
    
  </entry>
  
  <entry>
    <title>Netizen Sentiment Recognition During COVID-19</title>
    <link href="https://enblog.crocodilezs.top/202006/2020-06-18-Netizen-Sentiment/"/>
    <id>https://enblog.crocodilezs.top/202006/2020-06-18-Netizen-Sentiment/</id>
    <published>2020-08-19T14:42:24.000Z</published>
    <updated>2022-01-15T12:48:26.463Z</updated>
    
    <content type="html"><![CDATA[<p>The emergence of the COVID-19 has disrupted our normal life. People’s psychological state can also receive a great negative impact. We chose such a topic with humanistic concern in <code>Data Warehouse and Data Mining</code> course, hoping that through Weibo posts we can gain more insight into people’s emotional state during the COVID-19. We are awarded the best project (1/23) in this course, and we think we could do more in psychological care during COVID-19.</p><h1 id="Data-Source-and-Data-Structure"><a href="#Data-Source-and-Data-Structure" class="headerlink" title="Data Source and Data Structure"></a>Data Source and Data Structure</h1><p>This data comes from a data mining competition organized by DataFountain. The data set is a collection of Weibo crawled during the COVID-19. The link to the dataset is <a href="https://www.datafountain.cn/competitions/423/datasets">here</a>.</p><blockquote><p>The dataset cannot be downloaded directly from the official website because the competition has ended, the dataset can also be found on <a href="https://www.kaggle.com/liangqingyuan/chinese-text-multi-classification?select=nCoV_100k_train.labled.csv">Kaggle</a>.</p></blockquote><p>We used 100,000 posts from the training set provided by the competition website, but considering the time and computational cost, I only used the first 10,000 posts with annotations, and divided them into a training set and a test set in a 7/3 ratio. This dataset was based on 230 keywords related to the topic of “新冠肺炎”, which means COVID-19 in Chinese. </p><p>1,000,000 Weibo posts were collected from January 1, 2020 to February 20, 2020, and 100,000 Weibo posts were labeled with three categories: 1 (positive), 0 (neutral) and -1 (negative). The data is stored in csv format in <code>nCoV-100k.labeled.csv</code> file. The original dataset contains 100,000 user-labeled posts in the following format: <code>[post id, posting time, posting account, content, photos, videos, sentiment]</code>.</p><p>The original dataset has six attributes: <code>post id</code> (hashcode), <code>posting time</code> (Date), <code>posting account</code> (String), <code>content</code> (String), <code>photos</code> (String), and <code>videos</code> (String). Predicting <code>sentiment</code>(Int) by the above attributes. The purpose of this project is to use <strong>word bag preprocessing</strong>, <strong>TF-IDF preprocessing</strong>, <strong>word2vec</strong> and compare their effects, focusing on text processing and text sentiment analysis. So we only chose <code>content</code> attribute to predict the sentiment. (It was hard for us to read the emotion from photos and videos.)</p><p>Here is the statistical information of the dataset from Kaggle.</p><center><img src="/eysblog_en/imgsource/covid-figure-1.png" alt="" width="80%"/></center><p>The bar chart shows that the number of positive, neutral and negative posts varies considerably, with the highest number of neutral posts.</p><p>Here is the first post in dataset.</p><center><img src="/eysblog_en/imgsource/covid-figure-2.png" alt="" width="80%"/></center><p>We found this posts in Weibo APP.</p><center><img src="/eysblog_en/imgsource/covid-figure-3.png" alt="" width="80%"/></center><h1 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h1><p>We Used Kaggle Kernel. Kaggle provides free access to the Nvidia K80 GPU in the kernel. This benchmark shows that using the GPU for your kernel can achieve a 12.5x speedup in the training of deep learning models.</p><p>ref: <a href="https://www.cnblogs.com/pinard/p/6744056.html">https://www.cnblogs.com/pinard/p/6744056.html</a></p><h2 id="Data-import-and-turncut"><a href="#Data-import-and-turncut" class="headerlink" title="Data import and turncut"></a>Data import and turncut</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd filepath = <span class="string">&#x27;/kaggle/input/chinese-text-multi-classification/nCoV_100k_train.labled.cs v&#x27;</span> file_data = pd.read_csv(filepath)</span><br><span class="line">data = file_data.head(<span class="number">10000</span>)</span><br><span class="line"><span class="comment"># chose content and sentiment</span></span><br><span class="line">data = data[[<span class="string">&#x27;微博中⽂内容&#x27;</span>, <span class="string">&#x27;情感倾向&#x27;</span>]]</span><br></pre></td></tr></table></figure><h2 id="Handling-missing-values"><a href="#Handling-missing-values" class="headerlink" title="Handling missing values"></a>Handling missing values</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># handling missing values</span></span><br><span class="line">data.isnull().<span class="built_in">sum</span>()</span><br><span class="line">data = data.dropna()</span><br></pre></td></tr></table></figure><h2 id="Remove-meaningless-symbols"><a href="#Remove-meaningless-symbols" class="headerlink" title="Remove meaningless symbols"></a>Remove meaningless symbols</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clean_zh_text</span>(<span class="params">text</span>): </span><br><span class="line">    <span class="comment"># keep English, digital and Chinese </span></span><br><span class="line">    comp = re.<span class="built_in">compile</span>(<span class="string">&#x27;[^A-Z^a-z^0-9^\u4e00-\u9fa5]&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> comp.sub(<span class="string">&#x27;&#x27;</span>, text)</span><br><span class="line">data[<span class="string">&#x27;微博中⽂内容&#x27;</span>] = data.微博中⽂内容.apply(clean_zh_text)</span><br></pre></td></tr></table></figure><h2 id="Word-Cut"><a href="#Word-Cut" class="headerlink" title="Word Cut"></a>Word Cut</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># word cut </span></span><br><span class="line"><span class="keyword">import</span> jieba </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chinese_word_cut</span>(<span class="params">mytext</span>): </span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot; &quot;</span>.join(jieba.cut(mytext))</span><br><span class="line"></span><br><span class="line">data[<span class="string">&#x27;cut_comment&#x27;</span>] = data.微博中⽂内容.apply(chinese_word_cut)</span><br><span class="line"></span><br><span class="line"><span class="comment"># divided them into a training set and a test set in a 7/3 ratio.</span></span><br><span class="line">lentrain = <span class="built_in">int</span>((<span class="number">10000</span>-<span class="number">30</span>)*<span class="number">0.7</span>) </span><br><span class="line">lentest = <span class="built_in">int</span>((<span class="number">10000</span>-<span class="number">30</span>)*<span class="number">0.3</span>) </span><br><span class="line">x_train = data.head(lentrain)[<span class="string">&#x27;微博中⽂内容&#x27;</span>] </span><br><span class="line">y_train = data.head(lentrain)[<span class="string">&#x27;情感倾向&#x27;</span>] </span><br><span class="line">x_test = data.tail(lentest)[<span class="string">&#x27;微博中⽂内容&#x27;</span>]</span><br><span class="line">y_test = data.tail(lentest)[<span class="string">&#x27;情感倾向&#x27;</span>]</span><br></pre></td></tr></table></figure><center><img src="/eysblog_en/imgsource/covid-figure-4.png" alt="" width="80%"/></center><center><img src="/eysblog_en/imgsource/covid-figure-5.png" alt="" width="80%"/></center><h2 id="Import-Stop-Words"><a href="#Import-Stop-Words" class="headerlink" title="Import Stop Words"></a>Import Stop Words</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import stop words</span></span><br><span class="line">stpwrdpath = <span class="string">&quot;/kaggle/input/stop-wordstxt/stop_words.txt&quot;</span> </span><br><span class="line">stpwrd_dic = <span class="built_in">open</span>(stpwrdpath, <span class="string">&#x27;rb&#x27;</span>) </span><br><span class="line">stpwrd_content = stpwrd_dic.read() </span><br><span class="line"></span><br><span class="line"><span class="comment">#transform into list </span></span><br><span class="line">stpwrdlst = stpwrd_content.splitlines()</span><br><span class="line">stpwrd_dic.close()</span><br></pre></td></tr></table></figure><h2 id="Word-Bag-Preprocess"><a href="#Word-Bag-Preprocess" class="headerlink" title="Word Bag Preprocess"></a>Word Bag Preprocess</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text </span><br><span class="line"><span class="keyword">import</span> CountVectorizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># CountVectorizer initialize</span></span><br><span class="line">count_vec = CountVectorizer(stop_words=stpwrdlst) </span><br><span class="line">x_train_list = x_train.tolist() </span><br><span class="line">x_train_cv = count_vec.fit_transform(x_train_list).toarray() </span><br><span class="line">x_test_list = x_test.tolist()</span><br><span class="line">x_test_cv = count_vec.fit_transform(x_test_list).toarray()</span><br></pre></td></tr></table></figure><center><img src="/eysblog_en/imgsource/covid-figure-6.png" alt="" width="80%"/></center><h2 id="TF-IDF-Preprocess"><a href="#TF-IDF-Preprocess" class="headerlink" title="TF-IDF Preprocess"></a>TF-IDF Preprocess</h2><p>ref:<a href="https://blog.csdn.net/blmoistawinde/article/details/80816179">https://blog.csdn.net/blmoistawinde/article/details/80816179</a></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer </span><br><span class="line">tfidf_vec = TfidfVectorizer(token_pattern=<span class="string">r&quot;(?u)\b\w+\b&quot;</span>, max_df=<span class="number">0.6</span>, stop_words=stpwr dlst) </span><br><span class="line">x_train_tiv = tfidf_vec.fit_transform(x_train_list).toarray()</span><br><span class="line">x_test_tiv = tfidf_vec.fit_transform(x_test_list).toarray()</span><br></pre></td></tr></table></figure><center><img src="/eysblog_en/imgsource/covid-figure-7.png" alt="" width="80%"/></center><h2 id="word2vec-embedding"><a href="#word2vec-embedding" class="headerlink" title="word2vec embedding"></a>word2vec embedding</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># word2vec </span></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec </span><br><span class="line">model = Word2Vec(x_train_list, hs=<span class="number">1</span>,min_count=<span class="number">1</span>,window=<span class="number">10</span>,size=<span class="number">100</span>)</span><br><span class="line"><span class="keyword">from</span> gensim.test.utils <span class="keyword">import</span> common_texts, get_tmpfile </span><br><span class="line">path = get_tmpfile(<span class="string">&quot;word2vec.model&quot;</span>) </span><br><span class="line">model.save(<span class="string">&quot;word2vec.model&quot;</span>)</span><br><span class="line"><span class="comment"># model = Word2Vec.load(&quot;word2vec.model&quot;)</span></span><br></pre></td></tr></table></figure><center><img src="/eysblog_en/imgsource/covid-figure-8.png" alt="" width="80%"/></center><p>The corpus of 10,000 training data is still a bit small, but the results are slightly more productive. For example, if we look at the close synonyms of “开心”(happy), we can see that the results returned are more positive.</p><center><img src="/eysblog_en/imgsource/covid-figure-9.png" alt="" width="80%"/></center><h1 id="Data-Mining-Algorithm"><a href="#Data-Mining-Algorithm" class="headerlink" title="Data Mining Algorithm"></a>Data Mining Algorithm</h1><p>In this section, we will use <code>SVM</code>, <code>decision tree</code> and <code>RNN</code> algorithms to achieve classification. The embedding obtained by <code>BOW</code> and <code>TF-IDF</code> will be classified by SVM and decision tree algorithms, respectively, while the embedding obtained by Word2Vec will be classified by RNN.</p><p>The embedding obtained from Word2Vec will be classified using RNN. The detailed algorithm flow is as follows.</p><center><img src="/eysblog_en/imgsource/covid-figure-10.png" alt="" width="80%"/></center><h2 id="BOW-SVM"><a href="#BOW-SVM" class="headerlink" title="BOW + SVM"></a><code>BOW</code> + <code>SVM</code></h2><p>The 35596-dimensional embedding of the Weibo posts obtained by <code>BOW</code> is used as the input of the <code>SVM</code> in the <code>sklearn</code> package.<br>The parameters are as follows.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf = svm.SVC(C=<span class="number">0.8</span>, kernel=<span class="string">&#x27;rbf&#x27;</span>, gamma=<span class="number">20</span>, decision_function_shape=<span class="string">&#x27;ovr&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler </span><br><span class="line">scale = StandardScaler() </span><br><span class="line">scale_fit = scale.fit(x_cv) </span><br><span class="line"><span class="comment">#x = scale_fit.transform(x) </span></span><br><span class="line">lentrain = <span class="built_in">int</span>((<span class="number">10000</span>-<span class="number">30</span>)*<span class="number">0.7</span>) </span><br><span class="line">lentest = <span class="built_in">int</span>((<span class="number">10000</span>-<span class="number">30</span>)*<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">x_train_cv = scale_fit.transform(x_cv[:lentrain]) </span><br><span class="line">y_train = y[:lentrain] </span><br><span class="line">x_test_cv = scale_fit.transform(x_cv[(-<span class="number">1</span>)*lentest-<span class="number">1</span>:-<span class="number">1</span>]) </span><br><span class="line">y_test = y[(-<span class="number">1</span>)*lentest-<span class="number">1</span>:-<span class="number">1</span>] </span><br><span class="line"><span class="built_in">print</span>(x_train_cv.shape) </span><br><span class="line"><span class="built_in">print</span>(y_train.shape) </span><br><span class="line"><span class="built_in">print</span>(x_test_cv.shape)</span><br><span class="line"><span class="built_in">print</span>(y_test.shape)</span><br></pre></td></tr></table></figure><p><strong>TIPS:</strong></p><ul><li><code>SVM</code> and <code>Decision Tree</code> algorithms for 10,000 of 30,000-dimensional data can take a lot of time, and sklearn does not support GPU computing.</li><li>When you encounter a very large dataset, you should first use a small demo to check the correctness of the code, and then run a large demo with a large amount of data.</li><li><code>BOW</code> and <code>TF-IDF</code> should be used first before dividing the test and training sets, otherwise the test and training sets will not have the same dimensionality! It took a lot of time to fix this error.</li><li>Due to the excessive number of dimensions, remember to normalize the data before <code>SVM</code>.</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># prepare the data</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">scale = StandardScaler()</span><br><span class="line">scale_fit = scale.fit(x_cv)</span><br><span class="line"><span class="comment">#x = scale_fit.transform(x)</span></span><br><span class="line">lentrain = <span class="built_in">int</span>((<span class="number">10000</span>-<span class="number">30</span>)*<span class="number">0.7</span>)</span><br><span class="line">lentest = <span class="built_in">int</span>((<span class="number">10000</span>-<span class="number">30</span>)*<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">x_train_cv = scale_fit.transform(x_cv[:lentrain])</span><br><span class="line">y_train = y[:lentrain]</span><br><span class="line">x_test_cv = scale_fit.transform(x_cv[(-<span class="number">1</span>)*lentest-<span class="number">1</span>:-<span class="number">1</span>])</span><br><span class="line">y_test = y[(-<span class="number">1</span>)*lentest-<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(x_train_cv.shape)</span><br><span class="line"><span class="built_in">print</span>(y_train.shape)</span><br><span class="line"><span class="built_in">print</span>(x_test_cv.shape)</span><br><span class="line"><span class="built_in">print</span>(y_test.shape)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#test bow+svm</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x_train_cv.shape, <span class="string">&#x27;and &#x27;</span>, y_train.shape)</span><br><span class="line">clf = svm.SVC(C=<span class="number">0.8</span>, kernel=<span class="string">&#x27;rbf&#x27;</span>, gamma=<span class="number">20</span>, decision_function_shape=<span class="string">&#x27;ovr&#x27;</span>)</span><br><span class="line">cv_model = clf.fit(x_train_cv, y_train)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score, f1_score, accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Precision_score: &#x27;</span>, precision_score(y_hat_cv, y_test, average=<span class="string">&#x27;weighted&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Recall_score: &#x27;</span>, recall_score(y_hat_cv, y_test, average=<span class="string">&#x27;weighted&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;F1_score: &#x27;</span>, f1_score(y_hat_cv, y_test, average=<span class="string">&#x27;weighted&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy_score: &#x27;</span>, accuracy_score(y_hat_cv, y_test))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># roc_curve:真正率（True Positive Rate , TPR）或灵敏度（sensitivity）</span></span><br><span class="line"><span class="comment"># 横坐标：假正率（False Positive Rate , FPR）</span></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> interp</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> label_binarize</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> cycle</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve, auc</span><br><span class="line"></span><br><span class="line">nb_classes = <span class="number">3</span></span><br><span class="line"><span class="comment"># Binarize the output</span></span><br><span class="line">Y_valid = label_binarize(y_test, classes=[i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_classes)])</span><br><span class="line">Y_pred = label_binarize(y_hat_cv, classes=[i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_classes)])</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="comment"># Compute ROC curve and ROC area for each class</span></span><br><span class="line">fpr = <span class="built_in">dict</span>()</span><br><span class="line">tpr = <span class="built_in">dict</span>()</span><br><span class="line">roc_auc = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_classes):</span><br><span class="line">    fpr[i], tpr[i], _ = roc_curve(Y_valid[:, i], Y_pred[:, i])</span><br><span class="line">    roc_auc[i] = auc(fpr[i], tpr[i])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute micro-average ROC curve and ROC area</span></span><br><span class="line">fpr[<span class="string">&quot;micro&quot;</span>], tpr[<span class="string">&quot;micro&quot;</span>], _ = roc_curve(Y_valid.ravel(), Y_pred.ravel())</span><br><span class="line">roc_auc[<span class="string">&quot;micro&quot;</span>] = auc(fpr[<span class="string">&quot;micro&quot;</span>], tpr[<span class="string">&quot;micro&quot;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute macro-average ROC curve and ROC area</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># First aggregate all false positive rates</span></span><br><span class="line">all_fpr = np.unique(np.concatenate([fpr[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_classes)]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Then interpolate all ROC curves at this points</span></span><br><span class="line">mean_tpr = np.zeros_like(all_fpr)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_classes):</span><br><span class="line">    mean_tpr += interp(all_fpr, fpr[i], tpr[i])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Finally average it and compute AUC</span></span><br><span class="line">mean_tpr /= nb_classes</span><br><span class="line"></span><br><span class="line">fpr[<span class="string">&quot;macro&quot;</span>] = all_fpr</span><br><span class="line">tpr[<span class="string">&quot;macro&quot;</span>] = mean_tpr</span><br><span class="line">roc_auc[<span class="string">&quot;macro&quot;</span>] = auc(fpr[<span class="string">&quot;macro&quot;</span>], tpr[<span class="string">&quot;macro&quot;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot all ROC curves</span></span><br><span class="line">lw = <span class="number">2</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(fpr[<span class="string">&quot;micro&quot;</span>], tpr[<span class="string">&quot;micro&quot;</span>],</span><br><span class="line">         label=<span class="string">&#x27;micro-average ROC curve (area = &#123;0:0.2f&#125;)&#x27;</span></span><br><span class="line">               <span class="string">&#x27;&#x27;</span>.<span class="built_in">format</span>(roc_auc[<span class="string">&quot;micro&quot;</span>]),</span><br><span class="line">         color=<span class="string">&#x27;deeppink&#x27;</span>, linestyle=<span class="string">&#x27;:&#x27;</span>, linewidth=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(fpr[<span class="string">&quot;macro&quot;</span>], tpr[<span class="string">&quot;macro&quot;</span>],</span><br><span class="line">         label=<span class="string">&#x27;macro-average ROC curve (area = &#123;0:0.2f&#125;)&#x27;</span></span><br><span class="line">               <span class="string">&#x27;&#x27;</span>.<span class="built_in">format</span>(roc_auc[<span class="string">&quot;macro&quot;</span>]),</span><br><span class="line">         color=<span class="string">&#x27;navy&#x27;</span>, linestyle=<span class="string">&#x27;:&#x27;</span>, linewidth=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">colors = cycle([<span class="string">&#x27;aqua&#x27;</span>, <span class="string">&#x27;darkorange&#x27;</span>, <span class="string">&#x27;cornflowerblue&#x27;</span>])</span><br><span class="line"><span class="keyword">for</span> i, color <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(nb_classes), colors):</span><br><span class="line">    plt.plot(fpr[i], tpr[i], color=color, lw=lw,</span><br><span class="line">             label=<span class="string">&#x27;ROC curve of class &#123;0&#125; (area = &#123;1:0.2f&#125;)&#x27;</span></span><br><span class="line">             <span class="string">&#x27;&#x27;</span>.<span class="built_in">format</span>(i, roc_auc[i]))</span><br><span class="line"></span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">&#x27;k--&#x27;</span>, lw=lw)</span><br><span class="line">plt.xlim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.ylim([<span class="number">0.0</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;False Positive Rate&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;True Positive Rate&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Some extension of Receiver operating characteristic to multi-class&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;lower right&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>Here’s the result:</p><center><img src="/eysblog_en/imgsource/covid-figure-11.png" alt="" width="80%"/></center><center><img src="/eysblog_en/imgsource/covid-figure-12.png" alt="" width="80%"/></center><p>The reason for the coincidence of the ROC curve and the X-axis is that most of the predictions are zero. The reasons are as follows: </p><ol><li>The two embedding methods, <code>BOW</code> and <code>TF-IDF</code>, do not work well as <code>SVM</code>, and even after normalizing the input word (frequency) vector matrix, most of the predictions are still the same. </li><li>The number of “neutral” labels in the sample is much higher than the number of “positive” and “negative” labels, which is also a problem in the sample selection process.</li><li>The parameters of <code>SVM</code> can be adjusted more precisely to make the classification better.</li></ol><p>Instead of further tuning this model, we tried other algorithms first.</p><h2 id="TF-IDF-SVM"><a href="#TF-IDF-SVM" class="headerlink" title="TF-IDF+SVM"></a><code>TF-IDF</code>+<code>SVM</code></h2><p>The 38473-dimensional embedding of the <code>TF-IDF</code> derived posts is used as the input to the <code>SVM</code> in the sklearn package.</p><p>The source code is similar to the model above, so I will not repeat it here. The results are shown below.</p><center><img src="/eysblog_en/imgsource/covid-figure-13.png" alt="" width="80%"/></center><p>The parameters are as follows.<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf = svm.SVC(C=<span class="number">0.8</span>, kernel=<span class="string">&#x27;rbf&#x27;</span>, gamma=<span class="number">20</span>, decision_function_shape=<span class="string">&#x27;ovr&#x27;</span>)</span><br></pre></td></tr></table></figure></p><h2 id="BOW-Decision-Tree-amp-amp-TF-IDF-Decision-Tree"><a href="#BOW-Decision-Tree-amp-amp-TF-IDF-Decision-Tree" class="headerlink" title="BOW + Decision Tree &amp;&amp; TF-IDF + Decision Tree"></a><code>BOW</code> + <code>Decision Tree</code> &amp;&amp; <code>TF-IDF</code> + <code>Decision Tree</code></h2><p>The source code is similar to the model above, so I will not repeat it here.</p><p>Here’s the result for <code>BOW</code> + <code>Decision Tree</code>.</p><center><img src="/eysblog_en/imgsource/covid-figure-14.png" alt="" width="80%"/></center><p>Here’s the result for <code>TF-IDF</code> + <code>Decision Tree</code></p><center><img src="/eysblog_en/imgsource/covid-figure-15.png" alt="" width="80%"/></center><h2 id="Word2Vec-RNN"><a href="#Word2Vec-RNN" class="headerlink" title="Word2Vec + RNN"></a><code>Word2Vec</code> + <code>RNN</code></h2><p>Embedding Weibo content using Word2Vec, then the resulting 400-dimensional vector is fed into a 10<em>200</em>1 recurrent neural grid with one hidden layer.</p><p>Parameters:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">100</span> </span><br><span class="line">n_iters = <span class="number">20000</span> </span><br><span class="line">seq_dim = <span class="number">20</span> </span><br><span class="line">input_dim = <span class="number">20</span> <span class="comment"># input dimension</span></span><br><span class="line">hidden_dim = <span class="number">200</span> <span class="comment"># hidden layer dimension </span></span><br><span class="line">layer_dim = <span class="number">1</span> <span class="comment"># number of hidden layers</span></span><br><span class="line">output_dim = <span class="number">3</span> <span class="comment"># output dimension</span></span><br></pre></td></tr></table></figure></p><p>So far we have obtained the embedding of all words, the key problem is how to represent the sentences. I referred to the information below and chose to try it with <code>word2vec</code> using Gensim.</p><p>ref1: <a href="https://www.zhihu.com/question/29978268">https://www.zhihu.com/question/29978268</a></p><p>ref2: <a href="https://blog.csdn.net/John_xyz/article/details/79424284">https://blog.csdn.net/John_xyz/article/details/79424284</a></p><p>ref3: <a href="https://www.kaggle.com/kanncaa1/recurrent-neural-network-with-pytorch">https://www.kaggle.com/kanncaa1/recurrent-neural-network-with-pytorch</a></p><p>At the first time we try, there is exploding gradient.</p><center><img src="/eysblog_en/imgsource/covid-figure-16.png" alt="" width="80%"/></center><p>After adjusting the learning rate to 0.03, the results after 60,000 generations of training are shown below.</p><center><img src="/eysblog_en/imgsource/covid-figure-17.png" alt="" width="80%"/></center><center><img src="/eysblog_en/imgsource/covid-figure-18.png" alt="" width="80%"/></center><p>After adjusting hidden layers, the results after 20,000 generations are shown below.</p><center><img src="/eysblog_en/imgsource/covid-figure-19.png" alt="" width="80%"/></center><center><img src="/eysblog_en/imgsource/covid-figure-20.png" alt="" width="80%"/></center><h1 id="Analysis-of-Results"><a href="#Analysis-of-Results" class="headerlink" title="Analysis of Results"></a>Analysis of Results</h1><p>This is a triple classification problem on the emotion of NLP. The results are shown as follow.</p><center><img src="/eysblog_en/imgsource/covid-figure-21.png" alt="" width="80%"/></center><p>As can be seen from the above graphs, the <code>SVM</code> and <code>Decision Tree</code> algorithms have very little impact on the actual results, and the most important factor affecting the prediction results is the Embedding method. In this dataset, <code>TF-IDF</code> is more effective than <code>BOW</code>. In the end, the results of <code>TF-IDF</code>+<code>SVM</code>, <code>TF-IDF</code>+<code>Decision Tree</code> and <code>Word2Vec</code>+<code>RNN</code> are similar. The reasons for this result are as follows: </p><ol><li>The original dataset is not evenly distributed, and there are more “neutral” comments than “positive” and “negative” posts, so in the predictive classification process, most of the postss are not evenly distributed. The majority of posts tend to be classified as “neutral” in the prediction classification process, which is of course consistent with the actual situation. This is why the Embedding method has a greater impact on the results than the classification method.</li><li>The dataset is not large enough. I thought that a data set of about 10,000 would take a lot of training time, but Kaggle can use GPUs and the CPU speed of Kaggle is not slow, so I could have done it directly with the original data set of 10W, and the result would be better.</li><li>Parameter optimization. It is only fair to use the optimal parameters of each model for comparison of results.</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;The emergence of the COVID-19 has disrupted our normal life. People’s psychological state can also receive a great negative impact. We ch</summary>
      
    
    
    
    <category term="NLP (Natural Language Processing)" scheme="https://enblog.crocodilezs.top/categories/NLP-Natural-Language-Processing/"/>
    
    
  </entry>
  
  <entry>
    <title>[MCM 2020] Data Analysis for Sunshine Company&#39;s New Product</title>
    <link href="https://enblog.crocodilezs.top/202003/2020-03-10-MCM-Grocery/"/>
    <id>https://enblog.crocodilezs.top/202003/2020-03-10-MCM-Grocery/</id>
    <published>2020-03-10T02:24:24.000Z</published>
    <updated>2021-11-19T14:11:08.172Z</updated>
    
    <content type="html"><![CDATA[<p>This is my second time for MCM &amp; ICM. I teamed up with two friends respectively from computer science major and electronic information major. This is how we divide the work: one of my friends collecting information and the other writing the essay. I was responsible for modeling and coding. We finally got <strong>Meritorious Winner(Top 5%)</strong>. I’m so proud of it. This is our entry experience.</p><h1 id="Topic-amp-Dataset"><a href="#Topic-amp-Dataset" class="headerlink" title="Topic &amp; Dataset"></a>Topic &amp; Dataset</h1><blockquote><p>In the online marketplace it created, Amazon provides customers with an opportunity to rate and review purchases. Individual ratings - called “star ratings” - allow purchasers to express their level of satisfaction with a product using a scale of 1 (low rated, low satisfaction) to 5 (highly rated, high satisfaction). Additionally, customers can submit text-based messages - called “reviews” - that express further opinions and information about the product. Other customers can submit ratings on these reviews as being helpful or not - called a “helpfulness rating” - towards assisting their own product purchasing decision. Companies use these data to gain insights into the markets in which they participate, the timing of that participation, and the potential success of product design feature choices.</p><p>Sunshine Company is planning to introduce and sell three new products in the online marketplace: a microwave oven, a baby pacifier, and a hair dryer. They have hired your team as consultants to identify key patterns, relationships, measures, and parameters in past customer-supplied ratings and reviews associated with other competing products to 1) inform their online sales strategy and 2) identify potentially important design features that would enhance product desirability. Sunshine Company has used data to inform sales strategies in the past, but they have not previously used this particular combination and type of data. Of particular interest to Sunshine Companyare time-based patterns in these data, and whether they interact in ways that will help the company craft successful products.</p><p>To assist you, Sunshine’s data center has provided you with three data files for this project: hair_dryer.tsv, microwave.tsv, and pacifier.tsv. These data represent customer-supplied ratings and reviews for microwave ovens, baby pacifiers, and hair dryers sold in the Amazon marketplace over the time period(s) indicated in the data. A glossary of data label definitions is provided as well. </p></blockquote><p>The three data sets provided contain product user ratings and reviews extracted from the Amazon Customer Reviews Dataset thru Amazon Simple Storage Service (Amazon S3).</p><ul><li><code>hair_dryer.tsv</code></li><li><code>microwave.tsv</code></li><li><code>pacifier.tsv</code></li></ul><p>Data Set Definitions: Each row represents data partitioned into the following columns.</p><pre><code>- marketplace (string): 2 letter country code of the marketplace where the review was written.- customer_id (string): Random identifier that can be used to aggregate reviews written by a single author.- review_id (string): The unique ID of the review.- product_id (string): The unique Product ID the review pertains to.- product_parent (string): Random identifier that can be used to aggregate reviews for the same product.- product_title (string): Title of the product.- product_category (string): The major consumer category for the product.- star_rating (int): The 1-5 star rating of the review.- helpful_votes (int): Number of helpful votes.- total_votes (int): Number of total votes the review received.- vine (string): Customers are invited to become Amazon Vine Voices based on the trust that they have earned in the Amazon community for writing accurate and insightful reviews. Amazon provides Amazon Vine members with free copies of products that have been submitted to the program by vendors. Amazon doesn&#39;t influence the opinions of Amazon Vine members, nor do they modify or edit reviews.- verified_purchase (string): A &quot;Y&quot; indicates Amazon verified that the person writing the review purchased the product at - Amazon and didn&#39;t receive the product at a deep discount.- review_headline (string): The title of the review.- review_body (string): The review text.- review_date (bigint): The date the review was written.</code></pre><h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>Through data mining and modeling, we analyze the three types of products and provide a marketing strategy for Sunshine Company.</p><p>First of all, we preprocess the data. We fully analyze the 15 attributes in microwaves, pacifiers and hair dryers, filter them and syntheze three new attributes: review_text, popularity and reputation. We deal with missing values in the data set and remove those meaningless reviews. Through the Tokenization algorithm, we cut the sentence of the review headline and the review body into single words, which is convenient for us to analyze the emotion of the user and understand the relationship between reviews and star ratings. </p><p>Furthermore, we use the TF-IDF(Term FrequencyInverse Document Frequency) algorithm and MLP(Multi-layer Perceptron) to build a model and try to discover the relationship between reviews and star ratings. We use reviews to evaluate the products and use helpful votes to evaluate reviews, through this way we build a multidimension evaluation system. The TF-IDF algorithm uses the segmented words to generate a word frequency matrix, which is used as the input of the MLP. And the products’ star ratings are used as the output. We build models for the three types of products respectively, for the reviews of the three types of products have some differences, which may affect the accuracy of the results. The data set is divided into training set and test set. When the model training is completed, the effect of the model is tested with the test set. After the practice, our prediction is very similar to the ground truth. Then we get the relationship between star ratings and reviews. The traditional MLP model has poor interpretability, so we continue to do lexical analysis and syntax analysis on this basis, which enhances the interpretability of the whole model.</p><p>Eventually , we analyze the time-based popularity changes of the product, and make marketing suggestions to the Sunshine Company from the customer’s perspective. Through analysis, we found that the products’ popularity always reach the highest at the beginning and middle of each year, so the company could make promotional activities at that time to raise their profile. In this way, we found the relationship between period and popularity. From the perspective of customers, we observe that those users who score 5 stars or 1 star often quite emotional when giving reviews. We recommend three types of the most valuable users and select their reviews for the sunshine company. They are AmazonVine Voices Members, users who have purchased multiple similar products and users who have given one-star rate to the product. By analyzing the WordCloud of reviews of those who have given one-star rates, we can know how to improve the products.</p><h1 id="Preparation-for-the-modeling"><a href="#Preparation-for-the-modeling" class="headerlink" title="Preparation for the modeling"></a>Preparation for the modeling</h1><p>For the data mining problems that have large quantity and types of data, there are often a large number of default values, which may affect the efficiency and accuracy of the model. Therefore, the processing of these default values is of vital importance. In addition, in this data set, there is a large amount of text information in attributes such as prouct_title, and review_headline. Thus our team choose the tokenization algorithm to classify and organize the text information.</p><h2 id="Default-Value-Preprocessing"><a href="#Default-Value-Preprocessing" class="headerlink" title="Default Value Preprocessing"></a>Default Value Preprocessing</h2><p>Through our analysis, in all of the given data sets, only the review_headline and review_body attributes are default. The Amazon website stipulates that when a review is submitted, its star_rating, review_headline and review_body must be filled in, otherwise the review cannot be submitted. so the lack of data is not caused by user behavior, but created during the collection, transmission or storage of these reviews. Therefore the “default” here doesnt contain customers opinions towards the product.<br>In this case our team deal with the default records in this way.</p><ul><li>When only one of the review_headline or review_body is default, we will keep this data. Because it still contains a large amount of information;</li><li>When both of the review_headline and review_body are default, We will abandon this data. Because review is an essential part of our following analysis, in this case when both of them are default, this data is of little significance for data<br>mining.</li></ul><h2 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h2><p>The main goal of Tokenization is to normalize our texts, that is, to process a paragraph of text into short, non-redundant text information. Our tokenization process<br>includes the following steps:</p><p>a. Lowercase the values.<br>Because the text_market, product_title and other text attributes have a kind of phenomenon that some values with the same meaning have different capitalization. For example, the marketplace attribute contains ’US’ and ’us’, which is not convenient for our subsequent classification of products. Therefore we convert all text to lower<br>case.<br>b. Break the sentences into token<br>c. Remove punctuation and stop words<br>In computing, stop words are words which are filtered out before or after processing of natural language data (text).</p><p>We give an example in Figure 1.</p><center><img src="/eysblog_en/imgsource/mcm2020-figure-1.png" alt="" width="100%"/><p><font size=3 color="black">Figure 1: An example about Tokenization in Microwave Product Data Set</font></p></center><h2 id="Data-Selection-and-Synthesis"><a href="#Data-Selection-and-Synthesis" class="headerlink" title="Data Selection and Synthesis"></a>Data Selection and Synthesis</h2><p>In these 15 attributes that come from the provided data sets, some of them are not valuable. So we need to select the relatively more essential attributes. For the marketplace attribute, all evaluations in the dataset are from the US, so there is no reason to do data mining on this attribute, similarly in product_cetegory. Review_id is only used to distinguish different reviews, so we delete their corresponding data as well.</p><p>Before introducing product_id and product_parent, we need to learn more about parent-child relationships. </p><p>Each parent product may contain multiple child products, and each child parents may be different in sizes, colors, or prices. Each parent product corresponds to a product_parent, and each child product corresponds to a product_id. In the following data analysis, we mainly analyze the parent product and inspect these three types of products from a macro level. Take the microwave as an example, the parent-child product relationship as shown below. </p><p>In Figure 2 , we can find the four child products have different product_ids and they may have different sizes, colors or prices. However, they have the same product_parent. Please notice that we don’t know the specific information about these four child products.Their colors and sizes in Figure 2 are just for example. </p><p>We choose these following basic attributes in Table 1 to create the Customer Profile. Additionally, some synthetic variables are used in our model. These variables are described in Table 2.</p><center><img src="/eysblog_en/imgsource/mcm2020-figure-2.png" alt="" width="70%"/><p><font size=3 color="black">Figure 2: The Parent-Child Relationship in Microwave Products</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-table-1.png" alt="" width="100%"/></center><center><img src="/eysblog_en/imgsource/mcm2020-table-2.png" alt="" width="100%"/></center><br /># Competing Products AnalysisFirst we visualize the data sets to show the profiles of the three products. The number of reviews and parent products are shown in Table 3. Here is the products profile of microwave, the other two products are attached to the appendix. Figure 3 shows the star ratings and the number of reviews of every parent products. The horizontal axis reprensents the product titles. For Microwaves, we select the product that has the most reviews - "Danby 0.7 cu.ft. Countertop Microwave" to analyze its reviews. Before we create the wordcloud of review_text, we take stemming operations. Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root formgenerally a written word form. In Figure 5, High frequency words has bigger font size. In Figure 6, horizontal axis represents the review headline. In addition to the above analysis from the perspective of the product itself, we also try to delete the click farmers or construct Customer Profile from the user’s perspective. Through the Customer Profile, we can have a better understanding toward the interests of the target users, and thus we could provide better product services.We observe that a large number of users have bought more than one products. So we check out several reviews and found that many of them are click farmers, and their invalid reviews cannot be checked out through the helpful_votes, so eventually we manually deleted these reviews. For others who have really bought many products, we think their opinions are as valuable as those of Amazon Vine Voices Members. Therefore we filtered out 53 reviews from 9 users who have purchased more than 5 products and sent these comments to Sunshine Company for their reference. These 9 user comments are detailed in the appendix.<center><img src="/eysblog_en/imgsource/mcm2020-table-3.png" alt="" width="100%"/></center><br /><center><img src="/eysblog_en/imgsource/mcm2020-figure-3.png" alt="" width="80%"/><p><font size=3 color="black">Figure 3: The Star Ratings and the Number of Reviews of Microwave</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-figure-4.png" alt="" width="70%"/><p><font size=3 color="black">Figure 4: The proportion of People Who Were in Amazon Vine and People Who Had Bought the Product</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-figure-5.png" alt="" width="70%"/><p><font size=3 color="black">Figure 5: The Wordcloud of Microwave</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-figure-6.png" alt="" width="70%"/><p><font size=3 color="black">Figure 6: The Reviews' Total Votes and Helpful Votes of Danby 0.7 cu.ft. Countertop Microwave</font></p></center><h1 id="Model-Construction"><a href="#Model-Construction" class="headerlink" title="Model Construction"></a>Model Construction</h1><p>In the previous data preprocessing, we have processed a series of reviews_text through the Tokenization algorithm. Next, we will process the data through the TF-IDF (Term FrequencyInverse Document Frequency) algorithm and the MLP (Multi-Layer Perception) model to get the relationship between star_rating and review. </p><p>The keywords in different product reviews are quite different and will affect each other, so we build three prediction models for the products: microwave, pacifier, and hair dryer. We first use the TF-IDF algorithm to obtain the term frequency matrix. TF-IDF is a commonly used weighting technique for information retrieval and data mining. The algorithm can simply and efficiently find keywords in articles.</p><h2 id="Compute-the-TF-IDF-Matrix"><a href="#Compute-the-TF-IDF-Matrix" class="headerlink" title="Compute the TF-IDF Matrix"></a>Compute the TF-IDF Matrix</h2><p>TF-IDF consists of two parts, one is “Term Frequency” (abbreviated as TF), and the other is “Inverse Document Frequency” (abbreviated as IDF). </p><p>TF part will create a Term Frequency Matrix. We use the review_text in Figure 1 to<br>illustrate the process. The procession is shown in Figure 7.</p><center><img src="/eysblog_en/imgsource/mcm2020-figure-7.png" alt="" width="80%"/><p><font size=3 color="black">Figure 7: TF Procession</font></p></center><script type="math/tex; mode=display">f_i=\frac{c_i}{\sum^s_{i=1}c_i}\tag{1}</script><p>In Equation 1, \(f_i\) is the frequency of the \(i\)th word. \(c_i\) is the occurrence number in a review_text of the ith word. s is the amount of words in the review_text. we find out that in Figure 7 the word frequencies of ’look’, ’good’ and ’long’ are the highest. However, this does not mean these three words are equally important in the analysis process. We need to multiply this word frequency matrix by IDF (Inverse Document Frequency) to get the TF-IDF values of these words.</p><p>In Equation 2, \(d_i\) is the IDF value of the ith word. \(m\) is total number of articles in Corpus. \(n_i\) is the number of articles containing the ith word in Corpus. \(v_i\) is the TF-IDF value of the \(i\)th word.</p><script type="math/tex; mode=display">d_i=log \left( \frac{m}{n_i+1} \right)\tag{2}</script><script type="math/tex; mode=display">v_i=f_i*d_i\tag{3}</script><p>We give Figure 8 as an example for the TF-IDF values. After that, we will construct the TF-IDF matrix of the review based on the TF-IDF values of these words corresponding dictionaries. It is worth mentioning that we apply helpful<em>votes as a reference to the TF-IDF matrix. In Equation 4, \(M\) is the TF-IDF matrix for the product. The \(k\)th review’s \(i\)th word’s TF-IDF value is \(v</em>{ki}\).</p><script type="math/tex; mode=display">M_{ki}=v_{ki}\tag{4}</script><center><img src="/eysblog_en/imgsource/mcm2020-figure-8.png" alt="" width="80%"/><p><font size=3 color="black">Figure 8: TF-IDF Values</font></p></center><h2 id="Contrust-the-MLP-Model"><a href="#Contrust-the-MLP-Model" class="headerlink" title="Contrust the MLP Model"></a>Contrust the MLP Model</h2><p>We divide three data sets into training sets and test sets by a ratio of 3:1 respectively. Then we use the input and output of the training set to train MLP(Multi-Layer Perception). The core formulas of the MLP are show below.</p><p>Multi-Layer Perception made up of a lot of artificial neurons. The artificial neuron uses a nonlinear activation function to output an activity value.The training process of MLP can be divided into the following three steps:</p><ol><li>Calculate the state and activation value of each layer, until the last layer;</li><li>Calculate the error of each layer by backward propagation;</li><li>Calculate the partial derivative of each layer’s parameters, and update the parameters.</li></ol><p>Suppose neurons accept our TF-IDF Matrix \(X = (x1, x2, . . . , xn)\). State \(z\) is used to represent the weighted sum of input signal \(x\) obtained by a neuron, and the output is the activity value a of the neuron, which is defined as follows:</p><script type="math/tex; mode=display">z=W^TX+b\tag{5}</script><script type="math/tex; mode=display">a=f(z)\tag{6}</script><script type="math/tex; mode=display">z^{(l)}=W^{(l)} \cdot f_{l}\left(z^{(l-1)}\right)+b^{(l)}\tag{7}</script><p>In the process of forward propagation, we use notations as folloowing Table 4.</p><center><img src="/eysblog_en/imgsource/mcm2020-table-4.png" alt="" width="100%"/></center><br />$$X=a^{(0)} \rightarrow z^{(1)} \rightarrow a^{(1)} \rightarrow z^{(2)} \rightarrow \cdots \rightarrow a^{(L-1)} \rightarrow z^{(L)} \rightarrow a^{(L)}=y\tag{8}$$In the process of back propagation, given a set of samples \\((X(i), y(i)), 1 ≤ i ≤ N\\), the output of feedforward neural network whose objective function is \\(f(X(i)|W, b)\\) is as follows.$$\begin{aligned}J(W, b) &=\sum_{i=1}^{N} L\left(y^{(i)}, f\left(X^{(i)} \mid W, b\right)\right)+\frac{1}{2} \lambda\|W\|_{F}^{2} \\&=\sum_{i=1}^{N} J\left(W, b ; X(i), y^{(i)}\right)+\frac{1}{2} \lambda\|W\|_{F}^{2}\end{aligned}\tag{9}$$$$\|W\|_{F}^{2}=\sum_{l=1}^{L} \sum_{j=1}^{n^{l+1}} \sum_{1=1}^{n^{l}} W_{i j}^{(l)}\tag{10}$$We use gradient descent to minimize the Equation 9, finally we get \\(l\\)th layer's error in Equation 11.$$\begin{aligned}\delta^{(l)} & \triangleq \frac{J(W, b ; X, y)}{\partial z^{(l)}} \\&=\frac{\partial a^{(l)}}{\partial z^{(l)}} \cdot \frac{\partial z^{(l+1)}}{\partial a^{(l)}} \cdot \frac{J(W, b ; X, y)}{\partial z^{(l+1)}} \\&=\operatorname{diag}\left(f_{l}\left(z^{(l)}\right)\right) \cdot\left(W^{(l+1)}\right)^{T} \cdot \delta^{(l+1)} \\&=f_{l}\left(z^{(l)}\right) \odot\left(\left(W^{(l+1)}\right)^{T} \delta^{(l+1)}\right)\end{aligned}\tag{11}$$Then we continue to iterate and get an error-stable model. Finally, we use the trained model to predict the test data. We set the fitting figure of Microwave in three products as Figure 9.<center><img src="/eysblog_en/imgsource/mcm2020-figure-9.png" alt="" width="80%"/><p><font size=3 color="black">Figure 9: The fitting Figure of Hair Dryer</font></p></center><h2 id="Time-Based-Analysis"><a href="#Time-Based-Analysis" class="headerlink" title="Time-Based Analysis"></a>Time-Based Analysis</h2><p>Product reputation is composed of star rating and popularity. Product quality is the determing factor of star rating and review content. We will analyze popularity based on time to provide suggestions for sunshine company.</p><p>We analyze the 8 products with the highest popularity, and observed their popularity changes over time. It can be seen from the Figure 10 that five of the eight products have peak popularity only at the beginning of each year, and the other three products will have peak popularity at the beginning and middle of each year. There are three reasons for this:</p><p>a. At the beginning of the year, it’s just the time for people to purchase in the new year. People’s desire for shopping is strong, while microwave, pacifier and hair dryer are durable products, which can be used for a long time. No one will buy twice in a short time.</p><p>b. Amazon or merchants have promotional activities at the beginning of the year to improve product popularity.</p><p>c. The data retrieving process is not objective, we only get the data at a certain period.</p><center><img src="/eysblog_en/imgsource/mcm2020-figure-10.png" alt="" width="80%"/><p><font size=3 color="black">Figure 10: The Number of Reviews of the 10 Most Popularity Products</font></p></center><p>A products reputation consists the product’s praise and popularity, and the decisive factor that influences a products star ratings and the content of its reviews is its quality. In the following discussion we will offer a proposal for Sunshine Company from the perspective of the products popularity. </p><p>We analyze the top eight popular products and observe their popularity changes over time. To our surprise, 5 of these 8 products have popularity peaks only at the beginning of each year, and the remaining three products have popularity peaks at the beginning and middle of the year. There why This phenomenon happens are as follows:</p><ol><li>In the beginning of the year, everyone is buying goods for the coming of new year. So people’s desire for shopping is strong. On the other hand, the microwave, pacifier and hair dryer are all durable commodities, which can be used for a long time. So there will be no repurchase in a short time. Therefore, there will be peaks of popularity in only particular times.</li><li>Amazon or sellers have promotional campaign at the beginning of the year in order to increase the products popularity.</li><li>The data collection process is not objective. Only data of the beginning and middle of the year were collected.</li></ol><h1 id="Sales-Strategy"><a href="#Sales-Strategy" class="headerlink" title="Sales Strategy"></a>Sales Strategy</h1><p>Our goal is to allow Sunshine Company to expand market influence, continuously improve products and win a good corporate reputation among the public. We will make recommendations for Sunshine Company from two perspectives: How to Enhance the Reputation and How to Improve the Product.</p><h2 id="How-to-Enhance-the-Reputation"><a href="#How-to-Enhance-the-Reputation" class="headerlink" title="How to Enhance the Reputation"></a>How to Enhance the Reputation</h2><p>Reputation consists of two parts: popularity and star rating. First of all, from the perspective of popularity, we hope more people could pay attention to the company’s products. The most direct way to do this is to choose the right time to advertise and promote this product. we recommended that Sunshine Company are supposed to enhance propaganda and increase the discount at the beginning and middle of each year to increase product sales and popularity.</p><h2 id="How-to-Improve-the-Product"><a href="#How-to-Improve-the-Product" class="headerlink" title="How to Improve the Product"></a>How to Improve the Product</h2><p>On the other hand, for Star rating, the most fundamental influence of it is the product itself. By analyzing the content of the review, we could comprehend the relationship between star rating and review, furthermore, we could improve the product through the content review. Therefore, analyzing the content of the review is of vital importance. </p><p>From the previous model, we believe that there are three types of customers that worth pay attention to. Their reviews and attitude towards the product can effectively help us to improve the product.</p><ul><li>Amazon Vine Voices Members</li></ul><p>These people are selected by Amazon. They are the most trusted reviewers on Amazon to post opinions about new and pre-release items to help their fellow<br>customers make informed purchase decisions.</p><ul><li>People who have bought a lot of different products.</li></ul><p>These people have experienced more similar products than common customers, and they have a deeper understanding of this type of product. We have selected<br>nine such customers for Sunshine Company, their reviews are in the appendix.</p><ul><li>One-star rating customer</li></ul><p>our products or services may not meet the needs of all customers, so some of these customers give a one-star rating. Compared with five-star users who have very low-information reviews, these one-star rating users often directly mention the shortcomings of the product. So It is often helpful to look at these reviews<br>carefully.</p><p>we give the word cloud of one-star rating user of the microwave reviews below. There are words such as “service” and “warranty” in the word cloud. So we conclude that these users are dissatisfied with the product services and product warranty of the<br>microwave. In this way we can make specfic measures toward this issue. The word cloud of one-star rating users reviews by Pacifier and hair dryer is in the appendix. At the same time, we can find that users who give one star and users who give five stars tend to have more emotional reviews.</p><h1 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h1><p>Sunshine Company want to launch three new products in online marketplacemicrowave, pacifier and hair dryer. And they hire our team to help them to analyze the relevant products in the current market and make suggestions for their product sales.</p><p>At first,we process the dataset.We delete some data with missing values.At the same time,we find some data of click farmers.We also remove these useless data.We screen and integrate 15 attributes of the data set,leaving 11 basic attributes and 3 synthetic attributes.For a large number of texts in the review,we use the tokenization algorithm to cut the complex texts into simple words for our subsequent ananlysis.</p><p>Then,we analyze and visualize the attributes of the data set.In this way,we have a full understanding of the whole data set. We use the multi-demension evaluation system of “reviews evaluation products” and “helpful votes evaluation reviews”, through TF-IDF technology and MLP model to analyze the relationship between star rating, review and helpful votes,to help sunshine company to have a deeper understanding of products in the market.</p><p>Finally,on the basis of time ,we analyze the product heat,get a deeper understanding of customer review ,and provide a way for sunshine company to increase its products’popularity.</p><h1 id="Strenghths-and-Weaknesses"><a href="#Strenghths-and-Weaknesses" class="headerlink" title="Strenghths and Weaknesses"></a>Strenghths and Weaknesses</h1><h2 id="Strengths"><a href="#Strengths" class="headerlink" title="Strengths"></a>Strengths</h2><ul><li>Our model has strong interpretability</li></ul><p>The traditional MLP model has poor interpretability,and we continue to do lexical analysis and grammatical analysis on this basis, which enhances the interpretability of the whole model.</p><ul><li>We look at the product from the customer’s perspective</li></ul><p>We analyze the user’s emotion and build the user’s portrait through the user’s comments. At the same time, we find some customers who are meaningful to sunshine company.</p><h2 id="Weaknesses"><a href="#Weaknesses" class="headerlink" title="Weaknesses"></a>Weaknesses</h2><ul><li>There is a lack of analysis at the time level.</li></ul><p>At the time level, we only analyze the changes of product popularity, and there are a lot of contents that can be mined between time and review.</p><ul><li>We don’t build relationship among the three products.</li></ul><p>If our model can fully break the sales relationship among the three products in the market, it will be more helpful for sunshine company which sells the microwave, pacifier and hair dryer at the same time.</p><h1 id="Appendix-Product-Profile"><a href="#Appendix-Product-Profile" class="headerlink" title="Appendix: Product Profile"></a>Appendix: Product Profile</h1><center><img src="/eysblog_en/imgsource/mcm2020-figure-11.png" alt="" width="70%"/><p><font size=3 color="black">Figure 11: The Star Ratings and the Number of Reviews of Pacifier</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-figure-12.png" alt="" width="70%"/><p><font size=3 color="black">Figure 12: The Proportion of People Who Were in Amazon Vine and People Who Had Bought the Product</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-figure-13.png" alt="" width="70%"/><p><font size=3 color="black">Figure 13: The Wordcloud of Pacifier</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-figure-14.png" alt="" width="70%"/><p><font size=3 color="black">Figure 14: The Reviews’ Total Votes and Helpful Votes of a Pacifier Product</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-figure-15.png" alt="" width="70%"/><p><font size=3 color="black">Figure 15: The Star Ratings and the Number of Reviews of Pacifier</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-figure-16.png" alt="" width="70%"/><p><font size=3 color="black">Figure 16: The Proportion of People Who Were in Amazon Vine and People Who Had Bought the Product</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-figure-17.png" alt="" width="70%"/><p><font size=3 color="black">Figure 17: The Wordcloud of Pacifier</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-figure-18.png" alt="" width="70%"/><p><font size=3 color="black">Figure 18: The Reviews’ Total Votes and Helpful Votes of a Pacifier Product</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-figure-19.png" alt="" width="70%"/><p><font size=3 color="black">Figure 19: The WordCloud of Reviews with One Star about Pacifier</font></p></center><center><img src="/eysblog_en/imgsource/mcm2020-figure-20.png" alt="" width="70%"/><p><font size=3 color="black">Figure 20: The WordCloud of Reviews with One Star about Hair Dryer</font></p></center>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;This is my second time for MCM &amp;amp; ICM. I teamed up with two friends respectively from computer science major and electronic informatio</summary>
      
    
    
    
    <category term="Mathematical Modeling" scheme="https://enblog.crocodilezs.top/categories/Mathematical-Modeling/"/>
    
    <category term="NLP (Natural Language Processing)" scheme="https://enblog.crocodilezs.top/categories/Mathematical-Modeling/NLP-Natural-Language-Processing/"/>
    
    
  </entry>
  
  <entry>
    <title>Price Suggestion Challenge - A ML Algorithms Survey</title>
    <link href="https://enblog.crocodilezs.top/201911/2019-11-12-price-prediction/"/>
    <id>https://enblog.crocodilezs.top/201911/2019-11-12-price-prediction/</id>
    <published>2019-11-12T08:22:10.000Z</published>
    <updated>2022-08-25T13:24:40.556Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Topic"><a href="#Topic" class="headerlink" title="Topic"></a>Topic</h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>Considering the number of products sold online, product pricing becomes more difficult at scale. Apparel has strong seasonal pricing trends and is heavily influenced by brands, while electronics prices fluctuate based on product specifications. It is a meaningful question to help merchants effectively sell their goods by making reasonable pricing based on past information.</p><h2 id="Target"><a href="#Target" class="headerlink" title="Target"></a>Target</h2><p>The product description, product category and brand information is given and combined with the product price from the training data to set the price for the new product. For example,</p><center><img src="/eysblog_en/imgsource/img-price-prediction/1-1.png" alt="" width="100%" /></center><p>Obviously Versace’s clothes should be much higher in price than Metersbonwe’s clothes, and in the description of the goods, you can find a slight difference between the two descriptions. </p><blockquote><p>This project aims to analyze the text information, extract the important information from the text information and derive the potential relationship with the price。 </p></blockquote><h2 id="Analysis-of-atributes"><a href="#Analysis-of-atributes" class="headerlink" title="Analysis of atributes"></a>Analysis of atributes</h2><center><img src="/eysblog_en/imgsource/img-price-prediction/1-2.png" alt="" width="100%" /></center><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><ul><li><code>train.csv</code> training dataset (Include <code>price</code>)</li><li><code>test.csv</code> test dataset (Not include <code>price</code>) ; <code>label_test.csv</code> (Corresponding to the price of the test dataset)</li><li><code>f_test.csv</code> Final measurement data set (Not include <code>price</code>)</li></ul><h2 id="Evaluation-Indicators"><a href="#Evaluation-Indicators" class="headerlink" title="Evaluation Indicators"></a>Evaluation Indicators</h2><p>We used <code>Mean Squared Logarithmic Error</code> (MSLE) to evaluate the algorithm: </p><script type="math/tex; mode=display">MSLE = \cfrac{1}{n}\sum_{i=1}^n(log(p_i+1)-log(\alpha_i+1))^2\tag{1}</script><p>Which \(n\) means the number of samples in test dataset; \(p_i\)means the predicting price of sales; \(\alpha_i\) means the real price.</p><h1 id="Data-Process"><a href="#Data-Process" class="headerlink" title="Data Process"></a>Data Process</h1><h2 id="Learning-sample-code"><a href="#Learning-sample-code" class="headerlink" title="Learning sample code"></a>Learning sample code</h2><p>The sample code given was first tried to understand the general idea of solving this problem. The main processes to solve this price prediction problem are: importing data and data exploration, data pre-processing, model construction, price prediction and measurement.</p><h3 id="import-data-and-exploration"><a href="#import-data-and-exploration" class="headerlink" title="import data and exploration"></a>import data and exploration</h3><p>Import the data and get acknowledge with it.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data = pd.read_csv(<span class="string">&#x27;../data/4/train.csv&#x27;</span>, sep=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">&#x27;../data/4/test.csv&#x27;</span>,sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">train_data.info()</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;class &#x27;pandas.core.frame.DataFrame&#x27;&gt;</span><br><span class="line">RangeIndex: 300000 entries, 0 to 299999</span><br><span class="line">Data columns (total 8 columns):</span><br><span class="line">train_id             300000 non-null int64</span><br><span class="line">name                 300000 non-null object</span><br><span class="line">item_condition_id    300000 non-null int64</span><br><span class="line">category_name        298719 non-null object</span><br><span class="line">brand_name           171929 non-null object</span><br><span class="line">price                300000 non-null float64</span><br><span class="line">shipping             300000 non-null int64</span><br><span class="line">item_description     300000 non-null object</span><br><span class="line">dtypes: float64(1), int64(3), object(4)</span><br><span class="line">memory usage: 18.3+ MB</span><br></pre></td></tr></table></figure><h3 id="Data-Preprocess"><a href="#Data-Preprocess" class="headerlink" title="Data Preprocess"></a>Data Preprocess</h3><p>First of all, we need to remove <code>price</code> from the training data, and then remove <code>train_id</code> or <code>test_id</code> which are not useful. By looking at the data attributes above, we can see that <code>category_name</code> and <code>brand_name</code> have missing data, so the sample code is filled with <code>missing</code> directly.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">featureProcessing</span>(<span class="params">df</span>):</span><br><span class="line">    <span class="comment"># delete the data that will not be used</span></span><br><span class="line">    df = df.drop([<span class="string">&#x27;price&#x27;</span>, <span class="string">&#x27;test_id&#x27;</span>, <span class="string">&#x27;train_id&#x27;</span>], axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># deal with the missing value with a default value</span></span><br><span class="line">    df[<span class="string">&#x27;category_name&#x27;</span>] = df[<span class="string">&#x27;category_name&#x27;</span>].fillna(<span class="string">&#x27;missing&#x27;</span>).astype(<span class="built_in">str</span>)</span><br><span class="line">    df[<span class="string">&#x27;brand_name&#x27;</span>] = df[<span class="string">&#x27;brand_name&#x27;</span>].fillna(<span class="string">&#x27;missing&#x27;</span>).astype(<span class="built_in">str</span>)</span><br><span class="line">    df[<span class="string">&#x27;item_description&#x27;</span>] = df[<span class="string">&#x27;item_description&#x27;</span>].fillna(<span class="string">&#x27;No&#x27;</span>)</span><br><span class="line">    <span class="comment"># convert the data : int -&gt; str</span></span><br><span class="line">    df[<span class="string">&#x27;shipping&#x27;</span>] = df[<span class="string">&#x27;shipping&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">    df[<span class="string">&#x27;item_condition_id&#x27;</span>] = df[<span class="string">&#x27;item_condition_id&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure><h3 id="Model-Construction"><a href="#Model-Construction" class="headerlink" title="Model Construction"></a>Model Construction</h3><p>First of all, the input of the model is done and the matrix of word frequencies is generated by <code>CountVectorizer</code> and <code>TfidfVectorizer</code>. <code>Tfidf</code> is better because the number of occurrences of each word in all field clocks is taken into account and the generated word frequency matrix is weighted.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vectorizer = FeatureUnion([</span><br><span class="line">    (<span class="string">&#x27;name&#x27;</span>, CountVectorizer(ngram_range=(<span class="number">1</span>, <span class="number">2</span>), max_features=<span class="number">50000</span>, preprocessor=build_preprocessor_1(<span class="string">&#x27;name&#x27;</span>))),</span><br><span class="line">    (<span class="string">&#x27;category_name&#x27;</span>, CountVectorizer(token_pattern=<span class="string">&#x27;.+&#x27;</span>, preprocessor=build_preprocessor_1(<span class="string">&#x27;category_name&#x27;</span>))),</span><br><span class="line">    (<span class="string">&#x27;brand_name&#x27;</span>, CountVectorizer(token_pattern=<span class="string">&#x27;.+&#x27;</span>, preprocessor=build_preprocessor_1(<span class="string">&#x27;brand_name&#x27;</span>))),</span><br><span class="line">    (<span class="string">&#x27;shipping&#x27;</span>, CountVectorizer(token_pattern=<span class="string">&#x27;\d+&#x27;</span>, preprocessor=build_preprocessor_1(<span class="string">&#x27;shipping&#x27;</span>))),</span><br><span class="line">    (<span class="string">&#x27;item_condition_id&#x27;</span>, CountVectorizer(token_pattern=<span class="string">&#x27;\d+&#x27;</span>, preprocessor=build_preprocessor_1(<span class="string">&#x27;item_condition_id&#x27;</span>))),</span><br><span class="line">    (<span class="string">&#x27;item_description&#x27;</span>, TfidfVectorizer(ngram_range=(<span class="number">1</span>, <span class="number">3</span>),max_features=<span class="number">100000</span>, preprocessor=build_preprocessor_1(<span class="string">&#x27;item_description&#x27;</span>))),</span><br><span class="line">])</span><br></pre></td></tr></table></figure><p>Predict the price by Ridge Regression.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ridgeClassify</span>(<span class="params">train_data, train_label</span>):</span><br><span class="line">    ridgeClf = Ridge(</span><br><span class="line">        solver=<span class="string">&#x27;auto&#x27;</span>,</span><br><span class="line">        fit_intercept=<span class="literal">True</span>,</span><br><span class="line">        alpha=<span class="number">0.5</span>,</span><br><span class="line">        max_iter=<span class="number">500</span>,</span><br><span class="line">        normalize=<span class="literal">False</span>,</span><br><span class="line">        tol=<span class="number">0.05</span>)</span><br><span class="line">    <span class="comment"># Training</span></span><br><span class="line">    ridgeClf.fit(train_data, train_label)</span><br><span class="line">    <span class="keyword">return</span> ridgeClf</span><br></pre></td></tr></table></figure><p>By understanding the dataset and studying the sample code, we learned that there are three angles to start with to optimize the answer to this question.</p><ol><li>data preprocessing: How to handle missing values? How should the attributes be combined?</li><li>optimization when forming the word frequency matrix: adjusting the parameters of <code>CountVectorizer</code> and <code>TfidfVectorizer</code>.</li><li>Model selection and optimization: try models other than ridge regression, adjust model parameters.</li></ol><h2 id="Try-more-models"><a href="#Try-more-models" class="headerlink" title="Try more models"></a>Try more models</h2><p>In the sample code above, the result obtained using the ridge regression model is about 3.01. After the hints from the previous class and the online search, we are ready to try the <code>MLP</code> model and the <code>Lgmb</code> model again. After roughly trying both models, we decided to further optimize the model using <code>MLP</code>.</p><h3 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a><code>MLP</code></h3><p>The result of <code>MLP</code> is shown below:</p><center><img src="/eysblog_en/imgsource/img-price-prediction/2-2.png" alt="" width="100%" /></center><h3 id="LGBM"><a href="#LGBM" class="headerlink" title="LGBM"></a><code>LGBM</code></h3><p>The result of  <code>Lgbm</code> is shown below: </p><center><img src="/eysblog_en/imgsource/img-price-prediction/2-1.png" alt="" width="100%" /></center><h3 id="MLP-Combined-with-LGBM"><a href="#MLP-Combined-with-LGBM" class="headerlink" title="MLP Combined with LGBM"></a><code>MLP</code> Combined with <code>LGBM</code></h3><p><1> Preprocessing</p><ul><li>Import the data</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train = pd.read_csv(<span class="string">&#x27;data/train.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">&#x27;data/test.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line"><span class="comment"># train and test data are handled together</span></span><br><span class="line">df = pd.concat([train, test], axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><ul><li>Handling missing value</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Handling missing values</span></span><br><span class="line">   df[<span class="string">&#x27;category_name&#x27;</span>] = df[<span class="string">&#x27;category_name&#x27;</span>].fillna(<span class="string">&#x27;MISS&#x27;</span>).astype(<span class="built_in">str</span>)</span><br><span class="line">   df[<span class="string">&#x27;brand_name&#x27;</span>] = df[<span class="string">&#x27;brand_name&#x27;</span>].fillna(<span class="string">&#x27;missing&#x27;</span>).astype(<span class="built_in">str</span>)</span><br><span class="line">   df[<span class="string">&#x27;item_description&#x27;</span>] = df[<span class="string">&#x27;item_description&#x27;</span>].fillna(<span class="string">&#x27;No&#x27;</span>)</span><br><span class="line">   <span class="comment"># modifying data structure</span></span><br><span class="line">   df[<span class="string">&#x27;shipping&#x27;</span>] = df[<span class="string">&#x27;shipping&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">   df[<span class="string">&#x27;item_condition_id&#x27;</span>] = df[<span class="string">&#x27;item_condition_id&#x27;</span>].astype(<span class="built_in">str</span>)</span><br></pre></td></tr></table></figure><ul><li>Feature vectorization</li></ul><p>Use the <code>CountVectorizer</code> class in the <code>sklearn</code> library to vectorize the text features and use <code>FeatureUnion</code> for feature union.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vectorizer = FeatureUnion([</span><br><span class="line">        (<span class="string">&#x27;name&#x27;</span>, CountVectorizer(</span><br><span class="line">            ngram_range=(<span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            max_features=<span class="number">100000</span>,</span><br><span class="line">            preprocessor=build_preprocessor(<span class="string">&#x27;name&#x27;</span>))),</span><br><span class="line">        (<span class="string">&#x27;category_name&#x27;</span>, CountVectorizer(</span><br><span class="line">            token_pattern=<span class="string">&#x27;.+&#x27;</span>,</span><br><span class="line">            preprocessor=build_preprocessor(<span class="string">&#x27;category_name&#x27;</span>))),</span><br><span class="line">        (<span class="string">&#x27;brand_name&#x27;</span>, CountVectorizer(</span><br><span class="line">            token_pattern=<span class="string">&#x27;.+&#x27;</span>,</span><br><span class="line">            preprocessor=build_preprocessor(<span class="string">&#x27;brand_name&#x27;</span>))),</span><br><span class="line">        (<span class="string">&#x27;shipping&#x27;</span>, CountVectorizer(</span><br><span class="line">            token_pattern=<span class="string">&#x27;\d+&#x27;</span>,</span><br><span class="line">            preprocessor=build_preprocessor(<span class="string">&#x27;shipping&#x27;</span>))),</span><br><span class="line">        (<span class="string">&#x27;item_condition_id&#x27;</span>, CountVectorizer(</span><br><span class="line">            token_pattern=<span class="string">&#x27;\d+&#x27;</span>,</span><br><span class="line">            preprocessor=build_preprocessor(<span class="string">&#x27;item_condition_id&#x27;</span>))),</span><br><span class="line">        (<span class="string">&#x27;item_description&#x27;</span>, TfidfVectorizer(</span><br><span class="line">            ngram_range=(<span class="number">1</span>, <span class="number">3</span>),</span><br><span class="line">            max_features=<span class="number">200000</span>,</span><br><span class="line">            preprocessor=build_preprocessor(<span class="string">&#x27;item_description&#x27;</span>),</span><br><span class="line">            stop_words=<span class="string">&#x27;english&#x27;</span>)),</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure><p><2> Model Construction</p><p>The features were trained using the ridge regression model, the <code>Lgbm</code> model and the <code>mlp</code> model, respectively, and the solutions obtained in local tests were 3.01, 3.00, and 0.26, respectively.</p><ul><li>Ridge Regression</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ridge_classify</span>(<span class="params">train_data,train_label</span>):</span><br><span class="line">    <span class="comment">#model</span></span><br><span class="line">    model = Ridge(</span><br><span class="line">            solver=<span class="string">&#x27;auto&#x27;</span>,</span><br><span class="line">            fit_intercept=<span class="literal">True</span>,</span><br><span class="line">            alpha=<span class="number">0.4</span>,</span><br><span class="line">            max_iter=<span class="number">100</span>,</span><br><span class="line">            normalize=<span class="literal">False</span>,</span><br><span class="line">            tol=<span class="number">0.05</span>)</span><br><span class="line">    <span class="comment">#training</span></span><br><span class="line">    model.fit(train_data, train_label)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><ul><li><code>lgbm</code> Model</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lgbm_classify</span>(<span class="params">train_data,train_label</span>):</span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.75</span>,</span><br><span class="line">        <span class="string">&#x27;application&#x27;</span>: <span class="string">&#x27;regression&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">3</span>,</span><br><span class="line">        <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">100</span>,</span><br><span class="line">        <span class="string">&#x27;verbosity&#x27;</span>: -<span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;RMSE&#x27;</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    train_X, valid_X, train_y, valid_y = train_test_split(train_data, train_label, test_size=<span class="number">0.1</span>, random_state=<span class="number">144</span>)</span><br><span class="line">    d_train = lgb.Dataset(train_X, label=train_y)</span><br><span class="line">    d_valid = lgb.Dataset(valid_X, label=valid_y)</span><br><span class="line">    watchlist = [d_train, d_valid]</span><br><span class="line"></span><br><span class="line">    model = lgb.train(params, train_set=d_train, num_boost_round=<span class="number">2200</span>, valid_sets=watchlist, \</span><br><span class="line">                      early_stopping_rounds=<span class="number">50</span>, verbose_eval=<span class="number">100</span>)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><ul><li><code>mlp</code> Model</li></ul><p>The <code>MLP</code> model consists of two fully connected layers and a dropout layer, which is essentially a network of multiple hidden layers.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mlp_model</span>(<span class="params">train_data,train_label,row_train</span>):</span><br><span class="line">    model = Sequential()</span><br><span class="line">    <span class="comment"># fully connected layer</span></span><br><span class="line">    model.add(Dense(<span class="number">64</span>, input_shape=(row_train,), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">    <span class="comment"># DropOut layer</span></span><br><span class="line">    model.add(Dropout(<span class="number">0.4</span>))</span><br><span class="line">    <span class="comment"># fully connected layer + classifier</span></span><br><span class="line">    model.add(Dense(<span class="number">1</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;mean_squared_logarithmic_error&#x27;</span>,</span><br><span class="line">                  optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">                  metrics=[<span class="string">&#x27;accuracy&#x27;</span>]</span><br><span class="line">                  )</span><br><span class="line"></span><br><span class="line">    model.fit(train_data, train_label,</span><br><span class="line">              batch_size=<span class="number">300</span>,</span><br><span class="line">              epochs=<span class="number">1</span>,</span><br><span class="line">              )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model.predict(X_test)</span><br></pre></td></tr></table></figure><h2 id="Optimization-when-forming-word-frequency-matrix"><a href="#Optimization-when-forming-word-frequency-matrix" class="headerlink" title="Optimization when forming word frequency matrix"></a>Optimization when forming word frequency matrix</h2><p>In the sample code we tried to replace all <code>CountVectorizer</code> with <code>TdidfVectorizer</code> and then use the Ridge model for prediction, but the result is not much optimized, only up to 2.9.<br>Later, when using the <code>MLP</code>, we completely discarded the <code>CountVectorizer</code> and used only the <code>TdidfVectorizer</code>.</p><h2 id="Optimize-the-data-pre-processing-process"><a href="#Optimize-the-data-pre-processing-process" class="headerlink" title="Optimize the data pre-processing process"></a>Optimize the data pre-processing process</h2><p>The way we optimize the <code>MLP</code>, which is basically perfected above, is to <strong>try different combinations of features</strong>.</p><h3 id="Analysis-of-the-attributes"><a href="#Analysis-of-the-attributes" class="headerlink" title="Analysis of the attributes"></a>Analysis of the attributes</h3><p>First I analyzed the attributes:<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">item_condition_id    300000 non-null int64</span><br><span class="line">shipping             300000 non-null int64</span><br><span class="line"></span><br><span class="line">name                 300000 non-null object</span><br><span class="line">category_name        298719 non-null object</span><br><span class="line">brand_name           171929 non-null object</span><br><span class="line">item_description     300000 non-null object</span><br><span class="line"></span><br></pre></td></tr></table></figure><br><code>item_condition_id</code> and <code>shipping</code> are considered directly as inputs, while <code>name</code>, <code>category_name</code>, <code>brand_name</code>, <code>item_description</code> are considered for different combinations to try.</p><p>Before that, we found an example tutorial on data visualization to analyze the attributes of the data.<br>The optimal combination of inputs is obtained by observing the data.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train.head()</span><br></pre></td></tr></table></figure><center><img src="/eysblog_en/imgsource/img-price-prediction/1-3.png" alt="" width="100%" /></center><ol><li><code>price</code><br>By looking at the data after visualization we know why we have to do <code>log1p</code> on <code>price</code> to make the distribution of <code>price</code> better.</li></ol><center><img src="/eysblog_en/imgsource/img-price-prediction/1-4.png" alt="" width="100%" /></center><ol><li><code>category_name</code><br>Try to split the property into various subclasses and view the corresponding data.</li></ol><center><img src="/eysblog_en/imgsource/img-price-prediction/1-5.png" alt="" width="100%" /></center><ol><li><code>item_description</code><center><img src="/eysblog_en/imgsource/img-price-prediction/1-6.png" alt="" width="100%" /></center></li></ol><h3 id="Try-different-attributes-combination"><a href="#Try-different-attributes-combination" class="headerlink" title="Try different attributes combination"></a>Try different attributes combination</h3><ol><li>simply combine the attributes together in the sample code for text analysis, i.e. <code>name</code> + <code>item_condition_id</code> + <code>category_name</code> + <code>brand_name</code> + <code>shipping</code> + <code>item_description</code>. (6 inputs)</li><li>try <code>name</code>, <code>item_condition_id</code>, <code>shipping</code>,<code>category_name</code> + <code>item_description</code>, <code>brand_name</code>. (5 inputs)</li><li>try <code>name</code>, <code>item_condition_id</code>, <code>shipping</code>, <code>category_name</code> + <code>brand_name</code> + <code>item_description</code>. (4 inputs)</li><li>try <code>name</code>, <code>item_condition_id</code>, <code>shipping</code>, <code>name</code> + <code>category_name</code> + <code>brand_name</code> + <code>item_description</code>. (4 inputs)</li></ol><p>The results for the four combinations as input are very similar, except that combination 1 <code>MSLE</code> is around 0.4, combinations 2 and 3 are around 0.21, and combination 4 eventually runs to around 0.17. Combination 4 actually increases the weight of <code>name</code> to make the final result better.</p><h1 id="Final-source-code-and-experimental-results"><a href="#Final-source-code-and-experimental-results" class="headerlink" title="Final source code and experimental results"></a>Final source code and experimental results</h1><ol><li><p>Data preprocessing</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># data preprocessing</span></span><br><span class="line"><span class="comment"># There are 8 attributes, remove price, train_id will have no influence on the result.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_preprocess</span>(<span class="params">df</span>):</span><br><span class="line">    df[<span class="string">&#x27;name&#x27;</span>] = df[<span class="string">&#x27;name&#x27;</span>].fillna(<span class="string">&#x27;&#x27;</span>) + <span class="string">&#x27; &#x27;</span> + df[<span class="string">&#x27;brand_name&#x27;</span>].fillna(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    df[<span class="string">&#x27;text&#x27;</span>] = (df[<span class="string">&#x27;item_description&#x27;</span>].fillna(<span class="string">&#x27;&#x27;</span>) + <span class="string">&#x27; &#x27;</span> + df[<span class="string">&#x27;name&#x27;</span>] + <span class="string">&#x27; &#x27;</span> + df[<span class="string">&#x27;category_name&#x27;</span>].fillna(<span class="string">&#x27;&#x27;</span>))</span><br><span class="line">    <span class="keyword">return</span> df[[<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;text&#x27;</span>, <span class="string">&#x27;shipping&#x27;</span>, <span class="string">&#x27;item_condition_id&#x27;</span>]]</span><br></pre></td></tr></table></figure></li><li><p>Model Constructio</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fit_predict</span>(<span class="params">xs, y_train</span>):</span><br><span class="line">    X_train, X_test = xs</span><br><span class="line">    <span class="comment"># Configure the operation method of tf.Session, such as gpu operation or cpu operation</span></span><br><span class="line">    config = tf.ConfigProto(</span><br><span class="line">        <span class="comment"># Set the number of threads for multiple operations in parallel</span></span><br><span class="line">        intra_op_parallelism_threads=<span class="number">1</span>, use_per_session_threads=<span class="number">1</span>, inter_op_parallelism_threads=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># Session provides the environment for Operation execution and Tensor evaluation.</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session(graph=tf.Graph(), config=config) <span class="keyword">as</span> sess, timer(<span class="string">&#x27;fit_predict&#x27;</span>):</span><br><span class="line">        ks.backend.set_session(sess)</span><br><span class="line">        model_in = ks.Input(shape=(X_train.shape[<span class="number">1</span>],), dtype=<span class="string">&#x27;float32&#x27;</span>, sparse=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># ks.layers.Dense means the dimension of output</span></span><br><span class="line">        <span class="comment"># Dense full connected layer, equals to add one layer directly.</span></span><br><span class="line">        <span class="comment"># activation is the activation function.</span></span><br><span class="line">        out = ks.layers.Dense(<span class="number">192</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(model_in)</span><br><span class="line">        out = ks.layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(out)</span><br><span class="line">        out = ks.layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(out)</span><br><span class="line">        out = ks.layers.Dense(<span class="number">1</span>)(out)</span><br><span class="line">        model = ks.Model(model_in, out)</span><br><span class="line">        model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;mean_squared_error&#x27;</span>, optimizer=ks.optimizers.Adam(lr=<span class="number">3e-3</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            <span class="keyword">with</span> timer(<span class="string">f&#x27;epoch <span class="subst">&#123;i + <span class="number">1</span>&#125;</span>&#x27;</span>):</span><br><span class="line">                model.fit(x=X_train, y=y_train, batch_size=<span class="number">2</span> ** (<span class="number">11</span> + i), epochs=<span class="number">1</span>, verbose=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> model.predict(X_test)[:, <span class="number">0</span>]</span><br></pre></td></tr></table></figure></li><li><p>Model training and prediction</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    vectorizer = make_union(<span class="comment"># Assemble all transformers into a FeatureUnion. n_jobs means it can be done simultaneously</span></span><br><span class="line">        <span class="comment"># FunctionTransformer implements a custom transformation with no input validation when validate=False</span></span><br><span class="line">        <span class="comment"># TfidfVectorizer function, consider only the words in the first max_feature bits by word frequency, token_pattern=&#x27;\w+&#x27; matches at least one word</span></span><br><span class="line">        make_pipeline(FunctionTransformer(itemgetter(<span class="string">&#x27;name&#x27;</span>), validate=<span class="literal">False</span>), TfidfVectorizer(max_features=<span class="number">100000</span>, token_pattern=<span class="string">&#x27;\w+&#x27;</span>)),</span><br><span class="line">        make_pipeline(FunctionTransformer(itemgetter(<span class="string">&#x27;text&#x27;</span>), validate=<span class="literal">False</span>), TfidfVectorizer(max_features=<span class="number">100000</span>, token_pattern=<span class="string">&#x27;\w+&#x27;</span>)),</span><br><span class="line">        make_pipeline(FunctionTransformer(itemgetter([<span class="string">&#x27;shipping&#x27;</span>, <span class="string">&#x27;item_condition_id&#x27;</span>]), validate=<span class="literal">False</span>),</span><br><span class="line">                      FunctionTransformer(to_records, validate=<span class="literal">False</span>), DictVectorizer()),</span><br><span class="line">        n_jobs=<span class="number">4</span>)</span><br><span class="line">    <span class="comment"># StandardScaler() performs data normalization. Save the parameters (mean, variance) from the training set directly using its object to transform the test set data.</span></span><br><span class="line">    y_scaler = StandardScaler()</span><br><span class="line">    <span class="comment"># The with statement is used when accessing resources to ensure that the necessary &quot;cleanup&quot; operations are performed to release resources regardless of exceptions during use, such as automatic closure of files after use, automatic acquisition and release of locks in threads, etc.</span></span><br><span class="line">    <span class="keyword">with</span> timer(<span class="string">&#x27;process train&#x27;</span>):</span><br><span class="line">        train = pd.read_csv(<span class="string">&#x27;train.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        test = pd.read_csv(<span class="string">&#x27;test.csv&#x27;</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        <span class="comment"># remove &#x27;price&#x27;</span></span><br><span class="line">        train = train[train[<span class="string">&#x27;price&#x27;</span>] &gt; <span class="number">0</span>].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># normalization of price</span></span><br><span class="line">        y_train = y_scaler.fit_transform(np.log1p(train[<span class="string">&#x27;price&#x27;</span>].values.reshape(-<span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line">        X_train = vectorizer.fit_transform(data_preprocess(train)).astype(np.float32)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;X_train: <span class="subst">&#123;X_train.shape&#125;</span> of <span class="subst">&#123;X_train.dtype&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> timer(<span class="string">&#x27;process valid&#x27;</span>):</span><br><span class="line">        X_test = vectorizer.transform(data_preprocess(test)).astype(np.float32)</span><br><span class="line">    <span class="keyword">with</span> ThreadPool(processes=<span class="number">4</span>) <span class="keyword">as</span> pool:</span><br><span class="line">        Xb_train, Xb_test = [x.astype(np.<span class="built_in">bool</span>).astype(np.float32) <span class="keyword">for</span> x <span class="keyword">in</span> [X_train, X_test]]</span><br><span class="line">        xs = [[Xb_train, Xb_test], [X_train, X_test]] * <span class="number">2</span></span><br><span class="line">        <span class="comment"># prediction</span></span><br><span class="line">        y_pred = np.mean(pool.<span class="built_in">map</span>(partial(fit_predict, y_train=y_train), xs), axis=<span class="number">0</span>)</span><br><span class="line">    y_pred = np.expm1(y_scaler.inverse_transform(y_pred.reshape(-<span class="number">1</span>, <span class="number">1</span>))[:, <span class="number">0</span>])</span><br><span class="line">    <span class="comment"># print(type(y_pred))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Export prediction results to csv</span></span><br><span class="line">    test_id = np.array(<span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(y_pred)))</span><br><span class="line">    dataframe = pd.DataFrame(&#123;<span class="string">&#x27;test_id&#x27;</span>: test_id, <span class="string">&#x27;price&#x27;</span>: y_pred&#125;)</span><br><span class="line">    dataframe.to_csv(<span class="string">&quot;res.csv&quot;</span>, index=<span class="literal">False</span>, sep=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(&#x27;Valid MSLE: &#123;:.4f&#125;&#x27;.format(mean_squared_log_error(valid[&#x27;price&#x27;], y_pred)))</span></span><br></pre></td></tr></table></figure></li></ol><p>The final experimental result reached 0.179.</p><h1 id="Other-optimization-directions-in-the-MLP-model"><a href="#Other-optimization-directions-in-the-MLP-model" class="headerlink" title="Other optimization directions in the MLP model"></a>Other optimization directions in the <code>MLP</code> model</h1><ol><li>It can be observed that in the word cloud of <code>item_desciption</code>, there are words such as <code>shipping</code> and <code>free</code>, which may stand for free shipping and other meanings, and there is some duplication with the <code>shipping</code> attribute, and using it as a feature word to train the model will cause interference.</li><li>The information contained in a single keyword may not be comprehensive, and there may be great correlation between keywords.</li><li>In the final model <code>MLP</code> uses a four-layer perceptron, and the number of layers of the perceptron and the input size of each layer can be further tuned.</li></ol><h1 id="Experience"><a href="#Experience" class="headerlink" title="Experience"></a>Experience</h1><p>This experiment was very difficult and I didn’t know where to start.</p><p>After carefully studying the sample code given in the course and the content of data visualization and analysis, I got a preliminary understanding of both the dataset and the method of prediction.  </p><p>Since I was very unfamiliar with models such as <code>MLP</code> and <code>Lightgbm</code>, I started from the input point of view and experimented with the combination of different attributes to get the final and better results.  </p><p>In the following study, we should learn and understand the model more deeply, and try to create the model independently, instead of modifying other models that have been written.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1].<a href="https://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html">https://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html</a></p><p>[2].  <a href="https://github.com/pjankiewicz/mercari-solution">https://github.com/pjankiewicz/mercari-solution</a></p><p>[3].<a href="https://www.kaggle.com/thykhuely/mercari-interactive-eda-topic-modelling">https://www.kaggle.com/thykhuely/mercari-interactive-eda-topic-modelling</a></p><p>[4].<a href="https://wklchris.github.io/Py3-pandas.html#统计信息dfdescribe-svalue_counts--unique">https://wklchris.github.io/Py3-pandas.html#%E7%BB%9F%E8%AE%A1%E4%BF%A1%E6%81%AFdfdescribe-svalue_counts—unique</a></p><p>[5].<a href="https://zh.wikipedia.org/wiki/多层感知器">https://zh.wikipedia.org/wiki/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8</a></p><p>[6].<a href="https://blog.csdn.net/weixin_39807102/article/details/81912566">https://blog.csdn.net/weixin_39807102/article/details/81912566</a></p><p>[7].<a href="https://github.com/maiwen/NLP">https://github.com/maiwen/NLP</a></p><p>[8]. <a href="https://zh.wikipedia.org/wiki/正则表达式">https://zh.wikipedia.org/wiki/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F</a></p><p>[9].<a href="https://blog.csdn.net/u012609509/article/details/72911564">https://blog.csdn.net/u012609509/article/details/72911564</a></p><p>[10]. <a href="https://www.kaggle.com/tunguz/more-effective-ridge-lgbm-script-lb-0-44823">https://www.kaggle.com/tunguz/more-effective-ridge-lgbm-script-lb-0-44823</a></p><p>[11]. <a href="https://qiita.com/kazuhirokomoda/items/1e9b7ebcacf264b2d814">https://qiita.com/kazuhirokomoda/items/1e9b7ebcacf264b2d814</a></p><p>[12]. <a href="https://www.jianshu.com/p/c532424541ad">https://www.jianshu.com/p/c532424541ad</a></p><p>[13]. <a href="https://www.jiqizhixin.com/articles/2017-11-13-7">https://www.jiqizhixin.com/articles/2017-11-13-7</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Topic&quot;&gt;&lt;a href=&quot;#Topic&quot; class=&quot;headerlink&quot; title=&quot;Topic&quot;&gt;&lt;/a&gt;Topic&lt;/h1&gt;&lt;h2 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; </summary>
      
    
    
    
    <category term="NLP (Natural Language Processing)" scheme="https://enblog.crocodilezs.top/categories/NLP-Natural-Language-Processing/"/>
    
    
  </entry>
  
  <entry>
    <title>Linux开发环境及应用作业1</title>
    <link href="https://enblog.crocodilezs.top/201911/Linux%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%8F%8A%E5%BA%94%E7%94%A8%E4%BD%9C%E4%B8%9A%2020191031/"/>
    <id>https://enblog.crocodilezs.top/201911/Linux%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%8F%8A%E5%BA%94%E7%94%A8%E4%BD%9C%E4%B8%9A%2020191031/</id>
    <published>2019-10-31T14:10:14.000Z</published>
    <updated>2022-05-23T17:15:29.807Z</updated>
    
    <content type="html"><![CDATA[<h2 id="作业要求"><a href="#作业要求" class="headerlink" title="作业要求"></a>作业要求</h2><p>从因特网上搜索相关Web网页，处理网页<code>html</code>数据，从中提取出当前时间点北京各监测站的 PM2.5浓度，输出格式如下。要求：写出各个处 理步骤，并给出解释。<br>2018-03-15 13:00:00,海淀区万柳,73<br>2018-03-15 13:00:00,昌平镇,67<br>2018-03-15 13:00:00,奥体中心,66<br>2018-03-15 14:00:00,海淀区万柳,73<br>2018-03-15 14:00:00,昌平镇,73<br>2018-03-15 14:00:00,奥体中心,75</p><span id="more"></span><h2 id="实验过程"><a href="#实验过程" class="headerlink" title="实验过程"></a>实验过程</h2><h3 id="数据搜集"><a href="#数据搜集" class="headerlink" title="数据搜集"></a>数据搜集</h3><p>北京各监测站的<code>PM2.5</code>指数的数据来源网站：<a href="http://www.86pm25.com/city/beijing.html">http://www.86pm25.com/city/beijing.html</a><br><img src="https://s2.ax1x.com/2019/10/31/K5rmkV.jpg" alt=""></p><h3 id="数据整理及汇总"><a href="#数据整理及汇总" class="headerlink" title="数据整理及汇总"></a>数据整理及汇总</h3><p>先展示实现该操作的指令和最后的结果：<br><img src="https://s2.ax1x.com/2019/10/31/K5rl6J.jpg" alt=""><br><img src="https://s2.ax1x.com/2019/10/31/K5rQl4.jpg" alt=""><br><img src="https://s2.ax1x.com/2019/10/31/K5rJTx.jpg" alt=""></p><p>下面详细解释指令：</p><ol><li>首先利用<tr>标签把数据分成单独的行，<code>sed -e &#39;s/&lt;tr/\n&lt;tr/g&#39;</code></li><li>其次删掉html文件中的所有标签<code>-e &#39;s/&lt;[^&lt;&gt;]*&gt;/ /g</code>，把所有标签都换成了空格。</li><li>我先在html文件中寻找日期和时间，发现时间的那一行有“更新”的字样，于是建立awk文件，此时发现“更新”后面中文的冒号紧跟着日期，没发把日期分离开，于是先在中文冒号后面添加空格。顺便把日期和时间的格式改成标准的输出的格式。<code>-e &#39;s/：/： /g&#39; -e &#39;s/[年月]/-/g&#39; -e &#39;s/日//g -e &#39;s/时/:00:00/g&#39;</code><br><img src="https://s2.ax1x.com/2019/10/31/K5rrnA.jpg" alt=""></li><li>此时可以把时间和日期抽离出来了。在建立的awk文件中输入<code>/更新/ &#123;data = $2; time = $3&#125;</code></li><li>得到日期和时间 之后，我们去找监测站和pm2.5指数，发现在这些数据最后都有$m^3$单位在，于是在awk文件中添加<code>/m3/&#123;printf(&quot;%s %s,%s,%s\n&quot;,date, time, $1, $3);&#125;</code><br><img src="https://s2.ax1x.com/2019/10/31/K5rs0I.jpg" alt=""></li><li>最后把单位删掉，并输出到csv文件中即可。<code>awk -f flow.awk | sed -e &#39;s/[ug/m3]//g&#39; &gt; flow.csv</code></li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;作业要求&quot;&gt;&lt;a href=&quot;#作业要求&quot; class=&quot;headerlink&quot; title=&quot;作业要求&quot;&gt;&lt;/a&gt;作业要求&lt;/h2&gt;&lt;p&gt;从因特网上搜索相关Web网页，处理网页&lt;code&gt;html&lt;/code&gt;数据，从中提取出当前时间点北京各监测站的 PM2.5浓度，输出格式如下。要求：写出各个处 理步骤，并给出解释。&lt;br&gt;2018-03-15 13:00:00,海淀区万柳,73&lt;br&gt;2018-03-15 13:00:00,昌平镇,67&lt;br&gt;2018-03-15 13:00:00,奥体中心,66&lt;br&gt;2018-03-15 14:00:00,海淀区万柳,73&lt;br&gt;2018-03-15 14:00:00,昌平镇,73&lt;br&gt;2018-03-15 14:00:00,奥体中心,75&lt;/p&gt;</summary>
    
    
    
    <category term="Linux" scheme="https://enblog.crocodilezs.top/categories/Linux/"/>
    
    
    <category term="文本处理" scheme="https://enblog.crocodilezs.top/tags/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>神经网络前向传播和反向传播算法推导</title>
    <link href="https://enblog.crocodilezs.top/201911/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BD%9C%E4%B8%9A%E6%8A%A5%E5%91%8A/"/>
    <id>https://enblog.crocodilezs.top/201911/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BD%9C%E4%B8%9A%E6%8A%A5%E5%91%8A/</id>
    <published>2019-10-28T17:32:10.000Z</published>
    <updated>2022-05-23T16:54:41.206Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、目标"><a href="#一、目标" class="headerlink" title="一、目标"></a>一、目标</h2><ol><li>推导具有单隐层的神经网络的前向传播和反向传播算法，并进行编程（可以使用<code>sklearn</code>中的神经网络）。<ul><li>探讨10，30，100，300，1000，不同隐藏节点数对网络性能的影响。</li><li>探讨不同学习率和迭代次数对网络性能的影响。</li><li>改变数据的标准化方法，探讨对训练的影响。</li></ul></li><li>查阅资料说明什么是<code>Hebb</code>学习规则</li></ol><span id="more"></span><h2 id="二、推导单隐层神经网络的前向传播和反向传播算法"><a href="#二、推导单隐层神经网络的前向传播和反向传播算法" class="headerlink" title="二、推导单隐层神经网络的前向传播和反向传播算法"></a>二、推导单隐层神经网络的前向传播和反向传播算法</h2><p>参考资料：<a href="https://blog.csdn.net/Lucky_Go/article/details/89738286">https://blog.csdn.net/Lucky_Go/article/details/89738286</a><br><img src="image4\reduction1.jpg" alt=""><br><img src="image4\reduction2.jpg" alt=""><br><img src="image4\reduction3.jpg" alt=""><br><img src="image4\reduction4.jpg" alt=""></p><h2 id="三、算法实现"><a href="#三、算法实现" class="headerlink" title="三、算法实现"></a>三、算法实现</h2><p>参考资料：<a href="https://blog.csdn.net/zsx17/article/details/89342506">https://blog.csdn.net/zsx17/article/details/89342506</a></p><p>因为网上神经网络的代码基本都是用<code>tensorflow</code>实现的，这里是直接调库。在完成了作业的基本要求之后我也尝试了自己实现单隐层神经网络的代码（在实验报告的后部分）。</p><h3 id="1-载入数据"><a href="#1-载入数据" class="headerlink" title="1. 载入数据"></a>1. 载入数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1、载入数据</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.examples.tutorials.mnist.input_data <span class="keyword">as</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取mnist数据</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">&#x27;MNIST_data/&#x27;</span>, one_hot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="2-建立模型"><a href="#2-建立模型" class="headerlink" title="2. 建立模型"></a>2. 建立模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.建立模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.1 构建输入层</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>], name=<span class="string">&#x27;X&#x27;</span>)</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>], name=<span class="string">&#x27;Y&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.2 构建隐藏层</span></span><br><span class="line"><span class="comment"># 隐藏层神经元数量(随意设置）</span></span><br><span class="line">H1_NN = <span class="number">256</span></span><br><span class="line"><span class="comment"># 权重</span></span><br><span class="line">W1 = tf.Variable(tf.random_normal([<span class="number">784</span>, H1_NN]))</span><br><span class="line"><span class="comment"># 偏置项</span></span><br><span class="line">b1 = tf.Variable(tf.zeros([H1_NN]))</span><br><span class="line"></span><br><span class="line">Y1 = tf.nn.relu(tf.matmul(x, W1) + b1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.3 构建输出层</span></span><br><span class="line">W2 = tf.Variable(tf.random_normal([H1_NN, <span class="number">10</span>]))</span><br><span class="line">b2 = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line">forward = tf.matmul(Y1, W2) + b2</span><br><span class="line">pred = tf.nn.softmax(forward)</span><br></pre></td></tr></table></figure><h3 id="3-训练模型"><a href="#3-训练模型" class="headerlink" title="3. 训练模型"></a>3. 训练模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.训练模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.1 定义损失函数</span></span><br><span class="line"><span class="comment"># tensorflow提供了下面的函数，用于避免log(0)值为Nan造成数据不稳定</span></span><br><span class="line">loss_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=forward, labels=y))</span><br><span class="line"><span class="comment"># # 交叉熵损失函数</span></span><br><span class="line"><span class="comment"># loss_function = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.2 设置训练参数</span></span><br><span class="line">train_epochs = <span class="number">40</span>  <span class="comment"># 训练轮数</span></span><br><span class="line">batch_size = <span class="number">50</span>  <span class="comment"># 单次训练样本数(批次大小)</span></span><br><span class="line"><span class="comment"># 一轮训练的批次数</span></span><br><span class="line">total_batch = <span class="built_in">int</span>(mnist.train.num_examples / batch_size)</span><br><span class="line">display_step = <span class="number">1</span>  <span class="comment"># 显示粒数</span></span><br><span class="line">learning_rate = <span class="number">0.01</span>  <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.2 选择优化器</span></span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss_function)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.3定义准确率</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(pred, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.4 模型的训练</span></span><br><span class="line"><span class="comment"># 记录训练开始的时间</span></span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">startTime = time()</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(train_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> <span class="built_in">range</span>(total_batch):</span><br><span class="line">        <span class="comment"># 读取批次训练数据</span></span><br><span class="line">        xs, ys = mnist.train.next_batch(batch_size)</span><br><span class="line">        <span class="comment"># 执行批次训练</span></span><br><span class="line">        sess.run(optimizer, feed_dict=&#123;x: xs, y: ys&#125;)</span><br><span class="line">    <span class="comment"># 在total_batch批次数据训练完成后，使用验证数据计算误差和准确率，验证集不分批</span></span><br><span class="line">    loss, acc = sess.run([loss_function, accuracy], feed_dict=&#123;x: mnist.validation.images, y: mnist.validation.labels&#125;)</span><br><span class="line">    <span class="comment"># 打印训练过程中的详细信息</span></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;训练轮次：&#x27;</span>, <span class="string">&#x27;%02d&#x27;</span> % (epoch + <span class="number">1</span>),</span><br><span class="line">              <span class="string">&#x27;损失：&#x27;</span>, <span class="string">&#x27;&#123;:.9f&#125;&#x27;</span>.<span class="built_in">format</span>(loss),</span><br><span class="line">              <span class="string">&#x27;准确率：&#x27;</span>, <span class="string">&#x27;&#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(acc))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练结束&#x27;</span>)</span><br><span class="line"><span class="comment"># 显示总运行时间</span></span><br><span class="line">duration = time() - startTime</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;总运行时间为：&quot;</span>, <span class="string">&quot;&#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(duration))</span><br></pre></td></tr></table></figure><h3 id="4-模型评估"><a href="#4-模型评估" class="headerlink" title="4. 模型评估"></a>4. 模型评估</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 4.评估模型</span></span><br><span class="line">accu_test = sess.run(accuracy,</span><br><span class="line">                     feed_dict=&#123;x: mnist.test.images, y: mnist.test.labels&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试集准确率：&#x27;</span>, accu_test)</span><br></pre></td></tr></table></figure><h3 id="5-应用模型"><a href="#5-应用模型" class="headerlink" title="5. 应用模型"></a>5. 应用模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 5.应用模型</span></span><br><span class="line">prediction_result = sess.run(tf.argmax(pred, <span class="number">1</span>), feed_dict=&#123;x: mnist.test.images&#125;)</span><br><span class="line"><span class="comment"># 查看预测结果的前10项</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;前10项的结果：&quot;</span>, prediction_result[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.1找出预测错误的样本</span></span><br><span class="line">compare_lists = prediction_result == np.argmax(mnist.test.labels, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(compare_lists)</span><br><span class="line">err_lists = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(compare_lists)) <span class="keyword">if</span> compare_lists[i] == <span class="literal">False</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;预测错误的图片：&#x27;</span>, err_lists)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;预测错误图片的总数：&#x27;</span>, <span class="built_in">len</span>(err_lists))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个输出错误分类的函数</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">print_predict_errs</span>(<span class="params">labels,  <span class="comment"># 标签列表</span></span></span><br><span class="line"><span class="params">                       prediction</span>):  <span class="comment"># 预测值列表</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    compare_lists = (prediction == np.argmax(labels, <span class="number">1</span>))</span><br><span class="line">    err_lists = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(compare_lists)) <span class="keyword">if</span> compare_lists[i] == <span class="literal">False</span>]</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> err_lists:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;index=&#x27;</span> + <span class="built_in">str</span>(x) + <span class="string">&#x27;标签值=&#x27;</span>, np.argmax(labels[x]), <span class="string">&#x27;预测值=&#x27;</span>, prediction[x])</span><br><span class="line">        count = count + <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;总计：&quot;</span> + <span class="built_in">str</span>(count))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print_predict_errs(labels=mnist.test.labels, prediction=prediction_result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_images_labels_prediction</span>(<span class="params">images,  <span class="comment"># 图像列表</span></span></span><br><span class="line"><span class="params">                                  labels,  <span class="comment"># 标签列表</span></span></span><br><span class="line"><span class="params">                                  predication,  <span class="comment"># 预测值列表</span></span></span><br><span class="line"><span class="params">                                  index,  <span class="comment"># 从第index个开始显示</span></span></span><br><span class="line"><span class="params">                                  num=<span class="number">10</span></span>):  <span class="comment"># 缺省一次显示10幅</span></span><br><span class="line">    fig = plt.gcf()  <span class="comment"># 获取当前图表，get current figure</span></span><br><span class="line">    fig.set_size_inches(<span class="number">10</span>, <span class="number">12</span>)  <span class="comment"># 设为英寸，1英寸=2.53厘米</span></span><br><span class="line">    <span class="keyword">if</span> num &gt; <span class="number">25</span>:</span><br><span class="line">        num = <span class="number">25</span>  <span class="comment"># 最多显示25个子图</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num):</span><br><span class="line">        ax = plt.subplot(<span class="number">5</span>, <span class="number">5</span>, i + <span class="number">1</span>)  <span class="comment"># 获取当前要处理的子图</span></span><br><span class="line">        <span class="comment"># 显示第index图像</span></span><br><span class="line">        ax.imshow(np.reshape(images[index], (<span class="number">28</span>, <span class="number">28</span>)), cmap=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建该图上显示的title</span></span><br><span class="line">        title = <span class="string">&#x27;label=&#x27;</span> + <span class="built_in">str</span>(np.argmax(labels[index]))</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(predication) &gt; <span class="number">0</span>:</span><br><span class="line">            title += <span class="string">&quot;,predict=&quot;</span> + <span class="built_in">str</span>(predication[index])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 显示图上的title信息</span></span><br><span class="line">        ax.set_title(title, fontsize=<span class="number">10</span>)</span><br><span class="line">        ax.set_xticks([])  <span class="comment"># 不显示坐标轴</span></span><br><span class="line">        ax.set_yticks([])</span><br><span class="line">        index += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plot_images_labels_prediction(mnist.test.images,</span><br><span class="line">                              mnist.test.labels,</span><br><span class="line">                              prediction_result, <span class="number">10</span>, <span class="number">25</span>)</span><br><span class="line">plot_images_labels_prediction(mnist.test.images,</span><br><span class="line">                              mnist.test.labels,</span><br><span class="line">                              prediction_result, <span class="number">610</span>, <span class="number">20</span>)</span><br></pre></td></tr></table></figure><h3 id="6-结果展示"><a href="#6-结果展示" class="headerlink" title="6. 结果展示"></a>6. 结果展示</h3><p>上面的代码中隐层节点个数为256个，学习率为0.01，迭代次数为40次。训练结果如下：</p><p><img src="image4\256-0.01-40.jpg" alt=""></p><p>部分分类图像如下所示：</p><p><img src="image4\256-0.01-40-1.jpg" alt=""></p><p><img src="image4\256-0.01-40-2.jpg" alt=""></p><h2 id="四、算法调优"><a href="#四、算法调优" class="headerlink" title="四、算法调优"></a>四、算法调优</h2><p>在上面的模型中隐层结点数为256，学习率为0.01，迭代次数为40次。</p><p>下面分别从隐层节点数、学习率和迭代次数三个角度进行调优。</p><h3 id="1-隐层节点数"><a href="#1-隐层节点数" class="headerlink" title="1. 隐层节点数"></a>1. 隐层节点数</h3><p>将隐层节点数设为10，得到的结果如下图所示：</p><p><img src="image4\10-0.01-40.jpg" alt=""></p><p>将隐层节点设为30，100，300，1000的效果不再具体展示，效果如下所示：</p><div class="table-container"><table><thead><tr><th style="text-align:center">隐层节点个数</th><th style="text-align:center">总运行时间/s</th><th style="text-align:center">预测错误的图片数</th><th style="text-align:center">准确率</th></tr></thead><tbody><tr><td style="text-align:center">10</td><td style="text-align:center">46.29</td><td style="text-align:center">736</td><td style="text-align:center">0.9264</td></tr><tr><td style="text-align:center">30</td><td style="text-align:center">43.46</td><td style="text-align:center">528</td><td style="text-align:center">0.9472</td></tr><tr><td style="text-align:center">100</td><td style="text-align:center">59.06</td><td style="text-align:center">343</td><td style="text-align:center">0.9657</td></tr><tr><td style="text-align:center">256</td><td style="text-align:center">84.48</td><td style="text-align:center">249</td><td style="text-align:center">0.9751</td></tr><tr><td style="text-align:center">300</td><td style="text-align:center">76.64</td><td style="text-align:center">269</td><td style="text-align:center">0.9731</td></tr><tr><td style="text-align:center">1000</td><td style="text-align:center">302.27</td><td style="text-align:center">240</td><td style="text-align:center">0.976</td></tr></tbody></table></div><p>由表可知，准确率随着隐层节点个数的增加而增加，增加速率逐步减少。</p><h3 id="2-学习率"><a href="#2-学习率" class="headerlink" title="2. 学习率"></a>2. 学习率</h3><p>学习率分别为0.005，0.01， 0.02， 0.1，隐层节点数选择256，迭代次数选择40。分类结果如下：</p><div class="table-container"><table><thead><tr><th style="text-align:center">学习率</th><th style="text-align:center">总运行时间/s</th><th style="text-align:center">预测错误的图片数</th><th style="text-align:center">准确率</th></tr></thead><tbody><tr><td style="text-align:center">0.005</td><td style="text-align:center">78.81</td><td style="text-align:center">231</td><td style="text-align:center">0.9769</td></tr><tr><td style="text-align:center">0.01</td><td style="text-align:center">84.48</td><td style="text-align:center">249</td><td style="text-align:center">0.9751</td></tr><tr><td style="text-align:center">0.02</td><td style="text-align:center">69.72</td><td style="text-align:center">446</td><td style="text-align:center">0.9554</td></tr><tr><td style="text-align:center">0.1</td><td style="text-align:center">73.87</td><td style="text-align:center">2561</td><td style="text-align:center">0.7439</td></tr></tbody></table></div><p>由表可知，准确率随着学习率的增加而降低。在学习率低于0.01时，图片分类的准确率提升的速率较小。</p><h3 id="3-迭代次数"><a href="#3-迭代次数" class="headerlink" title="3. 迭代次数"></a>3. 迭代次数</h3><p>迭代次数分别为20，40，100，隐层节点数选择256，学习率选择0.01。分类结果如下：</p><div class="table-container"><table><thead><tr><th style="text-align:center">迭代次数</th><th style="text-align:center">总运行时间/s</th><th style="text-align:center">预测错误的图片数</th><th style="text-align:center">准确率</th></tr></thead><tbody><tr><td style="text-align:center">20</td><td style="text-align:center">37.12</td><td style="text-align:center">307</td><td style="text-align:center">0.9693</td></tr><tr><td style="text-align:center">40</td><td style="text-align:center">84.48</td><td style="text-align:center">249</td><td style="text-align:center">0.9751</td></tr><tr><td style="text-align:center">100</td><td style="text-align:center">184.39</td><td style="text-align:center">239</td><td style="text-align:center">0.9761</td></tr></tbody></table></div><p>由表可知，迭代次数对总运行时间的影响率很大，准确率随着迭代次数的增加而增加，但对准确率起决定因素的还是隐层的节点个数以及学习率。</p><h3 id="4-改变数据标准化方法"><a href="#4-改变数据标准化方法" class="headerlink" title="4. 改变数据标准化方法"></a>4. 改变数据标准化方法</h3><h4 id="最大-最小规范化"><a href="#最大-最小规范化" class="headerlink" title="最大-最小规范化"></a>最大-最小规范化</h4><h4 id="Z-score规范化"><a href="#Z-score规范化" class="headerlink" title="Z-score规范化"></a><code>Z-score</code>规范化</h4><h2 id="五、Hebb学习规则"><a href="#五、Hebb学习规则" class="headerlink" title="五、Hebb学习规则"></a>五、<code>Hebb</code>学习规则</h2><p>参考资料：<a href="https://baike.baidu.com/item/Hebb学习规则/3061563?fr=aladdin">https://baike.baidu.com/item/Hebb%E5%AD%A6%E4%B9%A0%E8%A7%84%E5%88%99/3061563?fr=aladdin</a></p><p><code>Hebb</code>学习规则是一个无监督学习规则，这种学习的结果是使网络能够提取训练集的统计特性，从而把输入信息按照它们的相似性程度划分为若干类。这一点与人类观察和认识世界的过程非常吻合，人类观察和认识世界在相当程度上就是在根据事物的统计特征进行分类。<code>Hebb</code>学习规则只根据神经元连接间的激活水平改变权值，因此这种方法又称为相关学习或并联学习。</p><p>无监督学习规则<br> 唐纳德·赫布（1904-1985）是加拿大著名生理心理学家。<code>Hebb</code>学习规则与“条件反射”机理一致，并且已经得到了神经细胞学说的证实。<br> 巴甫洛夫的条件反射实验：每次给狗喂食前都先响铃，时间一长，狗就会将铃声和食物联系起来。以后如果响铃但是不给食物，狗也会流口水。<br> 受该实验的启发，Hebb的理论认为在同一时间被激发的神经元间的联系会被强化。比如，铃声响时一个神经元被激发，在同一时间食物的出现会激发附近的另一个神经元，那么这两个神经元间的联系就会强化，从而记住这两个事物之间存在着联系。相反，如果两个神经元总是不能同步激发，那么它们间的联系将会越来越弱。<br> <code>Hebb</code>学习律可表示为：<br>$W<em>{ij}(t+1)=W</em>{ij}(t)+a⋅y<em>i⋅y_j$<br>$W</em>{ij}(t+1)=W_{ij}(t)+a⋅y_i⋅y_j$</p><p> 其中$W<em>{ij}$表示神经元$j$到神经元$i$的连接权，$y_i$与$y_j$表示两个神经元的输出，$a$是表示学习速率的常数，如果$y_i$与$y_j$同时被激活，即$y_i$与$y_j$同时为正，那么$W</em>{ij}$将增大。如果$y<em>i$被激活，而$y_j$处于抑制状态，即$y_i$为正$y_j$为负，那么$W</em>{ij}$将变小。</p><p><img src="https://images2015.cnblogs.com/blog/520787/201510/520787-20151021081107630-1544768706.png" alt=""></p><h2 id="六、-自己实现单隐层神经网络"><a href="#六、-自己实现单隐层神经网络" class="headerlink" title="六、 自己实现单隐层神经网络"></a>六、 自己实现单隐层神经网络</h2><p>参考资料：<a href="https://blog.csdn.net/hellozhxy/article/details/81055391">https://blog.csdn.net/hellozhxy/article/details/81055391</a></p><p>网络结构的函数定义：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">layer_sizes</span>(<span class="params">X, Y</span>):</span><br><span class="line">    n_x = X.shape[<span class="number">0</span>] <span class="comment"># size of input layer</span></span><br><span class="line">    n_h = <span class="number">4</span> <span class="comment"># size of hidden layer</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>] <span class="comment"># size of output layer</span></span><br><span class="line">    <span class="keyword">return</span> (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><p>参数初始化函数：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_parameters</span>(<span class="params">n_x, n_h, n_y</span>):</span><br><span class="line">    W1 = np.random.randn(n_h, n_x)*<span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)*<span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>)) </span><br><span class="line">   </span><br><span class="line">    <span class="keyword">assert</span> (W1.shape == (n_h, n_x))    </span><br><span class="line">    <span class="keyword">assert</span> (b1.shape == (n_h, <span class="number">1</span>))    </span><br><span class="line">    <span class="keyword">assert</span> (W2.shape == (n_y, n_h))    </span><br><span class="line">    <span class="keyword">assert</span> (b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line">    parameters = &#123;<span class="string">&quot;W1&quot;</span>: W1, </span><br><span class="line">                  <span class="string">&quot;b1&quot;</span>: b1,                 </span><br><span class="line">                  <span class="string">&quot;W2&quot;</span>: W2,                  </span><br><span class="line">                  <span class="string">&quot;b2&quot;</span>: b2&#125;   </span><br><span class="line">                   </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>前向传播计算函数：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_propagation</span>(<span class="params">X, parameters</span>):</span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary &quot;parameters&quot;</span></span><br><span class="line">    W1 = parameters[<span class="string">&#x27;W1&#x27;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&#x27;b1&#x27;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&#x27;W2&#x27;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&#x27;b2&#x27;</span>]    </span><br><span class="line">    <span class="comment"># Implement Forward Propagation to calculate A2 (probabilities)</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = np.tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2, Z1) + b2</span><br><span class="line">    A2 = sigmoid(Z2)    </span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = &#123;<span class="string">&quot;Z1&quot;</span>: Z1,                   </span><br><span class="line">             <span class="string">&quot;A1&quot;</span>: A1,                   </span><br><span class="line">             <span class="string">&quot;Z2&quot;</span>: Z2,                  </span><br><span class="line">             <span class="string">&quot;A2&quot;</span>: A2&#125;    </span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure><p>计算损失函数：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost</span>(<span class="params">A2, Y, parameters</span>):</span><br><span class="line">    m = Y.shape[<span class="number">1</span>] <span class="comment"># number of example</span></span><br><span class="line">    <span class="comment"># Compute the cross-entropy cost</span></span><br><span class="line">    logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(<span class="number">1</span>-A2), <span class="number">1</span>-Y)</span><br><span class="line">    cost = -<span class="number">1</span>/m * np.<span class="built_in">sum</span>(logprobs)</span><br><span class="line">    cost = np.squeeze(cost)     <span class="comment"># makes sure cost is the dimension we expect.</span></span><br><span class="line">    <span class="keyword">assert</span>(<span class="built_in">isinstance</span>(cost, <span class="built_in">float</span>))    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><p>反向传播函数：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backward_propagation</span>(<span class="params">parameters, cache, X, Y</span>):</span><br><span class="line">    m = X.shape[<span class="number">1</span>]    </span><br><span class="line">    <span class="comment"># First, retrieve W1 and W2 from the dictionary &quot;parameters&quot;.</span></span><br><span class="line">    W1 = parameters[<span class="string">&#x27;W1&#x27;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&#x27;W2&#x27;</span>]    </span><br><span class="line">    <span class="comment"># Retrieve also A1 and A2 from dictionary &quot;cache&quot;.</span></span><br><span class="line">    A1 = cache[<span class="string">&#x27;A1&#x27;</span>]</span><br><span class="line">    A2 = cache[<span class="string">&#x27;A2&#x27;</span>]    </span><br><span class="line">    <span class="comment"># Backward propagation: calculate dW1, db1, dW2, db2. </span></span><br><span class="line">    dZ2 = A2-Y</span><br><span class="line">    dW2 = <span class="number">1</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1</span>/m * np.<span class="built_in">sum</span>(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dZ1 = np.dot(W2.T, dZ2)*(<span class="number">1</span>-np.power(A1, <span class="number">2</span>))</span><br><span class="line">    dW1 = <span class="number">1</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1</span>/m * np.<span class="built_in">sum</span>(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    grads = &#123;<span class="string">&quot;dW1&quot;</span>: dW1,</span><br><span class="line">             <span class="string">&quot;db1&quot;</span>: db1,                      </span><br><span class="line">             <span class="string">&quot;dW2&quot;</span>: dW2,             </span><br><span class="line">             <span class="string">&quot;db2&quot;</span>: db2&#125;   </span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><p>权值更新函数：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">update_parameters</span>(<span class="params">parameters, grads, learning_rate = <span class="number">1.2</span></span>):</span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary &quot;parameters&quot;</span></span><br><span class="line">    W1 = parameters[<span class="string">&#x27;W1&#x27;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&#x27;b1&#x27;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&#x27;W2&#x27;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&#x27;b2&#x27;</span>]    </span><br><span class="line">    <span class="comment"># Retrieve each gradient from the dictionary &quot;grads&quot;</span></span><br><span class="line">    dW1 = grads[<span class="string">&#x27;dW1&#x27;</span>]</span><br><span class="line">    db1 = grads[<span class="string">&#x27;db1&#x27;</span>]</span><br><span class="line">    dW2 = grads[<span class="string">&#x27;dW2&#x27;</span>]</span><br><span class="line">    db2 = grads[<span class="string">&#x27;db2&#x27;</span>]    </span><br><span class="line">    <span class="comment"># Update rule for each parameter</span></span><br><span class="line">    W1 -= dW1 * learning_rate</span><br><span class="line">    b1 -= db1 * learning_rate</span><br><span class="line">    W2 -= dW2 * learning_rate</span><br><span class="line">    b2 -= db2 * learning_rate</span><br><span class="line">    parameters = &#123;<span class="string">&quot;W1&quot;</span>: W1, </span><br><span class="line">                  <span class="string">&quot;b1&quot;</span>: b1,            </span><br><span class="line">                  <span class="string">&quot;W2&quot;</span>: W2,   </span><br><span class="line">                  <span class="string">&quot;b2&quot;</span>: b2&#125;    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>最终的神经网络模型：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">nn_model</span>(<span class="params">X, Y, n_h, num_iterations = <span class="number">10000</span>, print_cost=<span class="literal">False</span></span>):</span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>]</span><br><span class="line">    n_y = layer_sizes(X, Y)[<span class="number">2</span>]    </span><br><span class="line">    <span class="comment"># Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: &quot;n_x, n_h, n_y&quot;. Outputs = &quot;W1, b1, W2, b2, parameters&quot;.</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">    W1 = parameters[<span class="string">&#x27;W1&#x27;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&#x27;b1&#x27;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&#x27;W2&#x27;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&#x27;b2&#x27;</span>]    </span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_iterations):        </span><br><span class="line">    <span class="comment"># Forward propagation. Inputs: &quot;X, parameters&quot;. Outputs: &quot;A2, cache&quot;.</span></span><br><span class="line">        A2, cache = forward_propagation(X, parameters)        </span><br><span class="line">        <span class="comment"># Cost function. Inputs: &quot;A2, Y, parameters&quot;. Outputs: &quot;cost&quot;.</span></span><br><span class="line">        cost = compute_cost(A2, Y, parameters)        </span><br><span class="line">        <span class="comment"># Backpropagation. Inputs: &quot;parameters, cache, X, Y&quot;. Outputs: &quot;grads&quot;.</span></span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)        </span><br><span class="line">        <span class="comment"># Gradient descent parameter update. Inputs: &quot;parameters, grads&quot;. Outputs: &quot;parameters&quot;.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate=<span class="number">1.2</span>)        </span><br><span class="line">        <span class="comment"># Print the cost every 1000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:            </span><br><span class="line">            <span class="built_in">print</span> (<span class="string">&quot;Cost after iteration %i: %f&quot;</span> %(i, cost))    </span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;一、目标&quot;&gt;&lt;a href=&quot;#一、目标&quot; class=&quot;headerlink&quot; title=&quot;一、目标&quot;&gt;&lt;/a&gt;一、目标&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;推导具有单隐层的神经网络的前向传播和反向传播算法，并进行编程（可以使用&lt;code&gt;sklearn&lt;/code&gt;中的神经网络）。&lt;ul&gt;
&lt;li&gt;探讨10，30，100，300，1000，不同隐藏节点数对网络性能的影响。&lt;/li&gt;
&lt;li&gt;探讨不同学习率和迭代次数对网络性能的影响。&lt;/li&gt;
&lt;li&gt;改变数据的标准化方法，探讨对训练的影响。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;查阅资料说明什么是&lt;code&gt;Hebb&lt;/code&gt;学习规则&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="机器学习" scheme="https://enblog.crocodilezs.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="神经网络" scheme="https://enblog.crocodilezs.top/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Finds算法和ID3算法</title>
    <link href="https://enblog.crocodilezs.top/201910/FINDS%E7%AE%97%E6%B3%95%E5%92%8CID3%E7%AE%97%E6%B3%95/"/>
    <id>https://enblog.crocodilezs.top/201910/FINDS%E7%AE%97%E6%B3%95%E5%92%8CID3%E7%AE%97%E6%B3%95/</id>
    <published>2019-10-28T04:21:10.000Z</published>
    <updated>2022-05-23T17:10:07.174Z</updated>
    
    <content type="html"><![CDATA[<h2 id="作业要求"><a href="#作业要求" class="headerlink" title="作业要求"></a>作业要求</h2><ol><li>实现<code>FINDS</code>算法</li><li>实现<code>ID3</code>算法</li></ol><ul><li>不要调库自己写。如果有能力可以继续用课件里的数据集测试两个算法（用天气的4条记录测试<code>FINDS</code>，用贷款的15条记录测试<code>ID3</code>）给出训练误差测试误差等；  </li><li>再有能力可以使用更大的数据集测试算法。</li></ul><span id="more"></span><h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><h3 id="FINDS算法"><a href="#FINDS算法" class="headerlink" title="FINDS算法"></a><code>FINDS</code>算法</h3><ol><li>目标：寻找极大特殊假设。</li><li>从假设集合H中最特殊的假设开始。在该假设不能正确地划分一个正例的时候将其进行一般化。算法如下：<br><img src="https://s2.ax1x.com/2019/10/22/K3ymOH.jpg" alt="算法流程"></li><li><code>FINDS</code>算法是一种利用<code>more-general-than</code>的偏序结构来搜索假设空间的方法，这一搜索沿着偏序链，从较特殊的假设逐渐演变为较一般的假设。</li><li>算法<code>Python</code>实现：</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string"> Created on 2019/10/21 21:02</span></span><br><span class="line"><span class="string"> FINDS</span></span><br><span class="line"><span class="string"> @Author  : Zhouy</span></span><br><span class="line"><span class="string"> @Blog    : www.crocodilezs.top</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create dataset</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">CreateDataset</span>():</span><br><span class="line">    dataset = [[<span class="string">&#x27;Sunny&#x27;</span>, <span class="string">&#x27;Warm&#x27;</span>, <span class="string">&#x27;Normal&#x27;</span>, <span class="string">&#x27;Strong&#x27;</span>, <span class="string">&#x27;Warm&#x27;</span>, <span class="string">&#x27;Same&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>],</span><br><span class="line">               [<span class="string">&#x27;Sunny&#x27;</span>, <span class="string">&#x27;Warm&#x27;</span>, <span class="string">&#x27;High&#x27;</span>, <span class="string">&#x27;Strong&#x27;</span>, <span class="string">&#x27;Warm&#x27;</span>, <span class="string">&#x27;Same&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>],</span><br><span class="line">               [<span class="string">&#x27;Rainy&#x27;</span>, <span class="string">&#x27;Cold&#x27;</span>, <span class="string">&#x27;High&#x27;</span>, <span class="string">&#x27;Strong&#x27;</span>, <span class="string">&#x27;Warm&#x27;</span>, <span class="string">&#x27;Change&#x27;</span>, <span class="string">&#x27;No&#x27;</span>],</span><br><span class="line">               [<span class="string">&#x27;Sunny&#x27;</span>, <span class="string">&#x27;Warm&#x27;</span>, <span class="string">&#x27;High&#x27;</span>, <span class="string">&#x27;Strong&#x27;</span>, <span class="string">&#x27;Cold&#x27;</span>, <span class="string">&#x27;Change&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>]]</span><br><span class="line">    labels = [<span class="string">&#x27;Sky&#x27;</span>, <span class="string">&#x27;Temp&#x27;</span>, <span class="string">&#x27;Humidity&#x27;</span>, <span class="string">&#x27;Wind&#x27;</span>, <span class="string">&#x27;Water&#x27;</span>, <span class="string">&#x27;Forest&#x27;</span>, <span class="string">&#x27;OutdoorSport&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> dataset, labels</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find one version space by using FINDS</span></span><br><span class="line"><span class="comment"># &#x27;/&#x27; means null, and &#x27;*&#x27; means generalization</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">FINDS</span>(<span class="params">dataset</span>):</span><br><span class="line">    constraint = [<span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;/&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> dataset:</span><br><span class="line">        <span class="keyword">if</span> item[-<span class="number">1</span>] == <span class="string">&#x27;Yes&#x27;</span>:</span><br><span class="line">            <span class="comment"># only go through positive instances</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(item)-<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span>(item[i] != constraint[i] <span class="keyword">and</span> constraint[i] != <span class="string">&#x27;*&#x27;</span>):</span><br><span class="line">                    <span class="keyword">if</span>(constraint[i] == <span class="string">&#x27;/&#x27;</span>):</span><br><span class="line">                        constraint[i] = item[i]</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        constraint[i] = <span class="string">&#x27;*&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> constraint</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    dataset, labels = CreateDataset()</span><br><span class="line">    constraint = FINDS(dataset)</span><br><span class="line">    <span class="built_in">print</span>(constraint)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h3 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a><code>ID3</code>算法</h3><ol><li>决策树：决策树是一种常用的分类与回归方法。决策树的模型为树形结构，在针对分类问题时，实际上就是针对输入数据的各个特征对实例进行分类的过程，即通过树形结构的模型，在每一层级上对特征值进行判断，进而到达决策树叶子节点，即完成分类过程。<br><strong>决策树的本质是概念学习。</strong></li><li><p>信息熵（香浓熵）、条件熵和信息增益的概念</p><ul><li>信息量：一件事发生的概率越小，我们说它所蕴含的信息量越大。<br><img src="https://s2.ax1x.com/2019/10/22/K36VNq.jpg" alt="信息量"></li><li>信息熵：信息熵就是所有可能发生的事件的信息量的期望<br><img src="https://s2.ax1x.com/2019/10/22/K36EEn.jpg" alt="信息熵"></li><li>条件熵：表示在X给定条件下，Y的条件概率分布的熵对X的数学期望。<br>![条件熵(<a href="https://s2.ax1x.com/2019/10/22/K36FBj.jpg">https://s2.ax1x.com/2019/10/22/K36FBj.jpg</a>)</li><li>信息增益：当我们用另一个变量X对原变量Y分类后，原变量Y的不确定性就会减小了(即熵值减小)。而熵就是不确定性，不确定程度减少了多少其实就是信息增益。这就是信息增益的由来，所以信息增益定义如下：<br><img src="https://s2.ax1x.com/2019/10/22/K36kHs.jpg" alt="信息增益"></li></ul></li><li><p>算法’python’实现:<br>(用课件上的贷款数据集一直没法成功分类，于是参考了csdn博客的另一个数据集合代码)</p></li></ol><p><code>myTrees.py</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string"> Created on 2019/10/22 11:59</span></span><br><span class="line"><span class="string"> myTrees</span></span><br><span class="line"><span class="string"> @Author  : Zhouy</span></span><br><span class="line"><span class="string"> @Blog    : www.crocodilezs.top</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 原始数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createDataSet</span>():</span><br><span class="line">    dataSet = [[<span class="number">1</span>, <span class="number">1</span>, <span class="string">&#x27;yes&#x27;</span>],</span><br><span class="line">               [<span class="number">1</span>, <span class="number">1</span>, <span class="string">&#x27;yes&#x27;</span>],</span><br><span class="line">               [<span class="number">1</span>, <span class="number">0</span>, <span class="string">&#x27;no&#x27;</span>],</span><br><span class="line">               [<span class="number">0</span>, <span class="number">1</span>, <span class="string">&#x27;no&#x27;</span>],</span><br><span class="line">               [<span class="number">0</span>, <span class="number">1</span>, <span class="string">&#x27;no&#x27;</span>]]</span><br><span class="line">    labels = [<span class="string">&#x27;no surfacing&#x27;</span>,<span class="string">&#x27;flippers&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> dataSet, labels</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多数表决器</span></span><br><span class="line"><span class="comment"># 列中相同值数量最多为结果</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">majorityCnt</span>(<span class="params">classList</span>):</span><br><span class="line">    classCounts = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> classList:</span><br><span class="line">        <span class="keyword">if</span> (value <span class="keyword">not</span> <span class="keyword">in</span> classCounts.keys()):</span><br><span class="line">            classCounts[value] = <span class="number">0</span></span><br><span class="line">        classCounts[value] += <span class="number">1</span></span><br><span class="line">    sortedClassCount = <span class="built_in">sorted</span>(classCounts.iteritems(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分数据集</span></span><br><span class="line"><span class="comment"># dataSet:原始数据集</span></span><br><span class="line"><span class="comment"># axis:进行分割的指定列索引</span></span><br><span class="line"><span class="comment"># value:指定列中的值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">splitDataSet</span>(<span class="params">dataSet, axis, value</span>):</span><br><span class="line">    retDataSet = []</span><br><span class="line">    <span class="keyword">for</span> featDataVal <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">if</span> featDataVal[axis] == value:</span><br><span class="line">            <span class="comment"># 下面两行去除某一项指定列的值，很巧妙有没有</span></span><br><span class="line">            reducedFeatVal = featDataVal[:axis]</span><br><span class="line">            reducedFeatVal.extend(featDataVal[axis + <span class="number">1</span>:])</span><br><span class="line">            retDataSet.append(reducedFeatVal)</span><br><span class="line">    <span class="keyword">return</span> retDataSet</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算香农熵</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcShannonEnt</span>(<span class="params">dataSet</span>):</span><br><span class="line">    <span class="comment"># 数据集总项数</span></span><br><span class="line">    numEntries = <span class="built_in">len</span>(dataSet)</span><br><span class="line">    <span class="comment"># 标签计数对象初始化</span></span><br><span class="line">    labelCounts = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> featDataVal <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="comment"># 获取数据集每一项的最后一列的标签值</span></span><br><span class="line">        currentLabel = featDataVal[-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 如果当前标签不在标签存储对象里，则初始化，然后计数</span></span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():</span><br><span class="line">            labelCounts[currentLabel] = <span class="number">0</span></span><br><span class="line">        labelCounts[currentLabel] += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 熵初始化</span></span><br><span class="line">    shannonEnt = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># 遍历标签对象，求概率，计算熵</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts.keys():</span><br><span class="line">        prop = labelCounts[key] / <span class="built_in">float</span>(numEntries)</span><br><span class="line">        shannonEnt -= prop * log(prop, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> shannonEnt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 选出最优特征列索引</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chooseBestFeatureToSplit</span>(<span class="params">dataSet</span>):</span><br><span class="line">    <span class="comment"># 计算特征个数，dataSet最后一列是标签属性，不是特征量</span></span><br><span class="line">    numFeatures = <span class="built_in">len</span>(dataSet[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">    <span class="comment"># 计算初始数据香农熵</span></span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)</span><br><span class="line">    <span class="comment"># 初始化信息增益，最优划分特征列索引</span></span><br><span class="line">    bestInfoGain = <span class="number">0.0</span></span><br><span class="line">    bestFeatureIndex = -<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numFeatures):</span><br><span class="line">        <span class="comment"># 获取每一列数据</span></span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">        <span class="comment"># 将每一列数据去重</span></span><br><span class="line">        uniqueVals = <span class="built_in">set</span>(featList)</span><br><span class="line">        newEntropy = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">            subDataSet = splitDataSet(dataSet, i, value)</span><br><span class="line">            <span class="comment"># 计算条件概率</span></span><br><span class="line">            prob = <span class="built_in">len</span>(subDataSet) / <span class="built_in">float</span>(<span class="built_in">len</span>(dataSet))</span><br><span class="line">            <span class="comment"># 计算条件熵</span></span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataSet)</span><br><span class="line">        <span class="comment"># 计算信息增益</span></span><br><span class="line">        infoGain = baseEntropy - newEntropy</span><br><span class="line">        <span class="keyword">if</span> (infoGain &gt; bestInfoGain):</span><br><span class="line">            bestInfoGain = infoGain</span><br><span class="line">            bestFeatureIndex = i</span><br><span class="line">    <span class="keyword">return</span> bestFeatureIndex</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 决策树创建</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createTree</span>(<span class="params">dataSet, labels</span>):</span><br><span class="line">    <span class="comment"># 获取标签属性，dataSet最后一列，区别于labels标签名称</span></span><br><span class="line">    classList = [example[-<span class="number">1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    <span class="comment"># 树极端终止条件判断</span></span><br><span class="line">    <span class="comment"># 标签属性值全部相同，返回标签属性第一项值</span></span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == <span class="built_in">len</span>(classList):</span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 只有一个特征（1列）</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(dataSet[<span class="number">0</span>]) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)</span><br><span class="line">    <span class="comment"># 获取最优特征列索引</span></span><br><span class="line">    bestFeatureIndex = chooseBestFeatureToSplit(dataSet)</span><br><span class="line">    <span class="comment"># 获取最优索引对应的标签名称</span></span><br><span class="line">    bestFeatureLabel = labels[bestFeatureIndex]</span><br><span class="line">    <span class="comment"># 创建根节点</span></span><br><span class="line">    myTree = &#123;bestFeatureLabel: &#123;&#125;&#125;</span><br><span class="line">    <span class="comment"># 去除最优索引对应的标签名，使labels标签能正确遍历</span></span><br><span class="line">    <span class="keyword">del</span> (labels[bestFeatureIndex])</span><br><span class="line">    <span class="comment"># 获取最优列</span></span><br><span class="line">    bestFeature = [example[bestFeatureIndex] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    uniquesVals = <span class="built_in">set</span>(bestFeature)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniquesVals:</span><br><span class="line">        <span class="comment"># 子标签名称集合</span></span><br><span class="line">        subLabels = labels[:]</span><br><span class="line">        <span class="comment"># 递归</span></span><br><span class="line">        myTree[bestFeatureLabel][value] = createTree(splitDataSet(dataSet, bestFeatureIndex, value), subLabels)</span><br><span class="line">    <span class="keyword">return</span> myTree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取分类结果</span></span><br><span class="line"><span class="comment"># inputTree:决策树字典</span></span><br><span class="line"><span class="comment"># featLabels:标签列表</span></span><br><span class="line"><span class="comment"># testVec:测试向量  例如：简单实例下某一路径 [1,1]  =&gt; yes（树干值组合，从根结点到叶子节点）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">classify</span>(<span class="params">inputTree, featLabels, testVec</span>):</span><br><span class="line">    <span class="comment"># 获取根结点名称，将dict转化为list</span></span><br><span class="line">    firstSide = <span class="built_in">list</span>(inputTree.keys())</span><br><span class="line">    <span class="comment"># 根结点名称String类型</span></span><br><span class="line">    firstStr = firstSide[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 获取根结点对应的子节点</span></span><br><span class="line">    secondDict = inputTree[firstStr]</span><br><span class="line">    <span class="comment"># 获取根结点名称在标签列表中对应的索引</span></span><br><span class="line">    featIndex = featLabels.index(firstStr)</span><br><span class="line">    <span class="comment"># 由索引获取向量表中的对应值</span></span><br><span class="line">    key = testVec[featIndex]</span><br><span class="line">    <span class="comment"># 获取树干向量后的对象</span></span><br><span class="line">    valueOfFeat = secondDict[key]</span><br><span class="line">    <span class="comment"># 判断是子结点还是叶子节点：子结点就回调分类函数，叶子结点就是分类结果</span></span><br><span class="line">    <span class="comment"># if type(valueOfFeat).__name__==&#x27;dict&#x27;: 等价 if isinstance(valueOfFeat, dict):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(valueOfFeat, <span class="built_in">dict</span>):</span><br><span class="line">        classLabel = classify(valueOfFeat, featLabels, testVec)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        classLabel = valueOfFeat</span><br><span class="line">    <span class="keyword">return</span> classLabel</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将决策树分类器存储在磁盘中，filename一般保存为txt格式</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">storeTree</span>(<span class="params">inputTree, filename</span>):</span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fw = <span class="built_in">open</span>(filename, <span class="string">&#x27;wb+&#x27;</span>)</span><br><span class="line">    pickle.dump(inputTree, fw)</span><br><span class="line">    fw.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将瓷盘中的对象加载出来，这里的filename就是上面函数中的txt文件</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">grabTree</span>(<span class="params">filename</span>):</span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fr = <span class="built_in">open</span>(filename, <span class="string">&#x27;rb&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> pickle.load(fr)</span><br></pre></td></tr></table></figure><p><code>treePlotter.py</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string"> Created on 2019/10/22 12:00</span></span><br><span class="line"><span class="string"> treePlotter</span></span><br><span class="line"><span class="string"> @Author  : Zhouy</span></span><br><span class="line"><span class="string"> @Blog    : www.crocodilezs.top</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">decisionNode = <span class="built_in">dict</span>(boxstyle=<span class="string">&quot;sawtooth&quot;</span>, fc=<span class="string">&quot;0.8&quot;</span>)</span><br><span class="line">leafNode = <span class="built_in">dict</span>(boxstyle=<span class="string">&quot;round4&quot;</span>, fc=<span class="string">&quot;0.8&quot;</span>)</span><br><span class="line">arrow_args = <span class="built_in">dict</span>(arrowstyle=<span class="string">&quot;&lt;-&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取树的叶子节点</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getNumLeafs</span>(<span class="params">myTree</span>):</span><br><span class="line">    numLeafs = <span class="number">0</span></span><br><span class="line">    <span class="comment"># dict转化为list</span></span><br><span class="line">    firstSides = <span class="built_in">list</span>(myTree.keys())</span><br><span class="line">    firstStr = firstSides[<span class="number">0</span>]</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="comment"># 判断是否是叶子节点（通过类型判断，子类不存在，则类型为str；子类存在，则为dict）</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(secondDict[</span><br><span class="line">                    key]).__name__ == <span class="string">&#x27;dict&#x27;</span>:  <span class="comment"># test to see if the nodes are dictonaires, if not they are leaf nodes</span></span><br><span class="line">            numLeafs += getNumLeafs(secondDict[key])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            numLeafs += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> numLeafs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取树的层数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getTreeDepth</span>(<span class="params">myTree</span>):</span><br><span class="line">    maxDepth = <span class="number">0</span></span><br><span class="line">    <span class="comment"># dict转化为list</span></span><br><span class="line">    firstSides = <span class="built_in">list</span>(myTree.keys())</span><br><span class="line">    firstStr = firstSides[<span class="number">0</span>]</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(secondDict[</span><br><span class="line">                    key]).__name__ == <span class="string">&#x27;dict&#x27;</span>:  <span class="comment"># test to see if the nodes are dictonaires, if not they are leaf nodes</span></span><br><span class="line">            thisDepth = <span class="number">1</span> + getTreeDepth(secondDict[key])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            thisDepth = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> thisDepth &gt; maxDepth: maxDepth = thisDepth</span><br><span class="line">    <span class="keyword">return</span> maxDepth</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotNode</span>(<span class="params">nodeTxt, centerPt, parentPt, nodeType</span>):</span><br><span class="line">    createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords=<span class="string">&#x27;axes fraction&#x27;</span>,</span><br><span class="line">                            xytext=centerPt, textcoords=<span class="string">&#x27;axes fraction&#x27;</span>,</span><br><span class="line">                            va=<span class="string">&quot;center&quot;</span>, ha=<span class="string">&quot;center&quot;</span>, bbox=nodeType, arrowprops=arrow_args)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotMidText</span>(<span class="params">cntrPt, parentPt, txtString</span>):</span><br><span class="line">    xMid = (parentPt[<span class="number">0</span>] - cntrPt[<span class="number">0</span>]) / <span class="number">2.0</span> + cntrPt[<span class="number">0</span>]</span><br><span class="line">    yMid = (parentPt[<span class="number">1</span>] - cntrPt[<span class="number">1</span>]) / <span class="number">2.0</span> + cntrPt[<span class="number">1</span>]</span><br><span class="line">    createPlot.ax1.text(xMid, yMid, txtString, va=<span class="string">&quot;center&quot;</span>, ha=<span class="string">&quot;center&quot;</span>, rotation=<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotTree</span>(<span class="params">myTree, parentPt, nodeTxt</span>):  <span class="comment"># if the first key tells you what feat was split on</span></span><br><span class="line">    numLeafs = getNumLeafs(myTree)  <span class="comment"># this determines the x width of this tree</span></span><br><span class="line">    depth = getTreeDepth(myTree)</span><br><span class="line">    firstSides = <span class="built_in">list</span>(myTree.keys())</span><br><span class="line">    firstStr = firstSides[<span class="number">0</span>]  <span class="comment"># the text label for this node should be this</span></span><br><span class="line">    cntrPt = (plotTree.xOff + (<span class="number">1.0</span> + <span class="built_in">float</span>(numLeafs)) / <span class="number">2.0</span> / plotTree.totalW, plotTree.yOff)</span><br><span class="line">    plotMidText(cntrPt, parentPt, nodeTxt)</span><br><span class="line">    plotNode(firstStr, cntrPt, parentPt, decisionNode)</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    plotTree.yOff = plotTree.yOff - <span class="number">1.0</span> / plotTree.totalD</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(secondDict[</span><br><span class="line">                    key]).__name__ == <span class="string">&#x27;dict&#x27;</span>:  <span class="comment"># test to see if the nodes are dictonaires, if not they are leaf nodes</span></span><br><span class="line">            plotTree(secondDict[key], cntrPt, <span class="built_in">str</span>(key))  <span class="comment"># recursion</span></span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># it&#x27;s a leaf node print the leaf node</span></span><br><span class="line">            plotTree.xOff = plotTree.xOff + <span class="number">1.0</span> / plotTree.totalW</span><br><span class="line">            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)</span><br><span class="line">            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, <span class="built_in">str</span>(key))</span><br><span class="line">    plotTree.yOff = plotTree.yOff + <span class="number">1.0</span> / plotTree.totalD</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># if you do get a dictonary you know it&#x27;s a tree, and the first element will be another dict</span></span><br><span class="line"><span class="comment"># 绘制决策树</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createPlot</span>(<span class="params">inTree</span>):</span><br><span class="line">    fig = plt.figure(<span class="number">1</span>, facecolor=<span class="string">&#x27;white&#x27;</span>)</span><br><span class="line">    fig.clf()</span><br><span class="line">    axprops = <span class="built_in">dict</span>(xticks=[], yticks=[])</span><br><span class="line">    createPlot.ax1 = plt.subplot(<span class="number">111</span>, frameon=<span class="literal">False</span>, **axprops)  <span class="comment"># no ticks</span></span><br><span class="line">    <span class="comment"># createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo puropses</span></span><br><span class="line">    plotTree.totalW = <span class="built_in">float</span>(getNumLeafs(inTree))</span><br><span class="line">    plotTree.totalD = <span class="built_in">float</span>(getTreeDepth(inTree))</span><br><span class="line">    plotTree.xOff = -<span class="number">0.5</span> / plotTree.totalW</span><br><span class="line">    plotTree.yOff = <span class="number">1.0</span></span><br><span class="line">    plotTree(inTree, (<span class="number">0.5</span>, <span class="number">1.0</span>), <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制树的根节点和叶子节点（根节点形状：长方形，叶子节点：椭圆形）</span></span><br><span class="line"><span class="comment"># def createPlot():</span></span><br><span class="line"><span class="comment">#    fig = plt.figure(1, facecolor=&#x27;white&#x27;)</span></span><br><span class="line"><span class="comment">#    fig.clf()</span></span><br><span class="line"><span class="comment">#    createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo puropses</span></span><br><span class="line"><span class="comment">#    plotNode(&#x27;a decision node&#x27;, (0.5, 0.1), (0.1, 0.5), decisionNode)</span></span><br><span class="line"><span class="comment">#    plotNode(&#x27;a leaf node&#x27;, (0.8, 0.1), (0.3, 0.8), leafNode)</span></span><br><span class="line"><span class="comment">#    plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">retrieveTree</span>(<span class="params">i</span>):</span><br><span class="line">    listOfTrees = [&#123;<span class="string">&#x27;no surfacing&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;no&#x27;</span>, <span class="number">1</span>: &#123;<span class="string">&#x27;flippers&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;no&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;yes&#x27;</span>&#125;&#125;&#125;&#125;,</span><br><span class="line">                   &#123;<span class="string">&#x27;no surfacing&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;no&#x27;</span>, <span class="number">1</span>: &#123;<span class="string">&#x27;flippers&#x27;</span>: &#123;<span class="number">0</span>: &#123;<span class="string">&#x27;head&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;no&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;yes&#x27;</span>&#125;&#125;, <span class="number">1</span>: <span class="string">&#x27;no&#x27;</span>&#125;&#125;&#125;&#125;</span><br><span class="line">                   ]</span><br><span class="line">    <span class="keyword">return</span> listOfTrees[i]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># thisTree = retrieveTree(0)</span></span><br><span class="line"><span class="comment"># createPlot(thisTree)</span></span><br><span class="line"><span class="comment"># createPlot()</span></span><br><span class="line"><span class="comment"># myTree = retrieveTree(0)</span></span><br><span class="line"><span class="comment"># numLeafs =getNumLeafs(myTree)</span></span><br><span class="line"><span class="comment"># treeDepth =getTreeDepth(myTree)</span></span><br><span class="line"><span class="comment"># print(u&quot;叶子节点数目：%d&quot;% numLeafs)</span></span><br><span class="line"><span class="comment"># print(u&quot;树深度：%d&quot;%treeDepth)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>testTrees_3.py</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string"> Created on 2019/10/22 12:00</span></span><br><span class="line"><span class="string"> testTrees_3</span></span><br><span class="line"><span class="string"> @Author  : Zhouy</span></span><br><span class="line"><span class="string"> @Blog    : www.crocodilezs.top</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> myTrees <span class="keyword">as</span> mt</span><br><span class="line"><span class="keyword">import</span> treePlotter <span class="keyword">as</span> tp</span><br><span class="line"><span class="comment">#测试</span></span><br><span class="line">dataSet, labels = mt.createDataSet()</span><br><span class="line"><span class="comment">#copy函数：新开辟一块内存，然后将list的所有值复制到新开辟的内存中</span></span><br><span class="line">labels1 = labels.copy()</span><br><span class="line"><span class="comment">#createTree函数中将labels1的值改变了，所以在分类测试时不能用labels1</span></span><br><span class="line">myTree = mt.createTree(dataSet,labels1)</span><br><span class="line"><span class="comment">#保存树到本地</span></span><br><span class="line">mt.storeTree(myTree,<span class="string">&#x27;myTree.txt&#x27;</span>)</span><br><span class="line"><span class="comment">#在本地磁盘获取树</span></span><br><span class="line">myTree = mt.grabTree(<span class="string">&#x27;myTree.txt&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">u&quot;决策树结构：%s&quot;</span>%myTree)</span><br><span class="line"><span class="comment">#绘制决策树</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">u&quot;绘制决策树：&quot;</span>)</span><br><span class="line">tp.createPlot(myTree)</span><br><span class="line">numLeafs =tp.getNumLeafs(myTree)</span><br><span class="line">treeDepth =tp.getTreeDepth(myTree)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">u&quot;叶子节点数目：%d&quot;</span>% numLeafs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">u&quot;树深度：%d&quot;</span>%treeDepth)</span><br><span class="line"><span class="comment">#测试分类 简单样本数据3列</span></span><br><span class="line">labelResult =mt.classify(myTree,labels,[<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">u&quot;[1,1] 测试结果为：%s&quot;</span>%labelResult)</span><br><span class="line">labelResult =mt.classify(myTree,labels,[<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">u&quot;[1,0] 测试结果为：%s&quot;</span>%labelResult)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;作业要求&quot;&gt;&lt;a href=&quot;#作业要求&quot; class=&quot;headerlink&quot; title=&quot;作业要求&quot;&gt;&lt;/a&gt;作业要求&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;实现&lt;code&gt;FINDS&lt;/code&gt;算法&lt;/li&gt;
&lt;li&gt;实现&lt;code&gt;ID3&lt;/code&gt;算法&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;不要调库自己写。如果有能力可以继续用课件里的数据集测试两个算法（用天气的4条记录测试&lt;code&gt;FINDS&lt;/code&gt;，用贷款的15条记录测试&lt;code&gt;ID3&lt;/code&gt;）给出训练误差测试误差等；  &lt;/li&gt;
&lt;li&gt;再有能力可以使用更大的数据集测试算法。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="机器学习" scheme="https://enblog.crocodilezs.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="FindS" scheme="https://enblog.crocodilezs.top/tags/FindS/"/>
    
    <category term="ID3" scheme="https://enblog.crocodilezs.top/tags/ID3/"/>
    
  </entry>
  
  <entry>
    <title>计算机系统基础实验一、Linux环境和GCC工具链</title>
    <link href="https://enblog.crocodilezs.top/201910/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80%E5%AE%9E%E9%AA%8C%E4%B8%80/"/>
    <id>https://enblog.crocodilezs.top/201910/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80%E5%AE%9E%E9%AA%8C%E4%B8%80/</id>
    <published>2019-10-25T07:00:12.000Z</published>
    <updated>2022-05-23T17:10:06.177Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Linux操作系统概述和实验环境介绍"><a href="#Linux操作系统概述和实验环境介绍" class="headerlink" title="Linux操作系统概述和实验环境介绍"></a><code>Linux</code>操作系统概述和实验环境介绍</h1><h2 id="操作系统概念"><a href="#操作系统概念" class="headerlink" title="操作系统概念"></a>操作系统概念</h2><p><code>OS</code>是管理和控制计算机硬件与软件资源的计算机程序，是直接在“裸机”上的最基本的系统软件。</p><h2 id="Linux的应用"><a href="#Linux的应用" class="headerlink" title="Linux的应用"></a><code>Linux</code>的应用</h2><ol><li>服务器端：Linux非常稳定，特别适合大型企业生产环境。</li><li>作为网络平台的后端服务器被使用。</li><li>作为应用服务器、数据库服务器被使用：解决海量数据、高并发的问题；</li><li>作为嵌入式操作系统被使用：智能控制、自动化、物联网等领域。</li></ol><span id="more"></span><h2 id="Linux历史"><a href="#Linux历史" class="headerlink" title="Linux历史"></a><code>Linux</code>历史</h2><p>追溯到<code>UNIX</code><br>简单地说，<code>Linux</code>是对<code>UNIX</code>的重新实现。世界各地的<code>Linux</code>开发人员借鉴了<code>UNIX</code>的技术和用户界面，并且融入了很多独创的技术。<code>Linux</code>不属于<code>BSD</code>和<code>AT&amp;T</code>风格的<code>UNIX</code>中的任何一种。因此，严格来说，<code>Linux</code>是有别于<code>UNIX</code>的另一种操作系统。</p><h2 id="Linux简介"><a href="#Linux简介" class="headerlink" title="Linux简介"></a><code>Linux</code>简介</h2><p><code>Linux</code>发现行版本举例：<code>Ubuntu</code>、<code>redhat</code></p><h2 id="操作系统的三个部分"><a href="#操作系统的三个部分" class="headerlink" title="操作系统的三个部分"></a>操作系统的三个部分</h2><h3 id="内核"><a href="#内核" class="headerlink" title="内核"></a>内核</h3><p>操作系统五大管理功能一般都由操作系统内核负责。</p><h3 id="外壳"><a href="#外壳" class="headerlink" title="外壳"></a>外壳</h3><ul><li>外壳程序负责接收用户操作，提供与 用户的交互界面。</li><li>一般操作系统提供给用户的界面主要有两种：文本界面、<code>GUI</code>图形界面。<h3 id="管理工具和附属软件"><a href="#管理工具和附属软件" class="headerlink" title="管理工具和附属软件"></a>管理工具和附属软件</h3></li></ul><h2 id="操作系统的功能"><a href="#操作系统的功能" class="headerlink" title="操作系统的功能"></a>操作系统的功能</h2><ol><li><code>CPU</code>的控制与管理：处理器管理</li><li>内存的分配与管理：存储器管理</li><li>外部设备的控制与管理：设备管理</li><li>文件管理</li><li>作业管理和控制：用户接口</li></ol><h2 id="Shell"><a href="#Shell" class="headerlink" title="Shell"></a><code>Shell</code></h2><ol><li>外壳程序对用户的输入命令进行解释，为用户提供一种通过操作系统使用计算机的操作环境。</li><li><code>Windows</code>的图形界面，由一个成为<code>Explorer</code>的模块解释用户的输入。</li><li>如<code>DOS</code>的命令行界面，<code>Command.com</code>是对命令输入进行解释的外壳程序(<code>Linux</code>的<code>Shell</code>)</li><li>Shell命令：从命令行输入语句，每输入一次就能得到一次响应，这些语句就是<code>shell</code>命令。</li><li><code>Shell</code>程序：又称<code>Shell</code>脚本。（把一系列的<code>shell</code>命令，按照一定的语法规则和控制结构，组织在一个文件中，然后由内核来一条接一条地解释和执行这些命令，这个文件就是shell程序，类似<code>DOS</code>/<code>Winsows</code>中的。bat批处理文件。）</li><li>[username@computername ~]$<br>user name为当前用户名，computername 为当前计算机名 ，$表示当前用户是一般用户。 <h2 id="ssh-secure-shell"><a href="#ssh-secure-shell" class="headerlink" title="ssh secure shell"></a><code>ssh</code> secure shell</h2>把<code>Linux</code>终端搬到<code>Windows</code>下，连接到BUPT1.<h2 id="Shell常用命令"><a href="#Shell常用命令" class="headerlink" title="Shell常用命令"></a><code>Shell</code>常用命令</h2><h3 id="目录操作命令"><a href="#目录操作命令" class="headerlink" title="目录操作命令"></a>目录操作命令</h3>目录操作命令能够对当前的目录进行查看、创建、删除，以及显示当前工作目录和改变当前目录等操作。</li></ol><div note="class info">    1. /etc - 系统所需的重要配置和管理文件<br />    2. /dev - 存放device file（装置文件）<br />    3. /boot - 存放系统激活的相关文件，不可任意删除。<br />    4. /home - 登陆用户的主目录<br />    5. /lib - 存放系统激活时需要的系统函数库<br />    6. /usr/lib - 存放一些应用程序的共享函数库<br />    7. /mnt - 系统默认的挂载点(mount point)    8. /proc - 虚拟文件系统，不占用硬盘空间，目录下的文件均放置于内存中<br />    9. /root - 系统管理用户root的主目录<br />    10. /bin - 存放一些系统启动时所需的普通程序和系统程序<br />    11. /tmp - 存放临时文件    12. /var - 存放被系统修改过的数据。</div><p>常用的目录操作命令包括：</p><ol><li>pwd 打印当前工作目录</li><li>cd 改变当前所在目录</li><li>ls 查看当前目录下的内容</li><li>dir 类似ls命令</li><li>mkdir 创建目录</li><li>rmdir 删除空目录</li></ol><h3 id="文件操作命令"><a href="#文件操作命令" class="headerlink" title="文件操作命令"></a>文件操作命令</h3><ul><li>在命令行环境下对文件进行操作将比在图形环境下操作文件更加快捷和高效</li><li>文件操作主要包括：搜索文件、复制和移动文件、删除文件以及合并文件内容</li></ul><p>常用文件操作命令：  </p><ul><li><code>cat</code>  </li><li><code>more</code>  </li><li><code>less</code>  </li><li><code>head</code></li><li><code>tail</code></li><li><code>cp</code></li><li><code>mv</code></li><li><code>rm</code></li><li><code>find</code></li><li><code>touch</code></li><li><code>ln</code></li></ul><h3 id="使用帮助命令"><a href="#使用帮助命令" class="headerlink" title="使用帮助命令"></a>使用帮助命令</h3><ol><li><code>man 命令名</code> </li><li><code>whatis 命令名</code>  </li><li><code>help 命令名</code>：适用于部分命令</li></ol><h2 id="Vi编辑器"><a href="#Vi编辑器" class="headerlink" title="Vi编辑器"></a>Vi编辑器</h2><h3 id="Vi简介"><a href="#Vi简介" class="headerlink" title="Vi简介"></a><code>Vi</code>简介</h3><ul><li><code>Vi</code>编辑器是<code>Visual interface</code>的简称，它可以执行输出、删除、查找、替换、块操作等众多文本操作</li><li><code>Vi</code>不是一个排版程序，只是一个文本编辑程序。</li><li>是全屏幕文本编辑器，没有菜单，只有命令。</li></ul><h3 id="Vi的基本概念"><a href="#Vi的基本概念" class="headerlink" title="Vi的基本概念"></a><code>Vi</code>的基本概念</h3><ol><li>命令行模式（command mode）<br>控制屏幕光标的移动、字符、字或行的删除、移动复制某区段及进入<code>Insert mode</code>下，或者到<code>last line mode</code>。</li><li>插入模式（Insert mode）<br>只有在<code>Insert mode</code>下，才可以做文字输入，按ESC键可回到命令行模式。</li><li>底行模式(last line mode)<br>将文件保存或退出vi，也可以设置编辑环境，如寻找字符串、列出行号。</li></ol><div class="note info">    $ vi test.txt<br />    即可进入vi（打开或新建文件）</div><p>操作：</p><ol><li>命令行模式 —-&gt;(i)  插入模式</li><li>插入模式  —-&gt;（ESC）  命令行模式</li><li>如果处于「插入模式」，就只能一直输入文字，如果发现输错了字想用光标往回移动将该字删除，就得先回到「命令行模式」</li><li>在「命令行模式」下，按下：进入底行模式<br><code>: w filename</code><br><code>: wq</code><br><code>: q!</code></li></ol><h2 id="GCC工具链"><a href="#GCC工具链" class="headerlink" title="GCC工具链"></a><code>GCC</code>工具链</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><ol><li><code>GCC</code>编译器能将<code>C</code>和<code>C++</code>语言源程序、汇编程序编译、链接成可执行文件。</li><li>使用<code>GCC</code>编译器时，编译过程可以被细分为四个阶段：<ul><li>预处理(Pre-Processing)</li><li>编译(Compiling)</li><li>汇编(Assembling)</li><li>链接(Linking)</li></ul></li></ol><h3 id="GDB的概述"><a href="#GDB的概述" class="headerlink" title="GDB的概述"></a><code>GDB</code>的概述</h3><p><code>GDB</code>是一款GNU开发组织并发布的UNIX/Linux下的程序调试工具。它使你能够在程序运行时观察程序的内部结构和内存的使用情况。以下是<code>GDB</code>提供的一些功能：</p><ol><li>监视程序中变量的值</li><li>设置断点以使程序在指定的代码行上停止运行</li><li>能逐行执行代码</li></ol><h2 id="Objdump简介"><a href="#Objdump简介" class="headerlink" title="Objdump简介"></a><code>Objdump</code>简介</h2><p><code>Objdump</code>是以一种可阅读的格式让你更多地了解二进制文件可能带有地附加信息。<br>对于想进一步了解系统地程序员，这个命令没有没有更多意义，对于想进一步了解系统的程序员，应该掌握这种工具，至少你可以自己写写<code>shellcode</code>了，或者看看人家给的<code>exploit</code>中的<code>shellcode</code>是什么东西。<br><strong>把C语言源代码编译链接生成的可执行程序反汇编后得到对应的汇编代码，可以帮助我们理解C语言和汇编语言之间的对应关系。非常有助于深入理解C语言</strong></p><div class="note warning">    至此，已经完成了计算机系统基础第一次实验的理论部分，其中有太多的东西还需要自己去实践、接下来开始实验！</div>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Linux操作系统概述和实验环境介绍&quot;&gt;&lt;a href=&quot;#Linux操作系统概述和实验环境介绍&quot; class=&quot;headerlink&quot; title=&quot;Linux操作系统概述和实验环境介绍&quot;&gt;&lt;/a&gt;&lt;code&gt;Linux&lt;/code&gt;操作系统概述和实验环境介绍&lt;/h1&gt;&lt;h2 id=&quot;操作系统概念&quot;&gt;&lt;a href=&quot;#操作系统概念&quot; class=&quot;headerlink&quot; title=&quot;操作系统概念&quot;&gt;&lt;/a&gt;操作系统概念&lt;/h2&gt;&lt;p&gt;&lt;code&gt;OS&lt;/code&gt;是管理和控制计算机硬件与软件资源的计算机程序，是直接在“裸机”上的最基本的系统软件。&lt;/p&gt;
&lt;h2 id=&quot;Linux的应用&quot;&gt;&lt;a href=&quot;#Linux的应用&quot; class=&quot;headerlink&quot; title=&quot;Linux的应用&quot;&gt;&lt;/a&gt;&lt;code&gt;Linux&lt;/code&gt;的应用&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;服务器端：Linux非常稳定，特别适合大型企业生产环境。&lt;/li&gt;
&lt;li&gt;作为网络平台的后端服务器被使用。&lt;/li&gt;
&lt;li&gt;作为应用服务器、数据库服务器被使用：解决海量数据、高并发的问题；&lt;/li&gt;
&lt;li&gt;作为嵌入式操作系统被使用：智能控制、自动化、物联网等领域。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="计算机系统基础" scheme="https://enblog.crocodilezs.top/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"/>
    
    
  </entry>
  
</feed>
